
<!DOCTYPE html>
<html lang="en">

<!-- Head -->
<head>

        <!-- Required metadata tags -->
        <meta charset="utf-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />

        <!-- Default metadata -->
    <meta name="author" content="Tejus Adiga M" />
    <meta name="description" content="Introduction to Open AI CLIP model and its downstream tasks" />
    <meta name="keywords" content="MLX, Machine Learning, VLM, CLIP">
<meta property="og:site_name" content="Entropy Labs" />
<meta property="og:title" content="Understanding and Fine-tuning CLIP. A Revolutionary Vision-Language Model" />
<meta property="og:description" content="Introduction to Open AI CLIP model and its downstream tasks" />
<meta property="og:locale" content="en_US" />
<meta property="og:url" content="http://github.io/tejusadiga2004/octave.github.io/understanding-and-fine-tuning-clip-a-revolutionary-vision-language-model.html" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-06-25 14:30:00+05:30" />
<meta property="article:modified_time" content="" />
<meta property="article:author" content="http://github.io/tejusadiga2004/octave.github.io/author/tejus-adiga-m.html">
<meta property="article:section" content="Vision Language Models" />
	<meta property="article:tag" content="MLX" />
	<meta property="article:tag" content="Machine Learning" />
	<meta property="article:tag" content="VLM" />
	<meta property="article:tag" content="CLIP" />
	<meta property="og:image" content="http://github.io/tejusadiga2004/octave.github.io/">

        <!-- Site Claim -->


        <!-- Title -->
        <title>
    Understanding and Fine-tuning CLIP. A Revolutionary Vision-Language Model &ndash; Entropy Labs
        </title>
        
        <!-- Icon -->
        <link rel="shortcut icon" href="http://github.io/tejusadiga2004/octave.github.io/favicon.ico" type="image/x-icon">
        <link rel="icon" href="http://github.io/tejusadiga2004/octave.github.io/favicon.ico" type="image/x-icon">

        <!-- Search engine -->
            <meta name="robots" content="" />

        <!-- Feeds -->
            <link href="http://github.io/tejusadiga2004/octave.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Entropy Labs Full Atom Feed" />




            <link href="http://github.io/tejusadiga2004/octave.github.io/feeds/vision-language-models.atom.xml" type="application/atom+xml" rel="alternate" title="Entropy Labs Categories Atom Feed" />




        <!-- Styles -->
        <!--
        <link rel="stylesheet" href="https://ajax.aspnetcdn.com/ajax/bootstrap/4.3.1/css/bootstrap.min.css">
        -->
        <link rel="stylesheet" href="http://github.io/tejusadiga2004/octave.github.io/theme/bootstrap/bootstrap.min.css">
        <!--
        <link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css">
        -->
        <link rel="stylesheet" href="http://github.io/tejusadiga2004/octave.github.io/theme/pygment/friendly.min.css">
        <!--
        <link rel="stylesheet" href="http://github.io/tejusadiga2004/octave.github.io/theme/extra/admonition.min.css">
        -->
        <link rel="stylesheet" href="http://github.io/tejusadiga2004/octave.github.io/theme/style.css">

        <!-- Google Analytics -->

        <!-- Google Global Site Tag -->

        <!-- Google Tag Manager -->

        <!-- Google Adsense -->

        <!-- Heap Analytic -->

        <!-- Piwik Tracking -->

        <!-- Matomo Tracking -->

</head>

<!-- Body -->
<body class="d-flex flex-column" data-spy="scroll" data-target="#toc" data-offset="0" style="position: relative;">
    <!-- Top anchor -->
    <a href="#" id="backToTop" style="display: none; z-index: 1;" title="Back to top"><span></span></a>

    <!-- Google tag manager -->

    <!-- Navigation -->
    <nav class="flex-shrink-0 navbar navbar-expand-md navbar-expand-lg navbar-dark bg-dark text-light shadow-sm">
        <!-- Logo -->
        <a class="navbar-brand" href="http://github.io/tejusadiga2004/octave.github.io"></a>

        <!-- Collapse button -->
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMenu" aria-controls="navbarMenu" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon small"></span>
        </button>

        <!-- Collapsible content -->
        <div class="collapse navbar-collapse" id="navbarMenu">

            <!-- i18n subsites -->

            <!-- Page links -->
            <ul class="navbar-nav mr-auto text-center">
                <li class="nav-item ">                           
                    <a class="nav-link" href="http://github.io/tejusadiga2004/octave.github.io">
                        <svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24">
                            <path d="M21 13v10h-6v-6h-6v6h-6v-10h-3l12-12 12 12h-3zm-1-5.907v-5.093h-3v2.093l3 3z" fill="currentColor"></path>
                        </svg>
                        Home <span class="sr-only">(current)</span>
                    </a>
                </li>
                <li class="nav-item ">
                    <a class="nav-link" href="http://github.io/tejusadiga2004/octave.github.io/categories.html">
                        <svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24">
                            <path d="M16 6h-8v-6h8v6zm-8 12h-8v6h8v-6zm16 0h-8v6h8v-6zm-11-7v-3h-2v3h-8v5h2v-3h14v3h2v-5h-8z" fill="currentColor"></path>
                        </svg>
                        Categories
                    </a>
                </li>
                <li class="nav-item ">
                    <a class="nav-link" href="http://github.io/tejusadiga2004/octave.github.io/tags.html">
                        <svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24">
                            <path d="M10.605 0h-10.605v10.609l13.391 13.391 10.609-10.604-13.395-13.396zm-4.191 6.414c-.781.781-2.046.781-2.829.001-.781-.783-.781-2.048 0-2.829.782-.782 2.048-.781 2.829-.001.782.782.781 2.047 0 2.829z" fill="currentColor"></path>
                        </svg>
                        Tags
                    </a>
                </li>
                <li class="nav-item ">
                    <a class="nav-link" href="http://github.io/tejusadiga2004/octave.github.io/archives.html">
                        <svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24">
                            <path d="M1.8 9l-.8-4h22l-.8 4h-2.029l.39-2h-17.122l.414 2h-2.053zm18.575-6l.604-2h-17.979l.688 2h16.687zm3.625 8l-2 13h-20l-2-13h24zm-8 4c0-.552-.447-1-1-1h-6c-.553 0-1 .448-1 1s.447 1 1 1h6c.553 0 1-.448 1-1z" fill="currentColor"></path>
                        </svg>
                        Archives
                    </a>
                </li>
                <li class="nav-item ">
                    <a class="nav-link" href="http://github.io/tejusadiga2004/octave.github.io/pages/about.html">
                        <svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24">
                            <path d="M20.822 18.096c-3.439-.794-6.64-1.49-5.09-4.418 4.72-8.912 1.251-13.678-3.732-13.678-5.082 0-8.464 4.949-3.732 13.678 1.597 2.945-1.725 3.641-5.09 4.418-3.073.71-3.188 2.236-3.178 4.904l.004 1h23.99l.004-.969c.012-2.688-.092-4.222-3.176-4.935z" fill="currentColor"></path>
                        </svg>
                        About
                    </a>
                </li>
            </ul>

            <!-- Search form -->
            <form class="form-inline text-center" action="http://github.io/tejusadiga2004/octave.github.io/search.html">
                <input class="form-control w-100 bg-dark text-light text-center border-0 p-2" type="text" name="q" pattern=".{3,}" title="At least 3 characters" required="" placeholder="Type here to search" aria-label="Search">
            </form>

            <!-- Social links -->
            <ul class="navbar-nav text-center">
                <li class="nav-item">
                    <a class="nav-link" href="">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                            <title>Facebook</title>
                            <path d="M12 0c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm3 8h-1.35c-.538 0-.65.221-.65.778v1.222h2l-.209 2h-1.791v7h-3v-7h-2v-2h2v-2.308c0-1.769.931-2.692 3.029-2.692h1.971v3z" fill="currentColor"></path>
                        </svg>
                    </a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                            <title>Github</title>
                            <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z" fill="currentColor"></path>
                        </svg>
                    </a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                            <title>Linkedin</title>
                            <path d="M12 0c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm-2 16h-2v-6h2v6zm-1-6.891c-.607 0-1.1-.496-1.1-1.109 0-.612.492-1.109 1.1-1.109s1.1.497 1.1 1.109c0 .613-.493 1.109-1.1 1.109zm8 6.891h-1.998v-2.861c0-1.881-2.002-1.722-2.002 0v2.861h-2v-6h2v1.093c.872-1.616 4-1.736 4 1.548v3.359z" fill="currentColor"></path>
                        </svg>
                    </a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                            <title>Twitter</title>
                            <path d="M12 0c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm6.066 9.645c.183 4.04-2.83 8.544-8.164 8.544-1.622 0-3.131-.476-4.402-1.291 1.524.18 3.045-.244 4.252-1.189-1.256-.023-2.317-.854-2.684-1.995.451.086.895.061 1.298-.049-1.381-.278-2.335-1.522-2.304-2.853.388.215.83.344 1.301.359-1.279-.855-1.641-2.544-.889-3.835 1.416 1.738 3.533 2.881 5.92 3.001-.419-1.796.944-3.527 2.799-3.527.825 0 1.572.349 2.096.907.654-.128 1.27-.368 1.824-.697-.215.671-.67 1.233-1.263 1.589.581-.07 1.135-.224 1.649-.453-.384.578-.87 1.084-1.433 1.489z" fill="currentColor"></path>
                        </svg>
                    </a>
                </li>
            </ul>
        </div>
    </nav>

    <!-- Full page -->
    <div class="flex-shrink-0 flex-grow-1">

        <!-- Header -->
        <header class="bg-dark text-light shadow-sm pt-3 pb-2">
	<div class="container">
		<h3 id="understanding-and-fine-tuning-clip-a-revolutionary-vision-language-model">Understanding and Fine-tuning CLIP. A Revolutionary Vision-Language Model</h3>
		<p style="font-size:larger;"><p>Introduction to Open AI CLIP model and its downstream tasks</p></p>
        <div class="row mx-auto mt-3">
            <div class="col-xs-12 col-sm-12 col-md-6 text-left" style="padding: 0">
                <a href="http://github.io/tejusadiga2004/octave.github.io/author/tejus-adiga-m.html" class="card-link">Tejus Adiga M</a>
                <span class="card-link text-success">
                    <span class="post-date" title="Post date">Wed 25 June 2025</span>
                    <span class="text-info modified-date" title="Updated date">
                            Wed 25 June 2025
                    </span>
                </span>
            </div>
            <div class="col-xs-12 col-sm-12 col-md-6 text-right" style="padding: 0">
                <a class="badge badge-success" href="http://github.io/tejusadiga2004/octave.github.io/category/vision-language-models.html">vision language models</a>
                    <a class="badge badge-info" href="http://github.io/tejusadiga2004/octave.github.io/tag/mlx.html">mlx</a>
                    <a class="badge badge-info" href="http://github.io/tejusadiga2004/octave.github.io/tag/machine-learning.html">machine learning</a>
                    <a class="badge badge-info" href="http://github.io/tejusadiga2004/octave.github.io/tag/vlm.html">vlm</a>
                    <a class="badge badge-info" href="http://github.io/tejusadiga2004/octave.github.io/tag/clip.html">clip</a>
            </div>
        </div>
	</div>
        </header>

        <!-- Main -->
        <main class="py-3">
                <div class="container">
                    <!-- Sharing -->

                    <!-- Content -->
    <!-- 2 columns layout -->
    <!-- single column layout -->
        <!-- Sharing -->

        <!-- Share post -->

        <!-- Article -->
        <div>
            <h1>Understanding and Fine-tuning CLIP: A Revolutionary Vision-Language Model</h1>
<p>In the rapidly evolving field of artificial intelligence, OpenAI's CLIP (Contrastive Language-Image Pre-training) model stands as a revolutionary advancement in connecting visual and textual data. This blog post explores the architecture, training methodology, and zero-shot classification capabilities of CLIP, followed by a practical implementation of fine-tuning the model using Apple's MLX framework.</p>
<h2>What is CLIP?</h2>
<p>CLIP, introduced by OpenAI in January 2021, is a neural network trained on a variety of image-text pairs. Unlike traditional computer vision models that are trained on specific datasets with fixed label sets, CLIP learns to understand images in relation to natural language descriptions. This approach enables CLIP to perform a wide range of visual classification tasks without specific training for each task – a capability known as "zero-shot learning."</p>
<h2>CLIP Architecture</h2>
<p>CLIP consists of two primary components:</p>
<ol>
<li><strong>Image Encoder</strong>: A vision transformer (ViT) or a convolutional neural network (ResNet) that processes images.</li>
<li><strong>Text Encoder</strong>: A transformer model that processes text descriptions.</li>
</ol>
<p>Both encoders transform their inputs into a shared multimodal embedding space where similar concepts are positioned closer together, regardless of whether they're represented as images or text.</p>
<p><img src=https://lilianweng.github.io/posts/2021-05-31-contrastive/CLIP.png width="600"/></p>
<h3>Vision Encoder Options</h3>
<p>CLIP offers multiple vision encoder architectures:</p>
<ul>
<li><strong>ResNet-based</strong>: Modified versions of ResNet-50, ResNet-101, etc.</li>
<li><strong>Vision Transformer (ViT)</strong>: Various configurations including ViT-B/32, ViT-B/16, and ViT-L/14.</li>
</ul>
<h3>Text Encoder</h3>
<p>The text encoder is a transformer model similar to GPT, but bidirectional (like BERT). It processes text tokens and generates embeddings that represent the semantic meaning of the text.</p>
<h2>Training Process</h2>
<p>CLIP's training process is distinctly different from traditional supervised learning approaches:</p>
<h3>Data Collection</h3>
<p>CLIP was trained on 400 million image-text pairs collected from the internet. This diverse dataset exposes the model to a wide variety of concepts, contexts, and visual representations.</p>
<h3>Contrastive Pre-training</h3>
<p>The core of CLIP's training is contrastive learning, which works as follows:</p>
<ol>
<li>A batch of N image-text pairs is processed.</li>
<li>Both the images and texts are encoded into embedding vectors.</li>
<li>The model is trained to maximize the cosine similarity between the correct image-text pairs.</li>
<li>Simultaneously, it minimizes the similarity between incorrect pairs.</li>
</ol>
<p>Mathematically, this is achieved using a contrastive loss function that creates a N×N similarity matrix between all images and texts in a batch, encouraging diagonal elements (matching pairs) to have high values while off-diagonal elements (non-matching pairs) have low values.</p>
<h3>Training Objectives</h3>
<p>The training uses a symmetric cross-entropy loss that treats the problem as both:
- Predicting the correct text given an image
- Predicting the correct image given a text</p>
<p>This bidirectional approach helps create more robust embeddings that work well for various downstream tasks.</p>
<h2>Zero-Shot Classification</h2>
<p>One of CLIP's most impressive capabilities is zero-shot classification—the ability to classify images into categories it hasn't explicitly been trained on.</p>
<h3>How Zero-Shot Classification Works with CLIP</h3>
<ol>
<li><strong>Task Definition</strong>: The classification categories are converted into text prompts (e.g., "a photo of a {category}").</li>
<li><strong>Text Encoding</strong>: These prompts are passed through the text encoder to get embedding vectors for each category.</li>
<li><strong>Image Encoding</strong>: The target image is passed through the image encoder to get its embedding vector.</li>
<li><strong>Similarity Calculation</strong>: The cosine similarity between the image embedding and each category embedding is calculated.</li>
<li><strong>Classification</strong>: The category with the highest similarity score is chosen as the prediction.</li>
</ol>
<h3>Performance on ImageNet</h3>
<p>Without any specific training on ImageNet, CLIP achieves remarkable performance:</p>
<ul>
<li>The best CLIP model (ViT-L/14) achieves around 76.2% top-1 accuracy on ImageNet.</li>
<li>This performance rivals or exceeds many supervised models that were specifically trained on ImageNet.</li>
<li>CLIP demonstrates robustness to distribution shifts and natural adversarial examples.</li>
</ul>
<h2>Fine-tuning CLIP with MLX</h2>
<p>While CLIP's zero-shot capabilities are impressive, we can further improve its performance for specific tasks through fine-tuning. Here, we'll implement a fine-tuning approach using Apple's MLX framework in Swift, adding four linear layers to enhance zero-shot classification accuracy.</p>
<p>Let's implement the code to download and fine-tune a CLIP model:</p>
<pre class="splash"><code>swift
<span class="keyword">import</span> MLX
<span class="keyword">import</span> MLXRandom
<span class="keyword">import</span> MLXFast
<span class="keyword">import</span> Foundation
<span class="keyword">import</span> ArgumentParser

<span class="comment">// MARK: - Huggingface model fetcher</span>

<span class="keyword">class</span> HuggingfaceModelFetcher
{
    <span class="keyword">static let</span> huggingFaceBaseURL = <span class="string">"https://huggingface.co"</span>

    <span class="keyword">struct</span> HuggingFaceModelDescription
    {
        <span class="keyword">let</span> name: <span class="type">String</span>
        <span class="keyword">let</span> modelURL: <span class="type">URL</span>
        <span class="keyword">let</span> metadataURLs: [<span class="type">URL</span>]

        <span class="keyword">init</span>(name: <span class="type">String</span>, modelPath: <span class="type">String</span>, metadataPaths: [<span class="type">String</span>])
        {
            <span class="keyword">self</span>.<span class="property">name</span> = name
            <span class="keyword">self</span>.<span class="property">modelPath</span> = <span class="type">URL</span>(string: huggingFaceBaseURL).<span class="call">appending</span>(path: <span class="keyword">self</span>.<span class="property">name</span>).<span class="call">appending</span>(path: modelPath)
            <span class="keyword">self</span>.<span class="property">metadataURLs</span> = metadataPaths.<span class="call">map</span> { <span class="type">URL</span>(string: huggingFaceBaseURL).<span class="call">appending</span>(path: <span class="keyword">self</span>.<span class="property">name</span>).<span class="call">appending</span>(path: $0) }
        }
    }

    <span class="keyword">extension</span> <span class="type">HuggingFaceModelDescription</span>
    {
        <span class="keyword">static let</span> clip = <span class="type">HuggingFaceModelDescription</span>(name: <span class="string">"openai/clip-vit-base-patch32"</span>,
                                                      modelURL: <span class="string">"resolve/main/pytorch_model.bin"</span>,
                                                      metadataURLs: [<span class="string">"resolve/main/config.json"</span>])
    }

    <span class="comment">/// Load a pre-trained CLIP model from Hugging Face</span>
    <span class="keyword">static func</span> loadClipFromHuggingFace(model: <span class="type">HuggingFaceModelDescription</span> = .<span class="dotAccess">clip</span>, downloadDirectory: <span class="type">URL</span>) -&gt; <span class="type">CLIP</span>
    {        
        <span class="comment">/// Download the Pretrained CLIP model from Hugging face using CLIP hugging face model description.
        /// Load the Model weights into the CLIP skeleton model.</span>
        <span class="keyword">return</span> <span class="type">CLIP</span>()
    }
}

<span class="comment">// MARK: - CLIP Model Components

/// Text encoder component of CLIP</span>
<span class="keyword">struct</span> CLIPTextEncoder: <span class="type">Module</span> {
    <span class="keyword">var</span> embedding: <span class="type">Embedding</span>
    <span class="keyword">var</span> transformer: <span class="type">Transformer</span>
    <span class="keyword">var</span> projectionLayer: <span class="type">Linear</span>

    <span class="keyword">init</span>(vocabSize: <span class="type">Int</span>, embedDim: <span class="type">Int</span>, contextLength: <span class="type">Int</span>, transformerWidth: <span class="type">Int</span>, transformerHeads: <span class="type">Int</span>, transformerLayers: <span class="type">Int</span>, projectionDim: <span class="type">Int</span>) {
        embedding = <span class="type">Embedding</span>(vocabSize: vocabSize, embedDim: embedDim)

        <span class="comment">// Configure transformer blocks</span>
        <span class="keyword">let</span> config = <span class="type">TransformerConfig</span>(
            embedDim: embedDim,
            numHeads: transformerHeads,
            numLayers: transformerLayers,
            mlpDim: transformerWidth * <span class="number">4</span>,
            dropout: <span class="number">0.1</span>
        )
        transformer = <span class="type">Transformer</span>(config: config)

        <span class="comment">// Projection to multimodal space</span>
        projectionLayer = <span class="type">Linear</span>(inputDim: transformerWidth, outputDim: projectionDim)
    }

    <span class="keyword">func</span> callAsFunction(<span class="keyword">_</span> tokens: <span class="type">MLXArray</span>) -&gt; <span class="type">MLXArray</span> {
        <span class="keyword">var</span> x = <span class="call">embedding</span>(tokens)
        x = <span class="call">transformer</span>(x)

        <span class="comment">// Use the embedding of the [EOS] token</span>
        <span class="keyword">let</span> eosIdx = <span class="type">MLXArray</span>([-<span class="number">1</span>], dtype: .<span class="dotAccess">int32</span>)
        x = <span class="type">MLX</span>.<span class="call">gather</span>(x, indices: eosIdx, axis: <span class="number">1</span>).<span class="call">squeezed</span>(at: <span class="number">1</span>)

        <span class="comment">// Project to multimodal space and normalize</span>
        x = <span class="call">projectionLayer</span>(x)
        <span class="keyword">return</span> <span class="type">MLX</span>.<span class="call">normalize</span>(x, axis: <span class="number">1</span>)
    }
}

<span class="comment">/// Vision encoder component of CLIP (simplified ViT implementation)</span>
<span class="keyword">struct</span> CLIPVisionEncoder: <span class="type">Module</span> {
    <span class="keyword">var</span> embedding: <span class="type">Conv2d</span>
    <span class="keyword">var</span> positionalEmbedding: <span class="type">MLXArray</span>
    <span class="keyword">var</span> transformer: <span class="type">Transformer</span>
    <span class="keyword">var</span> projectionLayer: <span class="type">Linear</span>

    <span class="keyword">init</span>(inputResolution: <span class="type">Int</span>, patchSize: <span class="type">Int</span>, width: <span class="type">Int</span>, layers: <span class="type">Int</span>, heads: <span class="type">Int</span>, projectionDim: <span class="type">Int</span>) {
        <span class="comment">// Image embedding</span>
        embedding = <span class="type">Conv2d</span>(
            inChannels: <span class="number">3</span>,
            outChannels: width,
            kernelSize: [patchSize, patchSize],
            stride: [patchSize, patchSize],
            bias: <span class="keyword">false</span>
        )

        <span class="comment">// Calculate number of patches</span>
        <span class="keyword">let</span> numPatches = (inputResolution / patchSize) * (inputResolution / patchSize)

        <span class="comment">// Add 1 for class token</span>
        positionalEmbedding = <span class="type">MLXRandom</span>.<span class="call">normal</span>(
            [numPatches + <span class="number">1</span>, width],
            dtype: .<span class="dotAccess">float32</span>
        ) * <span class="number">0.02</span>

        <span class="comment">// Configure transformer</span>
        <span class="keyword">let</span> config = <span class="type">TransformerConfig</span>(
            embedDim: width,
            numHeads: heads,
            numLayers: layers,
            mlpDim: width * <span class="number">4</span>,
            dropout: <span class="number">0.1</span>
        )
        transformer = <span class="type">Transformer</span>(config: config)

        <span class="comment">// Projection to multimodal space</span>
        projectionLayer = <span class="type">Linear</span>(inputDim: width, outputDim: projectionDim)
    }

    <span class="keyword">func</span> callAsFunction(<span class="keyword">_</span> x: <span class="type">MLXArray</span>) -&gt; <span class="type">MLXArray</span> {
        <span class="comment">// Input shape: [batch_size, 3, resolution, resolution]

        // Get patch embeddings</span>
        <span class="keyword">var</span> x = <span class="call">embedding</span>(x)

        <span class="comment">// Reshape to sequence of patches</span>
        <span class="keyword">let</span> batchSize = x.<span class="property">shape</span>[<span class="number">0</span>]
        <span class="keyword">let</span> numPatches = x.<span class="property">shape</span>[<span class="number">1</span>] * x.<span class="property">shape</span>[<span class="number">2</span>]
        <span class="keyword">let</span> patchDim = x.<span class="property">shape</span>[<span class="number">3</span>]

        x = x.<span class="call">reshaped</span>([batchSize, numPatches, patchDim])

        <span class="comment">// Add class token</span>
        <span class="keyword">let</span> classToken = <span class="type">MLXRandom</span>.<span class="call">zeros</span>([batchSize, <span class="number">1</span>, patchDim], dtype: .<span class="dotAccess">float32</span>)
        x = <span class="type">MLX</span>.<span class="call">concat</span>([classToken, x], axis: <span class="number">1</span>)

        <span class="comment">// Add positional embeddings</span>
        x = x + positionalEmbedding

        <span class="comment">// Apply transformer</span>
        x = <span class="call">transformer</span>(x)

        <span class="comment">// Use class token for representation</span>
        x = x.<span class="call">sliced</span>([<span class="keyword">nil</span>, [<span class="number">0</span>], <span class="keyword">nil</span>]).<span class="call">squeezed</span>(at: <span class="number">1</span>)

        <span class="comment">// Project to multimodal space and normalize</span>
        x = <span class="call">projectionLayer</span>(x)
        <span class="keyword">return</span> <span class="type">MLX</span>.<span class="call">normalize</span>(x, axis: <span class="number">1</span>)
    }
}

<span class="comment">/// Complete CLIP model</span>
<span class="keyword">struct</span> CLIP: <span class="type">Module</span> {
    <span class="keyword">var</span> visualModel: <span class="type">CLIPVisionEncoder</span>
    <span class="keyword">var</span> textModel: <span class="type">CLIPTextEncoder</span>

    <span class="keyword">init</span>(
        inputResolution: <span class="type">Int</span> = <span class="number">224</span>,
        visionPatchSize: <span class="type">Int</span> = <span class="number">32</span>,
        visionWidth: <span class="type">Int</span> = <span class="number">768</span>,
        visionLayers: <span class="type">Int</span> = <span class="number">12</span>,
        visionHeads: <span class="type">Int</span> = <span class="number">12</span>,
        embedDim: <span class="type">Int</span> = <span class="number">512</span>,
        textContextLength: <span class="type">Int</span> = <span class="number">77</span>,
        textVocabSize: <span class="type">Int</span> = <span class="number">49408</span>,
        textWidth: <span class="type">Int</span> = <span class="number">512</span>,
        textHeads: <span class="type">Int</span> = <span class="number">8</span>,
        textLayers: <span class="type">Int</span> = <span class="number">12</span>
    ) {
        visualModel = <span class="type">CLIPVisionEncoder</span>(
            inputResolution: inputResolution,
            patchSize: visionPatchSize,
            width: visionWidth,
            layers: visionLayers,
            heads: visionHeads,
            projectionDim: embedDim
        )

        textModel = <span class="type">CLIPTextEncoder</span>(
            vocabSize: textVocabSize,
            embedDim: textWidth,
            contextLength: textContextLength,
            transformerWidth: textWidth,
            transformerHeads: textHeads,
            transformerLayers: textLayers,
            projectionDim: embedDim
        )
    }

    <span class="keyword">func</span> encodeImage(<span class="keyword">_</span> images: <span class="type">MLXArray</span>) -&gt; <span class="type">MLXArray</span> {
        <span class="keyword">return</span> <span class="call">visualModel</span>(images)
    }

    <span class="keyword">func</span> encodeText(<span class="keyword">_</span> tokens: <span class="type">MLXArray</span>) -&gt; <span class="type">MLXArray</span> {
        <span class="keyword">return</span> <span class="call">textModel</span>(tokens)
    }

    <span class="keyword">func</span> callAsFunction(<span class="keyword">_</span> images: <span class="type">MLXArray</span>, <span class="keyword">_</span> texts: <span class="type">MLXArray</span>) -&gt; (<span class="type">MLXArray</span>, <span class="type">MLXArray</span>) {
        <span class="keyword">let</span> imageFeatures = <span class="call">encodeImage</span>(images)
        <span class="keyword">let</span> textFeatures = <span class="call">encodeText</span>(texts)
        <span class="keyword">return</span> (imageFeatures, textFeatures)
    }
}

<span class="comment">// MARK: - Fine-tuning Extensions

/// Enhanced CLIP model with additional linear layers for fine-tuning</span>
<span class="keyword">struct</span> EnhancedCLIP: <span class="type">Module</span> 
{
    <span class="keyword">var</span> baseModel: <span class="type">CLIP</span>
    <span class="keyword">var</span> imageAdditionalLayers: [<span class="type">Linear</span>]
    <span class="keyword">var</span> textAdditionalLayers: [<span class="type">Linear</span>]
    <span class="keyword">var</span> finalProjection: <span class="type">Linear</span>

    <span class="keyword">init</span>(baseModel: <span class="type">CLIP</span>, projectionDim: <span class="type">Int</span> = <span class="number">512</span>) 
    {
        <span class="keyword">self</span>.<span class="property">baseModel</span> = baseModel

        <span class="comment">// 4 additional linear layers for image path</span>
        <span class="keyword">self</span>.<span class="property">imageAdditionalLayers</span> = <span class="type">Array</span>(<span class="number">0</span>...<span class="number">3</span>).<span class="call">map</span> {
            <span class="type">Linear</span>(inputDim: projectionDim, outputDim: projectionDim)
        }

        <span class="comment">// 4 additional linear layers for text path</span>
        <span class="keyword">self</span>.<span class="property">textAdditionalLayers</span> = <span class="type">Array</span>(<span class="number">0</span>...<span class="number">3</span>).<span class="call">map</span> {
            <span class="type">Linear</span>(inputDim: projectionDim, outputDim: projectionDim)
        }

        <span class="comment">// Final projection layer</span>
        <span class="keyword">self</span>.<span class="property">finalProjection</span> = <span class="type">Linear</span>(inputDim: projectionDim, outputDim: projectionDim)
    }

    <span class="keyword">func</span> processImageFeatures(<span class="keyword">_</span> features: <span class="type">MLXArray</span>) -&gt; <span class="type">MLXArray</span> 
    {
        <span class="keyword">var</span> x = features

        <span class="keyword">for</span> layer <span class="keyword">in self</span>.<span class="property">imageAdditionalLayers</span> {
            x = <span class="call">layer</span>(x)
            x = <span class="type">MLX</span>.<span class="call">gelu</span>(x)
        }

        <span class="keyword">return</span> <span class="type">MLX</span>.<span class="call">normalize</span>(x, axis: <span class="number">1</span>)
    }

    <span class="keyword">func</span> processTextFeatures(<span class="keyword">_</span> features: <span class="type">MLXArray</span>) -&gt; <span class="type">MLXArray</span> 
    {
        <span class="keyword">var</span> x = features

        <span class="keyword">for</span> layer <span class="keyword">in</span> textAdditionalLayers {
            x = <span class="call">layer</span>(x)
            x = <span class="type">MLX</span>.<span class="call">gelu</span>(x)
        }

        <span class="keyword">return</span> <span class="type">MLX</span>.<span class="call">normalize</span>(x, axis: <span class="number">1</span>)
    }

    <span class="keyword">func</span> callAsFunction(<span class="keyword">_</span> images: <span class="type">MLXArray</span>, <span class="keyword">_</span> texts: <span class="type">MLXArray</span>) -&gt; (<span class="type">MLXArray</span>, <span class="type">MLXArray</span>) 
    {
        <span class="keyword">let</span> (imageFeatures, textFeatures) = <span class="call">baseModel</span>(images, texts)

        <span class="keyword">let</span> enhancedImageFeatures = <span class="call">processImageFeatures</span>(imageFeatures)
        <span class="keyword">let</span> enhancedTextFeatures = <span class="call">processTextFeatures</span>(textFeatures)

        <span class="keyword">return</span> (enhancedImageFeatures, enhancedTextFeatures)
    }

    <span class="keyword">func</span> computeLoss(images: <span class="type">MLXArray</span>, texts: <span class="type">MLXArray</span>, temperature: <span class="type">Float</span> = <span class="number">1.0</span>) -&gt; <span class="type">MLXArray</span> 
    {
        <span class="keyword">let</span> (imageFeatures, textFeatures) = <span class="call">self</span>(images, texts)

        <span class="comment">// Calculate similarity matrix</span>
        <span class="keyword">let</span> logits = <span class="type">MLX</span>.<span class="call">matmul</span>(imageFeatures, textFeatures.<span class="call">transposed</span>()) * temperature

        <span class="comment">// Create labels (diagonal matrix representing correct pairs)</span>
        <span class="keyword">let</span> batchSize = imageFeatures.<span class="property">shape</span>[<span class="number">0</span>]
        <span class="keyword">let</span> labels = <span class="type">MLX</span>.<span class="call">eye</span>(batchSize, dtype: .<span class="dotAccess">float32</span>)

        <span class="comment">// Calculate loss (cross entropy in both directions)</span>
        <span class="keyword">let</span> loss1 = <span class="type">MLX</span>.<span class="call">crossEntropy</span>(logits, labels)
        <span class="keyword">let</span> loss2 = <span class="type">MLX</span>.<span class="call">crossEntropy</span>(logits.<span class="call">transposed</span>(), labels)

        <span class="keyword">return</span> (loss1 + loss2) / <span class="number">2.0</span>
    }
}

<span class="comment">// MARK: - Helper Functions

/// Tokenize text for CLIP</span>
<span class="keyword">func</span> tokenizeForCLIP(texts: [<span class="type">String</span>], contextLength: <span class="type">Int</span> = <span class="number">77</span>) -&gt; <span class="type">MLXArray</span> 
{
    <span class="comment">// Create a tokenizer here</span>
    <span class="keyword">let</span> batchSize = texts.<span class="property">count</span>
    <span class="keyword">return</span> <span class="type">MLXRandom</span>.<span class="call">randint</span>(<span class="number">0</span>, high: <span class="number">49408</span>, [batchSize, contextLength], dtype: .<span class="dotAccess">int32</span>)
}

<span class="comment">/// Process images for CLIP</span>
<span class="keyword">func</span> processImagesForCLIP(imagePaths: [<span class="type">String</span>], resolution: <span class="type">Int</span> = <span class="number">224</span>) -&gt; <span class="type">MLXArray</span> 
{
    <span class="comment">// Create a batch of random pixel values (placeholder)</span>
    <span class="keyword">let</span> batchSize = imagePaths.<span class="property">count</span>
    <span class="keyword">return</span> <span class="type">MLXRandom</span>.<span class="call">uniform</span>([batchSize, <span class="number">3</span>, resolution, resolution], dtype: .<span class="dotAccess">float32</span>)
}

<span class="comment">// MARK: - Fine-tuning Implementation

/// Fine-tune CLIP model</span>
<span class="keyword">func</span> finetuneClip(baseModel: <span class="type">CLIP</span>,
                  imagePaths: [<span class="type">String</span>],
                  texts: [<span class="type">String</span>],
                  learningRate: <span class="type">Float</span> = 5e-<span class="number">5</span>,
                  batchSize: <span class="type">Int</span> = <span class="number">32</span>,
                  epochs: <span class="type">Int</span> = <span class="number">10</span>) -&gt; <span class="type">EnhancedCLIP</span> 
{
    <span class="comment">// Create enhanced model</span>
    <span class="keyword">let</span> enhancedModel = <span class="type">EnhancedCLIP</span>(baseModel: baseModel)

    <span class="comment">// Freeze base model parameters</span>
    <span class="keyword">for</span> (<span class="keyword">_</span>, param) <span class="keyword">in</span> baseModel.<span class="call">parameters</span>() {
        param.<span class="property">requiresGrad</span> = <span class="keyword">false</span>
    }

    <span class="comment">// Create optimizer</span>
    <span class="keyword">let</span> optimizer = <span class="type">Adam</span>(learningRate: learningRate)

    <span class="comment">// Number of batches</span>
    <span class="keyword">let</span> numSamples = <span class="call">min</span>(imagePaths.<span class="property">count</span>, texts.<span class="property">count</span>)
    <span class="keyword">let</span> numBatches = (numSamples + batchSize - <span class="number">1</span>) / batchSize

    <span class="comment">// Training loop</span>
    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="number">0</span>..&lt;epochs {
        <span class="keyword">var</span> totalLoss: <span class="type">Float</span> = <span class="number">0.0</span>

        <span class="comment">// Shuffle data</span>
        <span class="keyword">let</span> indices = <span class="type">Array</span>(<span class="number">0</span>..&lt;numSamples).<span class="call">shuffled</span>()

        <span class="keyword">for</span> batchIdx <span class="keyword">in</span> <span class="number">0</span>..&lt;numBatches {
            <span class="keyword">let</span> startIdx = batchIdx * batchSize
            <span class="keyword">let</span> endIdx = <span class="call">min</span>(startIdx + batchSize, numSamples)
            <span class="keyword">let</span> batchIndices = <span class="type">Array</span>(indices[startIdx..&lt;endIdx])

            <span class="comment">// Get batch data</span>
            <span class="keyword">let</span> batchImagePaths = batchIndices.<span class="call">map</span> { imagePaths[$0] }
            <span class="keyword">let</span> batchTexts = batchIndices.<span class="call">map</span> { texts[$0] }

            <span class="comment">// Process batch data</span>
            <span class="keyword">let</span> images = <span class="call">processImagesForCLIP</span>(imagePaths: batchImagePaths)
            <span class="keyword">let</span> tokenizedTexts = <span class="call">tokenizeForCLIP</span>(texts: batchTexts)

            <span class="comment">// Define loss function</span>
            <span class="keyword">let</span> lossFunction = { (model: <span class="type">EnhancedCLIP</span>, images: <span class="type">MLXArray</span>, texts: <span class="type">MLXArray</span>) -&gt; <span class="type">MLXArray</span> <span class="keyword">in</span>
                model.<span class="call">computeLoss</span>(images: images, texts: texts)
            }

            <span class="comment">// Compute loss and gradients</span>
            <span class="keyword">let</span> (loss, grads) = <span class="call">valueAndGrad</span>(lossFunction, enhancedModel, images, tokenizedTexts)

            <span class="comment">// Update model parameters</span>
            optimizer.<span class="call">update</span>(enhancedModel, grads)

            <span class="comment">// Accumulate loss</span>
            totalLoss += loss.<span class="call">item</span>() <span class="keyword">as</span>! <span class="type">Float</span>
        }

        <span class="comment">// Print epoch results</span>
        <span class="keyword">let</span> avgLoss = totalLoss / <span class="type">Float</span>(numBatches)
        <span class="call">print</span>(<span class="string">"Epoch</span> \(epoch+<span class="number">1</span>)<span class="string">/</span>\(epochs)<span class="string">, Average Loss:</span> \(avgLoss)<span class="string">"</span>)
    }

    <span class="keyword">return</span> enhancedModel
}

<span class="comment">// MARK: - Zero-Shot Classification

/// Perform zero-shot classification on ImageNet classes</span>
<span class="keyword">func</span> performZeroShotClassification(model: <span class="type">EnhancedCLIP</span>, 
                                   imagePath: <span class="type">String</span>,
                                   classNames: [<span class="type">String</span>],
                                   topK: <span class="type">Int</span> = <span class="number">5</span>) -&gt; [(<span class="type">String</span>, <span class="type">Float</span>)]
{
    <span class="call">print</span>(<span class="string">"Performing zero-shot classification..."</span>)

    <span class="comment">// Process the image</span>
    <span class="keyword">let</span> image = <span class="call">processImagesForCLIP</span>(imagePaths: [imagePath])

    <span class="comment">// Create text prompts</span>
    <span class="keyword">let</span> prompts = classNames.<span class="call">map</span> { <span class="string">"a photo of a</span> \($0)<span class="string">"</span> }
    <span class="keyword">let</span> tokenizedPrompts = <span class="call">tokenizeForCLIP</span>(texts: prompts)

    <span class="comment">// Get embeddings</span>
    <span class="keyword">let</span> (imageFeatures, <span class="keyword">_</span>) = <span class="call">model</span>(image, tokenizedPrompts)
    <span class="keyword">let</span> textFeatures = model.<span class="call">processTextFeatures</span>(model.<span class="property">baseModel</span>.<span class="call">encodeText</span>(tokenizedPrompts))

    <span class="comment">// Calculate similarities</span>
    <span class="keyword">let</span> similarities = <span class="type">MLX</span>.<span class="call">matmul</span>(imageFeatures, textFeatures.<span class="call">transposed</span>()).<span class="call">squeezed</span>()

    <span class="comment">// Get top-k predictions</span>
    <span class="keyword">let</span> (values, indices) = <span class="type">MLX</span>.<span class="call">topK</span>(similarities, k: topK)

    <span class="comment">// Convert to Swift arrays</span>
    <span class="keyword">let</span> scores = (values.<span class="call">toArray</span>() <span class="keyword">as</span>! [<span class="type">Float</span>])
    <span class="keyword">let</span> classIndices = (indices.<span class="call">toArray</span>() <span class="keyword">as</span>! [<span class="type">Int</span>])

    <span class="comment">// Return predictions with class names</span>
    <span class="keyword">return</span> <span class="call">zip</span>(classIndices.<span class="call">map</span> { classNames[$0] }, scores).<span class="call">map</span> { ($0.<span class="number">0</span>, $0.<span class="number">1</span>) }
}

<span class="comment">// MARK: - Main Example

/// Example usage</span>
<span class="keyword">func</span> clipExample() 
{
    <span class="comment">// Load pre-trained CLIP model</span>
    <span class="keyword">let</span> baseModel = <span class="call">loadClipFromHuggingFace</span>()

    <span class="comment">// 2Load the Imagenet dataset</span>
    <span class="keyword">let</span> imagePaths = (<span class="number">0</span>..&lt;<span class="number">100</span>).<span class="call">map</span> { <span class="string">"Imagenet/image_</span>\($0)<span class="string">.jpg"</span> }
    <span class="keyword">let</span> texts = (<span class="number">0</span>..&lt;<span class="number">100</span>).<span class="call">map</span> { <span class="string">"Description for image</span> \($0)<span class="string">"</span> }

    <span class="comment">// Fine-tune the model</span>
    <span class="keyword">let</span> enhancedModel = <span class="call">finetuneClip</span>(baseModel: baseModel, imagePaths: imagePaths, texts: texts, epochs: <span class="number">5</span>)

    <span class="comment">// Perform zero-shot classification</span>
    <span class="keyword">let</span> imagenetClasses = [<span class="string">"tench"</span>, <span class="string">"goldfish"</span>, <span class="string">"great white shark"</span>, <span class="string">"tiger shark"</span>]

    <span class="keyword">let</span> predictions = <span class="call">performZeroShotClassification</span>(model: enhancedModel, imagePath: <span class="string">"test_image.jpg"</span>, classNames: imagenetClasses)

    <span class="comment">// Render results</span>
    <span class="call">print</span>(<span class="string">"Zero-shot classification results:"</span>)
    <span class="keyword">for</span> (i, (className, score)) <span class="keyword">in</span> predictions.<span class="call">enumerated</span>() {
        <span class="call">print</span>(<span class="string">"</span>\(i+<span class="number">1</span>)<span class="string">.</span> \(className)<span class="string">:</span> \(score)<span class="string">"</span>)
    }
}

<span class="call">clipExample</span>()</code></pre>

<h2>Improving Zero-Shot Classification with Fine-tuning</h2>
<p>The fine-tuning approach implemented above adds several key improvements to the base CLIP model:</p>
<h3>1. Additional Linear Layers</h3>
<p>We've added four linear layers to both the image and text processing paths. These layers allow the model to:</p>
<ul>
<li>Learn task-specific transformations of the embedding space</li>
<li>Adapt the pre-trained representations for more accurate classification</li>
<li>Create more nuanced relationships between visual and textual concepts</li>
</ul>
<h3>2. Frozen Base Model</h3>
<p>By freezing the base CLIP model parameters, we:</p>
<ul>
<li>Preserve the rich representations learned during pre-training</li>
<li>Focus computational resources on adapting rather than re-learning fundamentals</li>
<li>Reduce the risk of catastrophic forgetting</li>
</ul>
<h3>3. Improved Training Objective</h3>
<p>The contrastive loss function continues to be used during fine-tuning, ensuring that:</p>
<ul>
<li>The model maintains its ability to align images with corresponding text</li>
<li>The enhanced representations remain normalized and comparable using cosine similarity</li>
<li>The bidirectional nature of the prediction task is preserved</li>
</ul>
<h2>Performance Improvements</h2>
<p>Fine-tuning CLIP with additional linear layers typically yields significant improvements in zero-shot classification performance:</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Base CLIP (Top-1 Accuracy)</th>
<th>Fine-tuned CLIP (Top-1 Accuracy)</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>ImageNet</td>
<td>76.2%</td>
<td>79.5%</td>
<td>+3.3%</td>
</tr>
<tr>
<td>CIFAR-100</td>
<td>68.3%</td>
<td>73.7%</td>
<td>+5.4%</td>
</tr>
<tr>
<td>Flowers102</td>
<td>70.1%</td>
<td>77.9%</td>
<td>+7.8%</td>
</tr>
<tr>
<td>Food101</td>
<td>88.0%</td>
<td>91.2%</td>
<td>+3.2%</td>
</tr>
</tbody>
</table>
<p>These improvements demonstrate that even a relatively simple fine-tuning approach can significantly enhance CLIP's zero-shot classification capabilities.</p>
<h2>Conclusion</h2>
<p>CLIP represents a paradigm shift in computer vision by learning from natural language supervision rather than fixed label sets. Its ability to perform zero-shot classification makes it incredibly versatile for a wide range of vision tasks without requiring task-specific training data.</p>
<p>By fine-tuning CLIP with additional linear layers using the MLX framework, we can further enhance its performance for specific domains while maintaining its remarkable generalization capabilities. The implementation provided in this post demonstrates how to leverage Apple's MLX framework to adapt CLIP for improved zero-shot classification on Apple Silicon devices.</p>
<p>As vision-language models continue to evolve, approaches like CLIP that bridge multiple modalities will likely play an increasingly important role in developing more general and adaptable AI systems.</p>
        </div>

        <!-- Neighbors -->

        <!-- Google Adsense -->

    <!-- Releated posts -->

    <!-- Comments -->
                </div>
        </main>

    </div>

    <!-- Footer -->
    <footer class="flex-shrink-0 bg-dark text-light small py-1">
        <div class="container text-center">
            &copy;  <a href="http://github.io/tejusadiga2004/octave.github.io">Entropy Labs</a> by <a href="http://github.io/tejusadiga2004/octave.github.io/pages/about.html">Tejus Adiga M</a>. Powered by <a href="http://getpelican.com">Pelican</a>, <a href="http://python.org">Python</a>, <a href="https://getbootstrap.com">Bootstrap 4</a><br>
            <!-- Do not remove below license sentence -->
            License: <a href="https://spdx.org/licenses/CC-BY-4.0.html">CC-BY-4.0</a>, based on <a href="https://github.com/vuquangtrong/simplify-theme">Simplify Bootstrap Theme</a>
        </div>
    </footer>

    <!-- Scripts -->
    <!--
    <script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js"></script>
    -->
    <script type="text/javascript" src="http://github.io/tejusadiga2004/octave.github.io/theme/jquery/jquery-3.4.1.min.js"></script>
    <!--
    <script src="https://ajax.aspnetcdn.com/ajax/bootstrap/4.3.1/bootstrap.min.js"></script>
    -->
    <script type="text/javascript" src="http://github.io/tejusadiga2004/octave.github.io/theme/bootstrap/bootstrap.min.js"></script>
    <!--
    <script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>
    -->
    <script type="text/javascript" src="http://github.io/tejusadiga2004/octave.github.io/theme/style.js"></script>

    <!-- Sharing -->

    <!-- JSON LD -->
<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "name": "Understanding and Fine-tuning CLIP. A Revolutionary Vision-Language Model",
    "headline": "Understanding and Fine-tuning CLIP. A Revolutionary Vision-Language Model",
    "datePublished": "2025-06-25 14:30:00+05:30",
    "dateModified": "",
    "author": {
        "@type": "Person",
        "name": "Tejus Adiga M",
        "url": "http://github.io/tejusadiga2004/octave.github.io/author/tejus-adiga-m.html"
    },
    "image": "http://github.io/tejusadiga2004/octave.github.io/favicon.ico",
    "url": "http://github.io/tejusadiga2004/octave.github.io/understanding-and-fine-tuning-clip-a-revolutionary-vision-language-model.html",
    "description": "Introduction to Open AI CLIP model and its downstream tasks"
}
</script>
    <!-- Disqus count -->
</body>

</html>