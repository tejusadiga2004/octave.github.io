<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Entropy Pages - Deep Learning</title><link href="https://blogs.entropypages.in/" rel="alternate"/><link href="https://blogs.entropypages.in/feeds/deep-learning.atom.xml" rel="self"/><id>https://blogs.entropypages.in/</id><updated>2025-08-27T00:00:00+05:30</updated><entry><title>GPT-OSS: OpenAI’s Frontier-Grade Open-Weight Models</title><link href="https://blogs.entropypages.in/gpt-oss-openai-models.html" rel="alternate"/><published>2025-08-27T00:00:00+05:30</published><updated>2025-08-27T00:00:00+05:30</updated><author><name>Tejus Adiga M</name></author><id>tag:blogs.entropypages.in,2025-08-27:/gpt-oss-openai-models.html</id><summary type="html">&lt;p&gt;A deep dive into OpenAI's GPT-OSS models—GPT-OSS-120B and GPT-OSS-20B—covering architecture, pretraining, performance, and agentic capabilities.&lt;/p&gt;</summary><content type="html">&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Yesterday, August 5, 2025 Open AI droped its opensource open weight GPT model gpt-oss to the world. It comes with two configurations. GPT-OSS-120B targetted for server run large scale reasoning and GPT-OSS-20B targetted for ondevice reasoning and generation.
These models are designed to deliver high-performance reasoning, agentic capabilities, and real-world usability—all while being fully customizable and deployable on consumer-grade hardware. These models are licenced under Apache 2.0 license. OpenAI claims these models outperforms similarly sized open modoels on reasoning capabilities and toolchain use capabilities.&lt;/p&gt;
&lt;h2 id="pretraining-methodology"&gt;Pretraining Methodology&lt;/h2&gt;
&lt;p&gt;The GPT-OSS models were trained using a mix of reinforcement learning and techniques informed by OpenAI’s most advanced internal models, including o3 and other frontier systems. Training techniques focused perticularly on reasoning, efficiency and real world usability.&lt;/p&gt;
&lt;p&gt;The models were trained on NVIDIA H100 GPUs using PyTorch with optimized Triton kernels.&lt;/p&gt;
&lt;h3 id="dataset-composition"&gt;Dataset Composition&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Language: Primarily English&lt;/li&gt;
&lt;li&gt;Content Focus: STEM, coding, general knowledge&lt;/li&gt;
&lt;li&gt;Tokenization: Superset tokenizer &lt;code&gt;o200k_harmony&lt;/code&gt;, derived from GPT-4o and o4-mini&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="architecture"&gt;Architecture&lt;/h3&gt;
&lt;p&gt;Both models use Mixture-of-Experts (MoE) Transformer architecture. They use [Flash Attention algorithm(https://arxiv.org/pdf/2205.14135) to reduce the memory requirements and accelarate training. Reference of Flash Attention can be found here. &lt;a href="https://github.com/Dao-AILab/flash-attention"&gt;Flash attention&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mixture-of-Experts&lt;/strong&gt;: Each MoE block consists of a fixed number of experts (128 for gpt-oss 120b and 32 for gpt-oss-20b), as well as a standard linear router projection which maps residual activations to scores for each expert. For both models, we select the top-4 experts for each token
given by the router, and weight the output of each expert by the softmax of the router projection over only the selected experts. The MoE blocks use the gated &lt;a href="https://arxiv.org/pdf/2002.05202"&gt;SwiGLU activation&lt;/a&gt; function.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Attention&lt;/strong&gt;: Following GPT-3, attention blocks alternate between banded window and fullydense patterns, where the bandwidth is 128 tokens. Each layer has 64 query heads of dimension 64, and uses [Grouped Query Attention(https://arxiv.org/pdf/2305.13245) with 8 key-value heads. The embeddings used is &lt;a href="https://arxiv.org/pdf/2104.09864"&gt;rotary position embeddings&lt;/a&gt; and extend the context length of dense layers to 131,072
tokens using &lt;a href="YaRN: Efficient context window extension of large language models"&gt;YaRN&lt;/a&gt;. Each attention head has a learned bias in the denominator of the softmax, similar to off-by-one attention and attention sinks, which enables the attention
mechanism to pay no attention to any tokens.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tokenizer&lt;/strong&gt;: Across all stages o200k_harmony tokenizer is used from &lt;a href="https://github.com/openai/tiktoken"&gt;TikToken&lt;/a&gt; library. It is a Byte Pair Encoding (BPE) and has total of 201,088 tokens.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Normalization method&lt;/strong&gt;: Both models have a residual stream dimension of 2880, applying &lt;a href="https://arxiv.org/pdf/1910.07467"&gt;root mean square normalization&lt;/a&gt; on the activations before each attention and MoE block. Similar to GPT-2 we use Pre-LN placement.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Layers&lt;/th&gt;
&lt;th&gt;Total Params&lt;/th&gt;
&lt;th&gt;Active Params/Token&lt;/th&gt;
&lt;th&gt;Experts/Layer&lt;/th&gt;
&lt;th&gt;Active Experts&lt;/th&gt;
&lt;th&gt;Context Length&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;GPT-OSS-120B&lt;/td&gt;
&lt;td&gt;36&lt;/td&gt;
&lt;td&gt;117B&lt;/td&gt;
&lt;td&gt;5.1B&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;128k tokens&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GPT-OSS-20B&lt;/td&gt;
&lt;td&gt;24&lt;/td&gt;
&lt;td&gt;21B&lt;/td&gt;
&lt;td&gt;3.6B&lt;/td&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;128k tokens&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Quantization:&lt;/strong&gt;: In the post training stage models are quantized to &lt;a href="https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf"&gt;MXFP4 format&lt;/a&gt; where MoE weights are optimized to 4.25 bits per parameter. This enables the 120b model varient to fit into 80GB GPU while 20b model varient can fit into 16GB GPU memory.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Additional features:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rotary Positional Embeddings (RoPE)&lt;/li&gt;
&lt;li&gt;Grouped Multi-Query Attention (GQA) with group size 8&lt;/li&gt;
&lt;li&gt;Alternating dense and locally banded sparse attention&lt;/li&gt;
&lt;li&gt;Gated SwiGLU activations&lt;/li&gt;
&lt;li&gt;MXFP4 quantization for efficient inference&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The model card can be viewed here. &lt;a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf"&gt;Model card&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="post-training"&gt;Post-Training&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Supervised fine-tuning aligned with OpenAI’s Model Spec&lt;/li&gt;
&lt;li&gt;High-compute Reinforcement Learning (RL) for Chain-of-Thought reasoning, tool use, and structured outputs&lt;/li&gt;
&lt;li&gt;No direct supervision on CoT reasoning&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="performance-highlights"&gt;Performance Highlights&lt;/h2&gt;
&lt;h3 id="gpt-oss-120b"&gt;GPT-OSS-120B&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Matches or exceeds GPT-4o-mini on AIME, MMLU, HealthBench&lt;/li&gt;
&lt;li&gt;Efficient long-context reasoning&lt;/li&gt;
&lt;li&gt;Tool use and structured output capabilities&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="gpt-oss-20b"&gt;GPT-OSS-20B&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Comparable to GPT-3.5-class models&lt;/li&gt;
&lt;li&gt;Edge-device friendly&lt;/li&gt;
&lt;li&gt;Strong performance on math and health tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="hallucinations-and-limitations"&gt;Hallucinations and Limitations&lt;/h2&gt;
&lt;p&gt;Despite their impressive capabilities, GPT-OSS models exhibit hallucination patterns typical of large language models, with some notable characteristics:&lt;/p&gt;
&lt;h3 id="common-hallucination-types"&gt;Common Hallucination Types&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Factual Inaccuracies&lt;/strong&gt;: Both models can generate plausible-sounding but incorrect information, particularly when asked about:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Recent events (post-training cutoff)&lt;/li&gt;
&lt;li&gt;Obscure historical facts or technical details&lt;/li&gt;
&lt;li&gt;Specific statistics without proper sourcing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Mathematical Reasoning Errors&lt;/strong&gt;: While generally strong at math, the models can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make computational errors in multi-step problems&lt;/li&gt;
&lt;li&gt;Misapply formulas or theorems&lt;/li&gt;
&lt;li&gt;Generate incorrect proofs that appear logically structured&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Code Generation Issues&lt;/strong&gt;: In programming tasks, hallucinations manifest as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Non-existent library functions or methods&lt;/li&gt;
&lt;li&gt;Incorrect API usage patterns&lt;/li&gt;
&lt;li&gt;Syntactically correct but functionally flawed code&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="mitigation-strategies"&gt;Mitigation Strategies&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Chain-of-Thought Reasoning&lt;/strong&gt;: The RL training for CoT helps reduce hallucinations by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Encouraging step-by-step verification&lt;/li&gt;
&lt;li&gt;Making reasoning processes more transparent&lt;/li&gt;
&lt;li&gt;Allowing users to identify potential errors in logic&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Tool Integration&lt;/strong&gt;: The models' tool-use capabilities help mitigate hallucinations through:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Real-time web search for current information&lt;/li&gt;
&lt;li&gt;Code execution environments for verification&lt;/li&gt;
&lt;li&gt;Structured output formats that enforce consistency&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Model Size Impact&lt;/strong&gt;: GPT-OSS-120B generally exhibits fewer hallucinations than GPT-OSS-20B due to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Larger parameter count enabling better factual recall&lt;/li&gt;
&lt;li&gt;More sophisticated reasoning patterns from increased model capacity&lt;/li&gt;
&lt;li&gt;Better calibration of uncertainty in responses&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="best-practices-for-users"&gt;Best Practices for Users&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Verify Critical Information&lt;/strong&gt;: Always cross-check important facts, especially for high-stakes decisions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Use Tool Integration&lt;/strong&gt;: Leverage the models' ability to search and execute code for verification&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Request Step-by-Step Reasoning&lt;/strong&gt;: Ask for detailed explanations to identify potential logical errors&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Provide Context&lt;/strong&gt;: Give relevant background information to reduce ambiguity&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Set Appropriate Expectations&lt;/strong&gt;: Understand that these models are probabilistic and can make mistakes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;OpenAI acknowledges these limitations and continues research into hallucination reduction techniques, including improved training methodologies and better uncertainty quantification.&lt;/p&gt;
&lt;h2 id="agentic-capabilities"&gt;Agentic Capabilities&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Tool use: Python execution, web browsing&lt;/li&gt;
&lt;li&gt;Structured outputs: JSON, function calls&lt;/li&gt;
&lt;li&gt;Harmony response format&lt;/li&gt;
&lt;li&gt;Reasoning effort configuration&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="running-this-on-device-and-expected-performance"&gt;Running This On Device and Expected Performance&lt;/h2&gt;
&lt;p&gt;GPT-OSS models are designed for flexible deployment across a range of hardware, from consumer laptops to enterprise servers and edge devices. Here’s what to expect:&lt;/p&gt;
&lt;h3 id="gpt-oss-120b-serverworkstation"&gt;GPT-OSS-120B (Server/Workstation)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Minimum Hardware&lt;/strong&gt;: 80GB GPU (NVIDIA A100/H100) or 160GB system RAM&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Real-time inference for most tasks on high-end GPUs; batch processing recommended for large workloads&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Latency&lt;/strong&gt;: ~1-2 seconds per response for typical prompts; can be reduced with quantization and model sharding&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Throughput&lt;/strong&gt;: Hundreds of requests per minute with multi-GPU setups&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="gpt-oss-20b-edgeconsumer"&gt;GPT-OSS-20B (Edge/Consumer)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Minimum Hardware&lt;/strong&gt;: 16GB GPU or 32GB system RAM&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Near real-time responses on consumer GPUs (RTX 4080/4090, Apple M2/M3 Ultra)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Latency&lt;/strong&gt;: ~1 second per response for most prompts&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile/Edge&lt;/strong&gt;: Deployable on iOS (Core ML), Android (ONNX/TFLite), and edge TPUs&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="running-on-macos-hardware"&gt;Running on macOS Hardware&lt;/h2&gt;
&lt;p&gt;Apple Silicon (M1/M2/M3) offers a robust platform for running GPT-OSS-20B and smaller models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Unified Memory&lt;/strong&gt;: Efficient sharing between CPU/GPU/ANE&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: GPT-OSS-20B runs at near real-time speeds on M2/M3 Ultra (16-32GB RAM); M1 can handle smaller models or quantized variants&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Acceleration&lt;/strong&gt;: Metal Performance Shaders and MLX library provide hardware-optimized tensor operations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt;: Use Core ML conversion for best results; ONNX Runtime also supported&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Energy Efficiency&lt;/strong&gt;: Apple Silicon delivers high performance per watt, making it ideal for continuous inference workloads&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="example-running-gpt-oss-20b-on-macbook-pro-m2-ultra"&gt;Example: Running GPT-OSS-20B on MacBook Pro (M2 Ultra)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Convert model to Core ML format using &lt;code&gt;coremltools&lt;/code&gt; or MLX&lt;/li&gt;
&lt;li&gt;Load model in a Swift or Python app using Core ML APIs&lt;/li&gt;
&lt;li&gt;Expect response times of ~1 second for typical prompts&lt;/li&gt;
&lt;li&gt;For best results, use quantized weights and batch requests&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id="python-example-inference-on-macos-apple-silicon"&gt;Python Example: Inference on macOS (Apple Silicon)&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;mlx.core&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;mx&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AutoTokenizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AutoModelForCausalLM&lt;/span&gt;

&lt;span class="c1"&gt;# Load tokenizer and model (ensure model is quantized and compatible with MLX)&lt;/span&gt;
&lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoTokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;openai/gpt-oss-20b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoModelForCausalLM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;openai/gpt-oss-20b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;device_map&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;mps&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# Use Apple Silicon GPU&lt;/span&gt;
    &lt;span class="n"&gt;torch_dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;auto&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Example prompt&lt;/span&gt;
&lt;span class="n"&gt;prompt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Explain the Mixture-of-Experts architecture in GPT-OSS.&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prompt&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;return_tensors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Run inference&lt;/span&gt;
&lt;span class="n"&gt;outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;generate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_new_tokens&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;skip_special_tokens&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4 id="mlx-example-inference-on-macos-apple-silicon"&gt;MLX Example: Inference on macOS (Apple Silicon)&lt;/h4&gt;
&lt;p&gt;MLX is Apple's open-source machine learning framework optimized for Apple Silicon. Here's a minimal example for running inference with MLX:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;mlx.core&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;mx&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;mlx.nn&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;nn&lt;/span&gt;

&lt;span class="c1"&gt;# Load a quantized GPT-OSS-20B model in MLX format (assume weights are converted)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GPT2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;path/to/gpt-oss-20b-mlx-weights&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Tokenize input (use compatible tokenizer)&lt;/span&gt;
&lt;span class="n"&gt;prompt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Explain the Mixture-of-Experts architecture in GPT-OSS.&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;input_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prompt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Run inference&lt;/span&gt;
&lt;span class="n"&gt;output_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;generate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_ids&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_new_tokens&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output_ids&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This example uses the &lt;code&gt;mps&lt;/code&gt; device for Apple Silicon GPU acceleration. For Core ML, you can convert the model using &lt;code&gt;coremltools&lt;/code&gt; and run inference in Swift or Python using Core ML APIs. MLX also provides native support for Apple hardware and efficient tensor operations.&lt;/p&gt;
&lt;h3 id="notes"&gt;Notes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;For larger models (120B), use cloud or workstation with multi-GPU setup&lt;/li&gt;
&lt;li&gt;macOS supports both CPU and GPU inference; GPU is recommended for speed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These capabilities make macOS hardware a strong choice for local AI development, prototyping, and even production workloads for small to medium models.&lt;/p&gt;
&lt;h2 id="final-thoughts"&gt;Final Thoughts&lt;/h2&gt;
&lt;p&gt;GPT-OSS is a landmark release that redefines what open-weight models can achieve. With transparent architecture, powerful reasoning, and flexible deployment, these models are poised to accelerate innovation across academia, industry, and government.&lt;/p&gt;</content><category term="Deep Learning"/><category term="GPT-OSS"/><category term="OpenAI"/><category term="LLM"/><category term="Mixture-of-Experts"/><category term="Transformer"/><category term="AI"/></entry></feed>