
<!DOCTYPE html>
<html lang="en">

<!-- Head -->
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-26R9CS17CT"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-26R9CS17CT');
    </script>


        <!-- Required metadata tags -->
        <meta charset="utf-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />

        <!-- Default metadata -->
    <meta name="author" content="Tejus Adiga M" />
    <meta name="description" content="Explore the DeepSpeech model architecture and discover how it revolutionized automatic speech recognition through end-to-end deep learning, from raw audio input to text transcription." />
    <meta name="keywords" content="Speech Recognition, Deep Learning, RNN, CTC, Audio Processing, DeepSpeech">
<meta property="og:site_name" content="Entropy Pages" />
<meta property="og:title" content="DeepSpeech: End-to-End Deep Learning for Speech Recognition" />
<meta property="og:description" content="Explore the DeepSpeech model architecture and discover how it revolutionized automatic speech recognition through end-to-end deep learning, from raw audio input to text transcription." />
<meta property="og:locale" content="en_US" />
<meta property="og:url" content="https://blogs.entropypages.in/deepspeech-architecture-speech-processing.html" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-07-15 00:00:00+05:30" />
<meta property="article:modified_time" content="" />
<meta property="article:author" content="https://blogs.entropypages.in/author/tejus-adiga-m.html">
<meta property="article:section" content="Machine Learning" />
	<meta property="article:tag" content="Speech Recognition" />
	<meta property="article:tag" content="Deep Learning" />
	<meta property="article:tag" content="RNN" />
	<meta property="article:tag" content="CTC" />
	<meta property="article:tag" content="Audio Processing" />
	<meta property="article:tag" content="DeepSpeech" />
	<meta property="og:image" content="https://blogs.entropypages.in/images/SiteImage.png">

        <!-- Site Claim -->


        <!-- Title -->
        <title>
    DeepSpeech: End-to-End Deep Learning for Speech Recognition &ndash; Entropy Pages
        </title>
        
        <!-- Icon -->
        <link rel="shortcut icon" href="https://blogs.entropypages.in/favicon.ico" type="image/x-icon">
        <link rel="icon" href="https://blogs.entropypages.in/favicon.ico" type="image/x-icon">

        <!-- Search engine -->
            <meta name="robots" content="" />

        <!-- Feeds -->
            <link href="https://blogs.entropypages.in/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Entropy Pages Full Atom Feed" />




            <link href="https://blogs.entropypages.in/feeds/machine-learning.atom.xml" type="application/atom+xml" rel="alternate" title="Entropy Pages Categories Atom Feed" />




        <!-- Styles -->
        <!--
        <link rel="stylesheet" href="https://ajax.aspnetcdn.com/ajax/bootstrap/4.3.1/css/bootstrap.min.css">
        -->
        <link rel="stylesheet" href="https://blogs.entropypages.in/theme/bootstrap/bootstrap.min.css">
        <!--
        <link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css">
        -->
        <link rel="stylesheet" href="https://blogs.entropypages.in/theme/pygment/friendly.css">
        <!--
        <link rel="stylesheet" href="https://blogs.entropypages.in/theme/extra/admonition.min.css">
        -->
        <link rel="stylesheet" href="https://blogs.entropypages.in/theme/style.css">
        
        <!-- Google Fonts -->
        <link href="https://fonts.googleapis.com/css2?family=Sankofa+Display:wght@400&display=swap" rel="stylesheet">

        <!-- Google Analytics -->

        <!-- Google Global Site Tag -->

        <!-- Google Tag Manager -->

        <!-- Google Adsense -->

        <!-- Heap Analytic -->

        <!-- Piwik Tracking -->

        <!-- Matomo Tracking -->

        <!-- MathJax Support -->
        <script type="text/javascript">
            window.MathJax = {
                tex: {
                    inlineMath: [['$', '$'], ['\\(', '\\)']],
                    displayMath: [['$$', '$$'], ['\\[', '\\]']],
                    processEscapes: true,
                    processEnvironments: true,
                    packages: {'[+]': ['ams', 'newcommand', 'configmacros']},
                    macros: {
                        land: "\\wedge",
                        lor: "\\vee", 
                        lnot: "\\neg"
                    }
                },
                options: {
                    ignoreHtmlClass: 'tex2jax_ignore',
                    processHtmlClass: 'tex2jax_process'
                }
            };
        </script>
        <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js">
        </script>

</head>

<!-- Body -->
<body class="d-flex flex-column" data-spy="scroll" data-target="#toc" data-offset="0" style="position: relative;">
    <!-- Top anchor -->
    <a href="#" id="backToTop" style="display: none; z-index: 1;" title="Back to top"><span></span></a>

    <!-- Google tag manager -->

    <!-- Navigation -->
    <nav class="flex-shrink-0 navbar navbar-expand-md navbar-expand-lg navbar-dark bg-dark text-light shadow-sm">
        <!-- Logo -->
        <a class="navbar-brand site-name" href="https://blogs.entropypages.in/">Entropy Pages</a>

        <!-- Desktop divider -->
        <div class="navbar-divider d-none d-md-block"></div>

        <!-- Collapse button -->
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMenu" aria-controls="navbarMenu" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon small"></span>
        </button>

        <!-- Collapsible content -->
        <div class="collapse navbar-collapse" id="navbarMenu">

            <!-- i18n subsites -->

            <!-- Page links -->
            <ul class="navbar-nav mr-auto text-center">
                <li class="nav-item ">                           
                    <a class="nav-link" href="https://blogs.entropypages.in">
                        <svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24">
                            <path d="M21 13v10h-6v-6h-6v6h-6v-10h-3l12-12 12 12h-3zm-1-5.907v-5.093h-3v2.093l3 3z" fill="currentColor"></path>
                        </svg>
                        Home <span class="sr-only">(current)</span>
                    </a>
                </li>
                <li class="nav-item ">
                    <a class="nav-link" href="https://blogs.entropypages.in/categories.html">
                        <svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24">
                            <path d="M16 6h-8v-6h8v6zm-8 12h-8v6h8v-6zm16 0h-8v6h8v-6zm-11-7v-3h-2v3h-8v5h2v-3h14v3h2v-5h-8z" fill="currentColor"></path>
                        </svg>
                        Categories
                    </a>
                </li>
                <li class="nav-item ">
                    <a class="nav-link" href="https://blogs.entropypages.in/archives.html">
                        <svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24">
                            <path d="M1.8 9l-.8-4h22l-.8 4h-2.029l.39-2h-17.122l.414 2h-2.053zm18.575-6l.604-2h-17.979l.688 2h16.687zm3.625 8l-2 13h-20l-2-13h24zm-8 4c0-.552-.447-1-1-1h-6c-.553 0-1 .448-1 1s.447 1 1 1h6c.553 0 1-.448 1-1z" fill="currentColor"></path>
                        </svg>
                        Archives
                    </a>
                </li>
                <li class="nav-item ">
                    <a class="nav-link" href="https://blogs.entropypages.in/pages/about.html">
                        <svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24">
                            <path d="M20.822 18.096c-3.439-.794-6.64-1.49-5.09-4.418 4.72-8.912 1.251-13.678-3.732-13.678-5.082 0-8.464 4.949-3.732 13.678 1.597 2.945-1.725 3.641-5.09 4.418-3.073.71-3.188 2.236-3.178 4.904l.004 1h23.99l.004-.969c.012-2.688-.092-4.222-3.176-4.935z" fill="currentColor"></path>
                        </svg>
                        About
                    </a>
                </li>
            </ul>

            <!-- Search form -->
            <form class="form-inline text-center" action="https://blogs.entropypages.in/pages/search.html">
                <input class="form-control w-100 bg-dark text-light text-center border-0 p-2" type="text" name="q" pattern=".{3,}" title="At least 3 characters" required="" placeholder="Type here to search" aria-label="Search">
            </form>

            <!-- Social links -->
            <ul class="navbar-nav text-center">
                <li class="nav-item">
                    <a class="nav-link" href="#">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                            <title>Facebook</title>
                            <path d="M12 0c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm3 8h-1.35c-.538 0-.65.221-.65.778v1.222h2l-.209 2h-1.791v7h-3v-7h-2v-2h2v-2.308c0-1.769.931-2.692 3.029-2.692h1.971v3z" fill="currentColor"></path>
                        </svg>
                    </a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="https://github.com/tejusadiga2004">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                            <title>Github</title>
                            <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z" fill="currentColor"></path>
                        </svg>
                    </a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="https://www.linkedin.com/in/tejusadigam/">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                            <title>Linkedin</title>
                            <path d="M12 0c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm-2 16h-2v-6h2v6zm-1-6.891c-.607 0-1.1-.496-1.1-1.109 0-.612.492-1.109 1.1-1.109s1.1.497 1.1 1.109c0 .613-.493 1.109-1.1 1.109zm8 6.891h-1.998v-2.861c0-1.881-2.002-1.722-2.002 0v2.861h-2v-6h2v1.093c.872-1.616 4-1.736 4 1.548v3.359z" fill="currentColor"></path>
                        </svg>
                    </a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="https://x.com/tejusadiga2004">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                            <title>Twitter</title>
                            <path d="M12 0c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm6.066 9.645c.183 4.04-2.83 8.544-8.164 8.544-1.622 0-3.131-.476-4.402-1.291 1.524.18 3.045-.244 4.252-1.189-1.256-.023-2.317-.854-2.684-1.995.451.086.895.061 1.298-.049-1.381-.278-2.335-1.522-2.304-2.853.388.215.83.344 1.301.359-1.279-.855-1.641-2.544-.889-3.835 1.416 1.738 3.533 2.881 5.92 3.001-.419-1.796.944-3.527 2.799-3.527.825 0 1.572.349 2.096.907.654-.128 1.27-.368 1.824-.697-.215.671-.67 1.233-1.263 1.589.581-.07 1.135-.224 1.649-.453-.384.578-.87 1.084-1.433 1.489z" fill="currentColor"></path>
                        </svg>
                    </a>
                </li>
            </ul>
        </div>
    </nav>

    <!-- Full page -->
    <div class="flex-shrink-0 flex-grow-1">

        <!-- Header -->
        <header class="bg-dark text-light shadow-sm pt-3 pb-2">
	<div class="container">
		<h3 id="deepspeech-architecture-speech-processing">DeepSpeech: End-to-End Deep Learning for Speech Recognition</h3>
		<p style="font-size:larger;"><p>Explore the DeepSpeech model architecture and discover how it revolutionized automatic speech recognition through end-to-end deep learning, from raw audio input to text transcription.</p></p>
        <div class="row mx-auto mt-3">
            <div class="col-xs-12 col-sm-12 col-md-6 text-left" style="padding: 0">
                <a href="https://blogs.entropypages.in/author/tejus-adiga-m.html" class="card-link">Tejus Adiga M</a>
                <span class="card-link text-success">
                    <span class="post-date" title="Post date">Tue 15 July 2025</span>
                </span>
            </div>
            <div class="col-xs-12 col-sm-12 col-md-6 text-right" style="padding: 0">
                <a class="badge badge-success" href="https://blogs.entropypages.in/category/machine-learning.html">machine learning</a>
                    <a class="badge badge-info" href="https://blogs.entropypages.in/tag/speech-recognition.html">speech recognition</a>
                    <a class="badge badge-info" href="https://blogs.entropypages.in/tag/deep-learning.html">deep learning</a>
                    <a class="badge badge-info" href="https://blogs.entropypages.in/tag/rnn.html">rnn</a>
                    <a class="badge badge-info" href="https://blogs.entropypages.in/tag/ctc.html">ctc</a>
                    <a class="badge badge-info" href="https://blogs.entropypages.in/tag/audio-processing.html">audio processing</a>
                    <a class="badge badge-info" href="https://blogs.entropypages.in/tag/deepspeech.html">deepspeech</a>
            </div>
        </div>
	</div>
        </header>

        <!-- Main -->
        <main class="py-3">
                <div class="container">
                    <!-- Sharing -->

                    <!-- Content -->
    <!-- 2 columns layout -->
    <!-- single column layout -->
        <!-- Sharing -->

        <!-- Share post -->

        <!-- Article -->
        <div>
            <p>In the realm of automatic speech recognition (ASR), <strong>DeepSpeech</strong> stands as a groundbreaking achievement that simplified the complex pipeline of traditional speech recognition systems. Developed by Baidu Research in 2014, DeepSpeech demonstrated that end-to-end deep learning could effectively replace the intricate, hand-engineered components that had dominated ASR for decades. This revolutionary approach not only improved accuracy but also made speech recognition more accessible and easier to implement.</p>
<h2 id="the-evolution-from-traditional-asr-to-deepspeech">The Evolution from Traditional ASR to DeepSpeech</h2>
<h3 id="traditional-asr-pipeline-complexity">Traditional ASR Pipeline Complexity</h3>
<p>Before DeepSpeech, automatic speech recognition systems were incredibly complex, involving multiple specialized components:</p>
<ol>
<li><strong>Feature Extraction</strong>: MFCC, PLP, or filterbank features</li>
<li><strong>Acoustic Modeling</strong>: Hidden Markov Models (HMMs) with Gaussian mixture models</li>
<li><strong>Pronunciation Modeling</strong>: Phonetic dictionaries and pronunciation rules</li>
<li><strong>Language Modeling</strong>: N-gram statistical models</li>
<li><strong>Decoding</strong>: Complex search algorithms to find the best word sequence</li>
</ol>
<p>Each component required extensive domain expertise and careful tuning, making ASR systems difficult to build and maintain.</p>
<h3 id="deepspeechs-revolutionary-approach">DeepSpeech's Revolutionary Approach</h3>
<p>DeepSpeech simplified this entire pipeline into a single neural network that learns to map audio spectrograms directly to text transcriptions. This end-to-end approach eliminated the need for:</p>
<ul>
<li>Phonetic dictionaries</li>
<li>Hand-crafted acoustic models</li>
<li>Separate language model integration during training</li>
<li>Complex multi-stage optimization</li>
</ul>
<h2 id="deepspeech-model-architecture">DeepSpeech Model Architecture</h2>
<h3 id="overview-of-the-architecture">Overview of the Architecture</h3>
<p>DeepSpeech employs a relatively straightforward yet powerful architecture consisting of:</p>
<div class="highlight"><pre><span></span><code>Input Audio → Spectrogram → RNN Layers → Fully Connected Layer → CTC Loss → Output Text
</code></pre></div>

<p><img alt="DeepSpeech Architecture Overview" src="https://blogs.entropypages.in/images/DeepSpeech.png"></p>
<p>Let's dive deep into each component:</p>
<h3 id="1-input-processing-and-feature-extraction">1. Input Processing and Feature Extraction</h3>
<h4 id="audio-preprocessing">Audio Preprocessing</h4>
<p>DeepSpeech begins with raw audio waveforms and converts them into spectrograms:</p>
<div class="highlight"><pre><span></span><code><span class="kd">import</span> <span class="nc">MLX</span>
<span class="kd">import</span> <span class="nc">MLXRandom</span>
<span class="kd">import</span> <span class="nc">Foundation</span>
<span class="kd">import</span> <span class="nc">Accelerate</span>

<span class="c1">// Audio preprocessing pipeline</span>
<span class="kd">func</span> <span class="nf">preprocessAudio</span><span class="p">(</span><span class="n">audioWaveform</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">,</span> <span class="n">sampleRate</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">16000</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">MLXArray</span> <span class="p">{</span>
    <span class="c1">// 1. Normalize audio</span>
    <span class="kd">let</span> <span class="nv">maxAbs</span> <span class="p">=</span> <span class="bp">max</span><span class="p">(</span><span class="bp">abs</span><span class="p">(</span><span class="n">audioWaveform</span><span class="p">))</span>
    <span class="kd">let</span> <span class="nv">audioNormalized</span> <span class="p">=</span> <span class="n">audioWaveform</span> <span class="o">/</span> <span class="n">maxAbs</span>

    <span class="c1">// 2. Apply pre-emphasis filter</span>
    <span class="kd">let</span> <span class="nv">preEmphasis</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">=</span> <span class="mf">0.97</span>
    <span class="kd">let</span> <span class="nv">shifted</span> <span class="p">=</span> <span class="n">MLXArray</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span>
        <span class="n">audioNormalized</span><span class="p">[</span><span class="mf">0.</span><span class="p">.&lt;</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">audioNormalized</span><span class="p">[</span><span class="mf">1.</span><span class="p">..]</span> <span class="o">-</span> <span class="n">preEmphasis</span> <span class="o">*</span> <span class="n">audioNormalized</span><span class="p">[</span><span class="mf">0.</span><span class="p">.&lt;</span><span class="n">audioNormalized</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">])</span>

    <span class="c1">// 3. Compute spectrogram using STFT</span>
    <span class="kd">let</span> <span class="nv">nFFT</span> <span class="p">=</span> <span class="mi">512</span>
    <span class="kd">let</span> <span class="nv">hopLength</span> <span class="p">=</span> <span class="mi">160</span>
    <span class="kd">let</span> <span class="nv">winLength</span> <span class="p">=</span> <span class="mi">400</span>

    <span class="kd">let</span> <span class="nv">spectrogram</span> <span class="p">=</span> <span class="n">stft</span><span class="p">(</span><span class="n">shifted</span><span class="p">,</span> <span class="n">nFFT</span><span class="p">:</span> <span class="n">nFFT</span><span class="p">,</span> <span class="n">hopLength</span><span class="p">:</span> <span class="n">hopLength</span><span class="p">,</span> <span class="n">winLength</span><span class="p">:</span> <span class="n">winLength</span><span class="p">)</span>

    <span class="c1">// 4. Convert to power spectrogram</span>
    <span class="kd">let</span> <span class="nv">powerSpectrogram</span> <span class="p">=</span> <span class="n">pow</span><span class="p">(</span><span class="bp">abs</span><span class="p">(</span><span class="n">spectrogram</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1">// 5. Apply log transformation</span>
    <span class="kd">let</span> <span class="nv">logSpectrogram</span> <span class="p">=</span> <span class="n">log</span><span class="p">(</span><span class="n">powerSpectrogram</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logSpectrogram</span><span class="p">.</span><span class="n">transposed</span><span class="p">()</span> <span class="c1">// Shape: (time_steps, frequency_bins)</span>
<span class="p">}</span>

<span class="c1">// Helper function for STFT</span>
<span class="kd">func</span> <span class="nf">stft</span><span class="p">(</span><span class="kc">_</span> <span class="n">signal</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">,</span> <span class="n">nFFT</span><span class="p">:</span> <span class="nb">Int</span><span class="p">,</span> <span class="n">hopLength</span><span class="p">:</span> <span class="nb">Int</span><span class="p">,</span> <span class="n">winLength</span><span class="p">:</span> <span class="nb">Int</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">MLXArray</span> <span class="p">{</span>
    <span class="c1">// Simplified STFT implementation using MLX</span>
    <span class="kd">let</span> <span class="nv">window</span> <span class="p">=</span> <span class="n">MLXArray</span><span class="p">.</span><span class="n">hann</span><span class="p">(</span><span class="n">winLength</span><span class="p">)</span>
    <span class="kd">let</span> <span class="nv">numFrames</span> <span class="p">=</span> <span class="p">(</span><span class="n">signal</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">winLength</span><span class="p">)</span> <span class="o">/</span> <span class="n">hopLength</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="kd">var</span> <span class="nv">frames</span><span class="p">:</span> <span class="p">[</span><span class="n">MLXArray</span><span class="p">]</span> <span class="p">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mf">0.</span><span class="p">.&lt;</span><span class="n">numFrames</span> <span class="p">{</span>
        <span class="kd">let</span> <span class="nv">start</span> <span class="p">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">hopLength</span>
        <span class="kd">let</span> <span class="nv">end</span> <span class="p">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">winLength</span>
        <span class="kd">let</span> <span class="nv">frame</span> <span class="p">=</span> <span class="n">signal</span><span class="p">[</span><span class="n">start</span><span class="p">..&lt;</span><span class="n">end</span><span class="p">]</span> <span class="o">*</span> <span class="n">window</span>
        <span class="kd">let</span> <span class="nv">paddedFrame</span> <span class="p">=</span> <span class="n">MLXArray</span><span class="p">.</span><span class="n">pad</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nFFT</span> <span class="o">-</span> <span class="n">winLength</span><span class="p">)])</span>
        <span class="n">frames</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">fft</span><span class="p">(</span><span class="n">paddedFrame</span><span class="p">))</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">MLXArray</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">frames</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">}</span>
</code></pre></div>

<h4 id="feature-representation">Feature Representation</h4>
<p>The input features consist of:</p>
<ul>
<li><strong>Spectrograms</strong>: 161 frequency bins (from 512-point FFT)</li>
<li><strong>Context Window</strong>: Each frame includes context from neighboring frames</li>
<li><strong>Temporal Sequence</strong>: Variable-length sequences representing utterances</li>
</ul>
<h3 id="2-recurrent-neural-network-layers">2. Recurrent Neural Network Layers</h3>
<p>The core of DeepSpeech consists of multiple RNN layers, typically using:</p>
<h4 id="bidirectional-rnns">Bidirectional RNNs</h4>
<div class="highlight"><pre><span></span><code><span class="kd">import</span> <span class="nc">MLX</span>
<span class="kd">import</span> <span class="nc">MLXNN</span>

<span class="kd">class</span> <span class="nc">DeepSpeechRNN</span><span class="p">:</span> <span class="n">Module</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nv">rnn1</span><span class="p">:</span> <span class="n">RNN</span>
    <span class="kd">let</span> <span class="nv">bidirectionalRNNs</span><span class="p">:</span> <span class="p">[</span><span class="n">RNN</span><span class="p">]</span>
    <span class="kd">let</span> <span class="nv">rnnFinal</span><span class="p">:</span> <span class="n">RNN</span>

    <span class="kd">init</span><span class="p">(</span><span class="n">inputSize</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">161</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">numLayers</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">5</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// First layer: Forward RNN</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">rnn1</span> <span class="p">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">inputSize</span><span class="p">:</span> <span class="n">inputSize</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">:</span> <span class="n">hiddenSize</span><span class="p">)</span>

        <span class="c1">// Middle layers: Bidirectional RNNs</span>
        <span class="kd">var</span> <span class="nv">bidirectionalLayers</span><span class="p">:</span> <span class="p">[</span><span class="n">RNN</span><span class="p">]</span> <span class="p">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="kc">_</span> <span class="k">in</span> <span class="mf">0.</span><span class="p">.</span><span class="o">&lt;</span><span class="p">(</span><span class="n">numLayers</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">bidirectionalLayers</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">RNN</span><span class="p">(</span><span class="n">inputSize</span><span class="p">:</span> <span class="n">hiddenSize</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">:</span> <span class="n">hiddenSize</span><span class="p">,</span> <span class="n">bidirectional</span><span class="p">:</span> <span class="kc">true</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">}</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">bidirectionalRNNs</span> <span class="p">=</span> <span class="n">bidirectionalLayers</span>

        <span class="c1">// Final layer: Forward RNN</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">rnnFinal</span> <span class="p">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">inputSize</span><span class="p">:</span> <span class="n">hiddenSize</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">:</span> <span class="n">hiddenSize</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="kd">func</span> <span class="nf">callAsFunction</span><span class="p">(</span><span class="kc">_</span> <span class="n">x</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">MLXArray</span> <span class="p">{</span>
        <span class="c1">// First RNN layer</span>
        <span class="kd">var</span> <span class="nv">output</span> <span class="p">=</span> <span class="n">rnn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1">// Bidirectional RNN layers</span>
        <span class="k">for</span> <span class="n">rnn</span> <span class="k">in</span> <span class="n">bidirectionalRNNs</span> <span class="p">{</span>
            <span class="n">output</span> <span class="p">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="p">}</span>

        <span class="c1">// Final RNN layer</span>
        <span class="n">output</span> <span class="p">=</span> <span class="n">rnnFinal</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h4 id="why-bidirectional-rnns">Why Bidirectional RNNs?</h4>
<ul>
<li><strong>Forward Direction</strong>: Captures left context (past information)</li>
<li><strong>Backward Direction</strong>: Captures right context (future information)</li>
<li><strong>Combined Representation</strong>: Provides complete temporal context for each time step</li>
</ul>
<h3 id="3-fully-connected-output-layer">3. Fully Connected Output Layer</h3>
<p>The final RNN output is fed through a fully connected layer:</p>
<div class="highlight"><pre><span></span><code><span class="kd">import</span> <span class="nc">MLX</span>
<span class="kd">import</span> <span class="nc">MLXNN</span>

<span class="kd">class</span> <span class="nc">DeepSpeechOutputLayer</span><span class="p">:</span> <span class="n">Module</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nv">fc</span><span class="p">:</span> <span class="n">Linear</span>

    <span class="kd">init</span><span class="p">(</span><span class="n">hiddenSize</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">numClasses</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">29</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// 26 letters + space + apostrophe + blank</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">fc</span> <span class="p">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hiddenSize</span><span class="p">,</span> <span class="n">numClasses</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="kd">func</span> <span class="nf">callAsFunction</span><span class="p">(</span><span class="kc">_</span> <span class="n">x</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">MLXArray</span> <span class="p">{</span>
        <span class="c1">// x shape: (batch_size, time_steps, hidden_size)</span>
        <span class="k">return</span> <span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">// Output: (batch_size, time_steps, num_classes)</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="4-connectionist-temporal-classification-ctc">4. Connectionist Temporal Classification (CTC)</h3>
<p>CTC is crucial for handling the alignment problem between variable-length audio and text:</p>
<h4 id="the-alignment-challenge">The Alignment Challenge</h4>
<p>Traditional approaches required knowing exactly when each phoneme or word occurs in the audio. CTC eliminates this requirement by:</p>
<ol>
<li><strong>Introducing a Blank Symbol</strong>: Represents "no character" at a time step</li>
<li><strong>Allowing Repetitions</strong>: Same character can be predicted consecutively</li>
<li><strong>Dynamic Programming</strong>: Efficiently computes all possible alignments</li>
</ol>
<h4 id="ctc-decoding-process">CTC Decoding Process</h4>
<div class="highlight"><pre><span></span><code><span class="kd">import</span> <span class="nc">MLX</span>
<span class="kd">import</span> <span class="nc">Foundation</span>

<span class="kd">func</span> <span class="nf">ctcDecode</span><span class="p">(</span><span class="n">predictions</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">,</span> <span class="n">blankId</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="p">[</span><span class="nb">Int</span><span class="p">]</span> <span class="p">{</span>
    <span class="s">&quot;&quot;&quot;</span>
<span class="s">    Simple CTC greedy decoding</span>
<span class="s">    predictions: (time_steps, num_classes) - log probabilities</span>
<span class="s">    &quot;&quot;&quot;</span>
    <span class="c1">// 1. Get the most likely character at each time step</span>
    <span class="kd">let</span> <span class="nv">predictedIds</span> <span class="p">=</span> <span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1">// 2. Remove consecutive duplicates</span>
    <span class="kd">var</span> <span class="nv">decoded</span><span class="p">:</span> <span class="p">[</span><span class="nb">Int</span><span class="p">]</span> <span class="p">=</span> <span class="p">[]</span>
    <span class="kd">var</span> <span class="nv">prevId</span><span class="p">:</span> <span class="nb">Int</span><span class="p">?</span> <span class="p">=</span> <span class="kc">nil</span>

    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mf">0.</span><span class="p">.&lt;</span><span class="n">predictedIds</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">{</span>
        <span class="kd">let</span> <span class="nv">predId</span> <span class="p">=</span> <span class="nb">Int</span><span class="p">(</span><span class="n">predictedIds</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">item</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">predId</span> <span class="o">!=</span> <span class="n">prevId</span> <span class="p">{</span>
            <span class="n">decoded</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">predId</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="n">prevId</span> <span class="p">=</span> <span class="n">predId</span>
    <span class="p">}</span>

    <span class="c1">// 3. Remove blank symbols</span>
    <span class="n">decoded</span> <span class="p">=</span> <span class="n">decoded</span><span class="p">.</span><span class="bp">filter</span> <span class="p">{</span> <span class="nv">$0</span> <span class="o">!=</span> <span class="n">blankId</span> <span class="p">}</span>

    <span class="k">return</span> <span class="n">decoded</span>
<span class="p">}</span>
</code></pre></div>

<h2 id="complete-deepspeech-architecture">Complete DeepSpeech Architecture</h2>
<h3 id="full-model-implementation">Full Model Implementation</h3>
<div class="highlight"><pre><span></span><code><span class="kd">import</span> <span class="nc">MLX</span>
<span class="kd">import</span> <span class="nc">MLXNN</span>

<span class="kd">class</span> <span class="nc">DeepSpeech</span><span class="p">:</span> <span class="n">Module</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nv">inputLayer</span><span class="p">:</span> <span class="n">Linear</span>
    <span class="kd">let</span> <span class="nv">rnnLayers</span><span class="p">:</span> <span class="p">[</span><span class="n">LSTM</span><span class="p">]</span>
    <span class="kd">let</span> <span class="nv">outputLayer</span><span class="p">:</span> <span class="n">Linear</span>
    <span class="kd">let</span> <span class="nv">batchNorms</span><span class="p">:</span> <span class="p">[</span><span class="n">BatchNorm</span><span class="p">]</span>

    <span class="kd">init</span><span class="p">(</span><span class="n">inputSize</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">161</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">numLayers</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">numClasses</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">29</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// Input layer</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">inputLayer</span> <span class="p">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">inputSize</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">)</span>

        <span class="c1">// RNN layers</span>
        <span class="kd">var</span> <span class="nv">rnnLayersList</span><span class="p">:</span> <span class="p">[</span><span class="n">LSTM</span><span class="p">]</span> <span class="p">=</span> <span class="p">[]</span>
        <span class="kd">var</span> <span class="nv">batchNormsList</span><span class="p">:</span> <span class="p">[</span><span class="n">BatchNorm</span><span class="p">]</span> <span class="p">=</span> <span class="p">[]</span>

        <span class="c1">// First LSTM (unidirectional)</span>
        <span class="n">rnnLayersList</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">inputSize</span><span class="p">:</span> <span class="n">hiddenSize</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">:</span> <span class="n">hiddenSize</span><span class="p">))</span>
        <span class="n">batchNormsList</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">BatchNorm</span><span class="p">(</span><span class="n">featureCount</span><span class="p">:</span> <span class="n">hiddenSize</span><span class="p">))</span>

        <span class="c1">// Bidirectional LSTM layers</span>
        <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mf">1.</span><span class="p">.</span><span class="o">&lt;</span><span class="p">(</span><span class="n">numLayers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
            <span class="kd">let</span> <span class="nv">inputDim</span> <span class="p">=</span> <span class="p">(</span><span class="n">i</span> <span class="p">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">?</span> <span class="n">hiddenSize</span> <span class="p">:</span> <span class="n">hiddenSize</span> <span class="o">*</span> <span class="mi">2</span>
            <span class="n">rnnLayersList</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">LSTM</span><span class="p">(</span><span class="n">inputSize</span><span class="p">:</span> <span class="n">inputDim</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">:</span> <span class="n">hiddenSize</span><span class="p">,</span> <span class="n">bidirectional</span><span class="p">:</span> <span class="kc">true</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">batchNormsList</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">BatchNorm</span><span class="p">(</span><span class="n">featureCount</span><span class="p">:</span> <span class="n">hiddenSize</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>
        <span class="p">}</span>

        <span class="c1">// Final LSTM (unidirectional)</span>
        <span class="n">rnnLayersList</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">inputSize</span><span class="p">:</span> <span class="n">hiddenSize</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">:</span> <span class="n">hiddenSize</span><span class="p">))</span>
        <span class="n">batchNormsList</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">BatchNorm</span><span class="p">(</span><span class="n">featureCount</span><span class="p">:</span> <span class="n">hiddenSize</span><span class="p">))</span>

        <span class="kc">self</span><span class="p">.</span><span class="n">rnnLayers</span> <span class="p">=</span> <span class="n">rnnLayersList</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">batchNorms</span> <span class="p">=</span> <span class="n">batchNormsList</span>

        <span class="c1">// Output layer</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">outputLayer</span> <span class="p">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hiddenSize</span><span class="p">,</span> <span class="n">numClasses</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="kd">func</span> <span class="nf">callAsFunction</span><span class="p">(</span><span class="kc">_</span> <span class="n">x</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">MLXArray</span> <span class="p">{</span>
        <span class="c1">// x shape: (batch_size, time_steps, frequency_bins)</span>
        <span class="kd">let</span> <span class="nv">batchSize</span> <span class="p">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="kd">let</span> <span class="nv">timeSteps</span> <span class="p">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1">// Input transformation</span>
        <span class="kd">var</span> <span class="nv">output</span> <span class="p">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">inputLayer</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="c1">// Apply batch normalization</span>
        <span class="n">output</span> <span class="p">=</span> <span class="n">output</span><span class="p">.</span><span class="n">transposed</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">// (batch_size, hidden_size, time_steps)</span>
        <span class="n">output</span> <span class="p">=</span> <span class="n">batchNorms</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">output</span><span class="p">)</span>
        <span class="n">output</span> <span class="p">=</span> <span class="n">output</span><span class="p">.</span><span class="n">transposed</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">// (batch_size, time_steps, hidden_size)</span>

        <span class="c1">// RNN layers</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="n">bn</span><span class="p">))</span> <span class="k">in</span> <span class="n">zip</span><span class="p">(</span><span class="n">rnnLayers</span><span class="p">,</span> <span class="n">batchNorms</span><span class="p">.</span><span class="bp">dropFirst</span><span class="p">()).</span><span class="n">enumerated</span><span class="p">()</span> <span class="p">{</span>
            <span class="n">output</span> <span class="p">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

            <span class="c1">// Apply batch normalization</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">rnnLayers</span><span class="p">.</span><span class="bp">count</span> <span class="o">-</span> <span class="mi">1</span> <span class="p">{</span> <span class="c1">// Not the last layer</span>
                <span class="n">output</span> <span class="p">=</span> <span class="n">output</span><span class="p">.</span><span class="n">transposed</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">==</span> <span class="n">bn</span><span class="p">.</span><span class="n">featureCount</span> <span class="p">{</span> <span class="c1">// Check dimension compatibility</span>
                    <span class="n">output</span> <span class="p">=</span> <span class="n">bn</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
                <span class="p">}</span>
                <span class="n">output</span> <span class="p">=</span> <span class="n">output</span><span class="p">.</span><span class="n">transposed</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="p">}</span>
        <span class="p">}</span>

        <span class="c1">// Output layer</span>
        <span class="n">output</span> <span class="p">=</span> <span class="n">outputLayer</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="c1">// Apply log softmax for CTC loss</span>
        <span class="n">output</span> <span class="p">=</span> <span class="n">logSoftmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h2 id="speech-processing-pipeline">Speech Processing Pipeline</h2>
<h3 id="step-by-step-processing-flow">Step-by-Step Processing Flow</h3>
<h4 id="1-audio-input-processing">1. Audio Input Processing</h4>
<div class="highlight"><pre><span></span><code><span class="kd">import</span> <span class="nc">MLX</span>
<span class="kd">import</span> <span class="nc">Foundation</span>
<span class="kd">import</span> <span class="nc">AVFoundation</span>

<span class="kd">func</span> <span class="nf">processAudioInput</span><span class="p">(</span><span class="n">audioFilePath</span><span class="p">:</span> <span class="nb">String</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="p">(</span><span class="n">MLXArray</span><span class="p">,</span> <span class="nb">Int</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Load audio file using AVFoundation</span>
    <span class="k">guard</span> <span class="kd">let</span> <span class="nv">audioFile</span> <span class="p">=</span> <span class="k">try</span><span class="p">?</span> <span class="n">AVAudioFile</span><span class="p">(</span><span class="n">forReading</span><span class="p">:</span> <span class="n">URL</span><span class="p">(</span><span class="n">fileURLWithPath</span><span class="p">:</span> <span class="n">audioFilePath</span><span class="p">))</span> <span class="k">else</span> <span class="p">{</span>
        <span class="bp">fatalError</span><span class="p">(</span><span class="s">&quot;Could not load audio file&quot;</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="kd">let</span> <span class="nv">sampleRate</span> <span class="p">=</span> <span class="nb">Int</span><span class="p">(</span><span class="n">audioFile</span><span class="p">.</span><span class="n">fileFormat</span><span class="p">.</span><span class="n">sampleRate</span><span class="p">)</span>
    <span class="kd">let</span> <span class="nv">frameCount</span> <span class="p">=</span> <span class="nb">Int</span><span class="p">(</span><span class="n">audioFile</span><span class="p">.</span><span class="n">length</span><span class="p">)</span>

    <span class="k">guard</span> <span class="kd">let</span> <span class="nv">buffer</span> <span class="p">=</span> <span class="n">AVAudioPCMBuffer</span><span class="p">(</span><span class="n">pcmFormat</span><span class="p">:</span> <span class="n">audioFile</span><span class="p">.</span><span class="n">processingFormat</span><span class="p">,</span> <span class="n">frameCapacity</span><span class="p">:</span> <span class="n">AVAudioFrameCount</span><span class="p">(</span><span class="n">frameCount</span><span class="p">))</span> <span class="k">else</span> <span class="p">{</span>
        <span class="bp">fatalError</span><span class="p">(</span><span class="s">&quot;Could not create audio buffer&quot;</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="k">try</span><span class="p">?</span> <span class="n">audioFile</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="n">into</span><span class="p">:</span> <span class="n">buffer</span><span class="p">)</span>

    <span class="c1">// Convert to MLXArray</span>
    <span class="kd">let</span> <span class="nv">audioData</span> <span class="p">=</span> <span class="nb">Array</span><span class="p">(</span><span class="nb">UnsafeBufferPointer</span><span class="p">(</span><span class="n">start</span><span class="p">:</span> <span class="n">buffer</span><span class="p">.</span><span class="n">floatChannelData</span><span class="p">![</span><span class="mi">0</span><span class="p">],</span> <span class="bp">count</span><span class="p">:</span> <span class="n">frameCount</span><span class="p">))</span>
    <span class="kd">var</span> <span class="nv">audio</span> <span class="p">=</span> <span class="n">MLXArray</span><span class="p">(</span><span class="n">audioData</span><span class="p">)</span>

    <span class="c1">// Ensure mono channel (if stereo, take mean)</span>
    <span class="k">if</span> <span class="n">buffer</span><span class="p">.</span><span class="n">format</span><span class="p">.</span><span class="n">channelCount</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="p">{</span>
        <span class="c1">// Handle stereo to mono conversion</span>
        <span class="kd">let</span> <span class="nv">channelData</span> <span class="p">=</span> <span class="n">buffer</span><span class="p">.</span><span class="n">floatChannelData</span><span class="p">!</span>
        <span class="kd">var</span> <span class="nv">monoData</span><span class="p">:</span> <span class="p">[</span><span class="nb">Float</span><span class="p">]</span> <span class="p">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mf">0.</span><span class="p">.&lt;</span><span class="n">frameCount</span> <span class="p">{</span>
            <span class="kd">let</span> <span class="nv">sample</span> <span class="p">=</span> <span class="p">(</span><span class="n">channelData</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">channelData</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">])</span> <span class="o">/</span> <span class="mf">2.0</span>
            <span class="n">monoData</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="n">audio</span> <span class="p">=</span> <span class="n">MLXArray</span><span class="p">(</span><span class="n">monoData</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="c1">// Normalize audio</span>
    <span class="kd">let</span> <span class="nv">maxAbs</span> <span class="p">=</span> <span class="bp">max</span><span class="p">(</span><span class="bp">abs</span><span class="p">(</span><span class="n">audio</span><span class="p">))</span>
    <span class="n">audio</span> <span class="p">=</span> <span class="n">audio</span> <span class="o">/</span> <span class="n">maxAbs</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="n">sampleRate</span><span class="p">)</span>
<span class="p">}</span>
</code></pre></div>

<h4 id="2-feature-extraction">2. Feature Extraction</h4>
<div class="highlight"><pre><span></span><code><span class="kd">import</span> <span class="nc">MLX</span>
<span class="kd">import</span> <span class="nc">Foundation</span>

<span class="kd">func</span> <span class="nf">extractFeatures</span><span class="p">(</span><span class="n">audio</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">,</span> <span class="n">sampleRate</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">16000</span><span class="p">,</span> <span class="n">nFFT</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">hopLength</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">160</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">MLXArray</span> <span class="p">{</span>
    <span class="c1">// Compute Short-Time Fourier Transform</span>
    <span class="kd">let</span> <span class="nv">stftResult</span> <span class="p">=</span> <span class="n">stft</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="n">nFFT</span><span class="p">:</span> <span class="n">nFFT</span><span class="p">,</span> <span class="n">hopLength</span><span class="p">:</span> <span class="n">hopLength</span><span class="p">)</span>

    <span class="c1">// Convert to magnitude spectrogram</span>
    <span class="kd">let</span> <span class="nv">magnitude</span> <span class="p">=</span> <span class="bp">abs</span><span class="p">(</span><span class="n">stftResult</span><span class="p">)</span>

    <span class="c1">// Apply log transformation</span>
    <span class="kd">let</span> <span class="nv">logSpectrogram</span> <span class="p">=</span> <span class="n">log</span><span class="p">(</span><span class="n">magnitude</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>

    <span class="c1">// Transpose for time-major format</span>
    <span class="kd">let</span> <span class="nv">features</span> <span class="p">=</span> <span class="n">logSpectrogram</span><span class="p">.</span><span class="n">transposed</span><span class="p">()</span> <span class="c1">// Shape: (time_steps, frequency_bins)</span>

    <span class="k">return</span> <span class="n">features</span>
<span class="p">}</span>

<span class="c1">// Enhanced STFT implementation for MLX</span>
<span class="kd">func</span> <span class="nf">stft</span><span class="p">(</span><span class="kc">_</span> <span class="n">signal</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">,</span> <span class="n">nFFT</span><span class="p">:</span> <span class="nb">Int</span><span class="p">,</span> <span class="n">hopLength</span><span class="p">:</span> <span class="nb">Int</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">MLXArray</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nv">windowSize</span> <span class="p">=</span> <span class="n">nFFT</span>
    <span class="kd">let</span> <span class="nv">window</span> <span class="p">=</span> <span class="n">MLXArray</span><span class="p">.</span><span class="n">hann</span><span class="p">(</span><span class="n">windowSize</span><span class="p">)</span>
    <span class="kd">let</span> <span class="nv">numFrames</span> <span class="p">=</span> <span class="p">(</span><span class="n">signal</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">windowSize</span><span class="p">)</span> <span class="o">/</span> <span class="n">hopLength</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="kd">var</span> <span class="nv">spectrogramFrames</span><span class="p">:</span> <span class="p">[</span><span class="n">MLXArray</span><span class="p">]</span> <span class="p">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mf">0.</span><span class="p">.&lt;</span><span class="n">numFrames</span> <span class="p">{</span>
        <span class="kd">let</span> <span class="nv">start</span> <span class="p">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">hopLength</span>
        <span class="kd">let</span> <span class="nv">end</span> <span class="p">=</span> <span class="bp">min</span><span class="p">(</span><span class="n">start</span> <span class="o">+</span> <span class="n">windowSize</span><span class="p">,</span> <span class="n">signal</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="kd">var</span> <span class="nv">frame</span> <span class="p">=</span> <span class="n">signal</span><span class="p">[</span><span class="n">start</span><span class="p">..&lt;</span><span class="n">end</span><span class="p">]</span>

        <span class="c1">// Pad if necessary</span>
        <span class="k">if</span> <span class="n">frame</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">windowSize</span> <span class="p">{</span>
            <span class="kd">let</span> <span class="nv">padSize</span> <span class="p">=</span> <span class="n">windowSize</span> <span class="o">-</span> <span class="n">frame</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">frame</span> <span class="p">=</span> <span class="n">MLXArray</span><span class="p">.</span><span class="n">pad</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="n">padSize</span><span class="p">)])</span>
        <span class="p">}</span>

        <span class="c1">// Apply window function</span>
        <span class="kd">let</span> <span class="nv">windowedFrame</span> <span class="p">=</span> <span class="n">frame</span> <span class="o">*</span> <span class="n">window</span>

        <span class="c1">// Apply FFT</span>
        <span class="kd">let</span> <span class="nv">fftResult</span> <span class="p">=</span> <span class="n">fft</span><span class="p">(</span><span class="n">windowedFrame</span><span class="p">)</span>
        <span class="n">spectrogramFrames</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">fftResult</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">MLXArray</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">spectrogramFrames</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="mi">0</span><span class="p">)</span>
<span class="p">}</span>
</code></pre></div>

<h4 id="3-model-inference">3. Model Inference</h4>
<div class="highlight"><pre><span></span><code><span class="kd">import</span> <span class="nc">MLX</span>
<span class="kd">import</span> <span class="nc">MLXNN</span>

<span class="kd">func</span> <span class="nf">transcribeAudio</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">DeepSpeech</span><span class="p">,</span> <span class="n">audioFeatures</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="p">[</span><span class="nb">Int</span><span class="p">]</span> <span class="p">{</span>
    <span class="c1">// Set model to evaluation mode (disable dropout, etc.)</span>
    <span class="n">MLX</span><span class="p">.</span><span class="n">eval</span> <span class="p">{</span>
        <span class="c1">// Convert to tensor and add batch dimension</span>
        <span class="kd">let</span> <span class="nv">featuresTensor</span> <span class="p">=</span> <span class="n">audioFeatures</span><span class="p">.</span><span class="n">expandedDimensions</span><span class="p">(</span><span class="n">axis</span><span class="p">:</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1">// Forward pass</span>
        <span class="kd">let</span> <span class="nv">logProbs</span> <span class="p">=</span> <span class="n">model</span><span class="p">(</span><span class="n">featuresTensor</span><span class="p">)</span>

        <span class="c1">// CTC decode</span>
        <span class="kd">let</span> <span class="nv">squeezedLogProbs</span> <span class="p">=</span> <span class="n">logProbs</span><span class="p">.</span><span class="n">squeezed</span><span class="p">(</span><span class="n">axis</span><span class="p">:</span> <span class="mi">0</span><span class="p">)</span>
        <span class="kd">let</span> <span class="nv">transcript</span> <span class="p">=</span> <span class="n">ctcDecode</span><span class="p">(</span><span class="n">predictions</span><span class="p">:</span> <span class="n">squeezedLogProbs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">transcript</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h2 id="deepspeech-training-methodology">DeepSpeech Training Methodology</h2>
<h3 id="historical-training-approach">Historical Training Approach</h3>
<p>The original DeepSpeech training methodology, developed by Baidu Research, established many of the best practices still used in modern speech recognition systems. Understanding their approach provides valuable insights into large-scale deep learning for speech.</p>
<h3 id="dataset-preparation-and-scale">Dataset Preparation and Scale</h3>
<h4 id="training-data-composition">Training Data Composition</h4>
<p>The original DeepSpeech was trained on a massive dataset comprising:</p>
<ul>
<li><strong>English Speech Corpus</strong>: 11,940 hours of speech data</li>
<li><strong>Mandarin Speech Corpus</strong>: 9,400 hours of speech data</li>
<li><strong>Data Sources</strong>:</li>
<li>Internal speech datasets</li>
<li>Publicly available corpora</li>
<li>Synthetic speech data</li>
<li>Read speech and conversational speech</li>
</ul>
<h4 id="data-preprocessing-pipeline">Data Preprocessing Pipeline</h4>
<div class="highlight"><pre><span></span><code><span class="kd">import</span> <span class="nc">MLX</span>
<span class="kd">import</span> <span class="nc">Foundation</span>

<span class="kd">class</span> <span class="nc">DeepSpeechDataPreprocessor</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nv">sampleRate</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">16000</span>
    <span class="kd">let</span> <span class="nv">windowSizeMs</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">=</span> <span class="mf">20.0</span>  <span class="c1">// 20ms windows</span>
    <span class="kd">let</span> <span class="nv">windowStrideMs</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">=</span> <span class="mf">10.0</span>  <span class="c1">// 10ms stride</span>

    <span class="kd">func</span> <span class="nf">preprocessTrainingBatch</span><span class="p">(</span><span class="kc">_</span> <span class="n">audioPaths</span><span class="p">:</span> <span class="p">[</span><span class="nb">String</span><span class="p">],</span> <span class="kc">_</span> <span class="n">transcripts</span><span class="p">:</span> <span class="p">[</span><span class="nb">String</span><span class="p">])</span> <span class="p">-&gt;</span> <span class="p">(</span><span class="n">MLXArray</span><span class="p">,</span> <span class="n">MLXArray</span><span class="p">,</span> <span class="n">MLXArray</span><span class="p">,</span> <span class="n">MLXArray</span><span class="p">)</span> <span class="p">{</span>
        <span class="kd">var</span> <span class="nv">processedAudio</span><span class="p">:</span> <span class="p">[</span><span class="n">MLXArray</span><span class="p">]</span> <span class="p">=</span> <span class="p">[]</span>
        <span class="kd">var</span> <span class="nv">processedTranscripts</span><span class="p">:</span> <span class="p">[[</span><span class="nb">Int</span><span class="p">]]</span> <span class="p">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="p">(</span><span class="n">audioPath</span><span class="p">,</span> <span class="n">transcript</span><span class="p">)</span> <span class="k">in</span> <span class="n">zip</span><span class="p">(</span><span class="n">audioPaths</span><span class="p">,</span> <span class="n">transcripts</span><span class="p">)</span> <span class="p">{</span>
            <span class="c1">// 1. Load and resample audio to 16kHz</span>
            <span class="kd">let</span> <span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="kc">_</span><span class="p">)</span> <span class="p">=</span> <span class="n">loadAndResampleAudio</span><span class="p">(</span><span class="n">path</span><span class="p">:</span> <span class="n">audioPath</span><span class="p">,</span> <span class="n">targetSampleRate</span><span class="p">:</span> <span class="n">sampleRate</span><span class="p">)</span>

            <span class="c1">// 2. Apply data augmentation during training</span>
            <span class="kd">let</span> <span class="nv">augmentedAudio</span> <span class="p">=</span> <span class="n">applyTrainingAugmentation</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>

            <span class="c1">// 3. Extract spectrograms</span>
            <span class="kd">let</span> <span class="nv">spectrogram</span> <span class="p">=</span> <span class="n">extractSpectrogramFeatures</span><span class="p">(</span><span class="n">augmentedAudio</span><span class="p">)</span>

            <span class="c1">// 4. Normalize features per utterance</span>
            <span class="kd">let</span> <span class="nv">normalizedFeatures</span> <span class="p">=</span> <span class="n">normalizeFeatures</span><span class="p">(</span><span class="n">spectrogram</span><span class="p">)</span>

            <span class="c1">// 5. Convert transcript to character indices</span>
            <span class="kd">let</span> <span class="nv">charIndices</span> <span class="p">=</span> <span class="n">transcriptToIndices</span><span class="p">(</span><span class="n">transcript</span><span class="p">)</span>

            <span class="n">processedAudio</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">normalizedFeatures</span><span class="p">)</span>
            <span class="n">processedTranscripts</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">charIndices</span><span class="p">)</span>
        <span class="p">}</span>

        <span class="c1">// 6. Create padded batches for efficient training</span>
        <span class="k">return</span> <span class="n">createPaddedBatch</span><span class="p">(</span><span class="n">processedAudio</span><span class="p">,</span> <span class="n">processedTranscripts</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="kd">private</span> <span class="kd">func</span> <span class="nf">applyTrainingAugmentation</span><span class="p">(</span><span class="kc">_</span> <span class="n">audio</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">MLXArray</span> <span class="p">{</span>
        <span class="kd">var</span> <span class="nv">augmented</span> <span class="p">=</span> <span class="n">audio</span>

        <span class="c1">// Speed perturbation (0.9x to 1.1x)</span>
        <span class="k">if</span> <span class="nb">Float</span><span class="p">.</span><span class="n">random</span><span class="p">(</span><span class="k">in</span><span class="p">:</span> <span class="mf">0.</span><span class="p">..</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.3</span> <span class="p">{</span>
            <span class="kd">let</span> <span class="nv">speedFactor</span> <span class="p">=</span> <span class="nb">Float</span><span class="p">.</span><span class="n">random</span><span class="p">(</span><span class="k">in</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">...</span><span class="mf">1.1</span><span class="p">)</span>
            <span class="n">augmented</span> <span class="p">=</span> <span class="n">timeStretch</span><span class="p">(</span><span class="n">augmented</span><span class="p">,</span> <span class="n">factor</span><span class="p">:</span> <span class="n">speedFactor</span><span class="p">)</span>
        <span class="p">}</span>

        <span class="c1">// Add background noise</span>
        <span class="k">if</span> <span class="nb">Float</span><span class="p">.</span><span class="n">random</span><span class="p">(</span><span class="k">in</span><span class="p">:</span> <span class="mf">0.</span><span class="p">..</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.4</span> <span class="p">{</span>
            <span class="kd">let</span> <span class="nv">noiseLevel</span> <span class="p">=</span> <span class="nb">Float</span><span class="p">.</span><span class="n">random</span><span class="p">(</span><span class="k">in</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">...</span><span class="mf">0.05</span><span class="p">)</span>
            <span class="kd">let</span> <span class="nv">noise</span> <span class="p">=</span> <span class="n">MLXRandom</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">augmented</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">mean</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="p">:</span> <span class="n">noiseLevel</span><span class="p">)</span>
            <span class="n">augmented</span> <span class="p">=</span> <span class="n">augmented</span> <span class="o">+</span> <span class="n">noise</span>
        <span class="p">}</span>

        <span class="c1">// Volume perturbation</span>
        <span class="k">if</span> <span class="nb">Float</span><span class="p">.</span><span class="n">random</span><span class="p">(</span><span class="k">in</span><span class="p">:</span> <span class="mf">0.</span><span class="p">..</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.3</span> <span class="p">{</span>
            <span class="kd">let</span> <span class="nv">volumeFactor</span> <span class="p">=</span> <span class="nb">Float</span><span class="p">.</span><span class="n">random</span><span class="p">(</span><span class="k">in</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">...</span><span class="mf">1.2</span><span class="p">)</span>
            <span class="n">augmented</span> <span class="p">=</span> <span class="n">augmented</span> <span class="o">*</span> <span class="n">volumeFactor</span>
        <span class="p">}</span>

        <span class="k">return</span> <span class="n">augmented</span>
    <span class="p">}</span>

    <span class="kd">private</span> <span class="kd">func</span> <span class="nf">extractSpectrogramFeatures</span><span class="p">(</span><span class="kc">_</span> <span class="n">audio</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">MLXArray</span> <span class="p">{</span>
        <span class="c1">// Convert to power spectrogram</span>
        <span class="kd">let</span> <span class="nv">windowLength</span> <span class="p">=</span> <span class="nb">Int</span><span class="p">(</span><span class="n">windowSizeMs</span> <span class="o">*</span> <span class="nb">Float</span><span class="p">(</span><span class="n">sampleRate</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1000.0</span><span class="p">)</span>
        <span class="kd">let</span> <span class="nv">hopLength</span> <span class="p">=</span> <span class="nb">Int</span><span class="p">(</span><span class="n">windowStrideMs</span> <span class="o">*</span> <span class="nb">Float</span><span class="p">(</span><span class="n">sampleRate</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1000.0</span><span class="p">)</span>

        <span class="kd">let</span> <span class="nv">stftResult</span> <span class="p">=</span> <span class="n">stft</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="n">nFFT</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span> <span class="n">hopLength</span><span class="p">:</span> <span class="n">hopLength</span><span class="p">,</span> <span class="n">winLength</span><span class="p">:</span> <span class="n">windowLength</span><span class="p">)</span>
        <span class="kd">let</span> <span class="nv">powerSpectrogram</span> <span class="p">=</span> <span class="n">pow</span><span class="p">(</span><span class="bp">abs</span><span class="p">(</span><span class="n">stftResult</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1">// Apply log transformation with small epsilon for numerical stability</span>
        <span class="kd">let</span> <span class="nv">logSpectrogram</span> <span class="p">=</span> <span class="n">log</span><span class="p">(</span><span class="n">powerSpectrogram</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">logSpectrogram</span><span class="p">.</span><span class="n">transposed</span><span class="p">()</span> <span class="c1">// Shape: (time_steps, frequency_bins)</span>
    <span class="p">}</span>

    <span class="kd">private</span> <span class="kd">func</span> <span class="nf">normalizeFeatures</span><span class="p">(</span><span class="kc">_</span> <span class="n">features</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">MLXArray</span> <span class="p">{</span>
        <span class="c1">// Per-utterance normalization (zero mean, unit variance)</span>
        <span class="kd">let</span> <span class="nv">mean</span> <span class="p">=</span> <span class="n">features</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axes</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">keepDims</span><span class="p">:</span> <span class="kc">true</span><span class="p">)</span>
        <span class="kd">let</span> <span class="nv">variance</span> <span class="p">=</span> <span class="p">((</span><span class="n">features</span> <span class="o">-</span> <span class="n">mean</span><span class="p">).</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)).</span><span class="n">mean</span><span class="p">(</span><span class="n">axes</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">keepDims</span><span class="p">:</span> <span class="kc">true</span><span class="p">)</span>
        <span class="kd">let</span> <span class="nv">std</span> <span class="p">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">features</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="training-infrastructure-and-distributed-learning">Training Infrastructure and Distributed Learning</h3>
<h4 id="multi-gpu-training-setup">Multi-GPU Training Setup</h4>
<p>The original DeepSpeech training utilized distributed training across multiple GPUs:</p>
<div class="highlight"><pre><span></span><code><span class="kd">import</span> <span class="nc">MLX</span>
<span class="kd">import</span> <span class="nc">MLXOptimizers</span>

<span class="kd">class</span> <span class="nc">DistributedDeepSpeechTrainer</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nv">model</span><span class="p">:</span> <span class="n">DeepSpeech</span>
    <span class="kd">let</span> <span class="nv">optimizer</span><span class="p">:</span> <span class="n">SGD</span>
    <span class="kd">let</span> <span class="nv">learningRateScheduler</span><span class="p">:</span> <span class="n">ExponentialDecayScheduler</span>

    <span class="c1">// Training hyperparameters from original paper</span>
    <span class="kd">let</span> <span class="nv">baseLearningRate</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">=</span> <span class="mf">3e-4</span>
    <span class="kd">let</span> <span class="nv">batchSize</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">256</span>  <span class="c1">// Per GPU</span>
    <span class="kd">let</span> <span class="nv">maxEpochs</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">20</span>
    <span class="kd">let</span> <span class="nv">gradientClipNorm</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">=</span> <span class="mf">5.0</span>

    <span class="kd">init</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">DeepSpeech</span><span class="p">)</span> <span class="p">{</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">model</span> <span class="p">=</span> <span class="n">model</span>

        <span class="c1">// SGD with momentum (as used in original DeepSpeech)</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">optimizer</span> <span class="p">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">learningRate</span><span class="p">:</span> <span class="n">baseLearningRate</span><span class="p">,</span> <span class="n">momentum</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">)</span>

        <span class="c1">// Learning rate schedule: exponential decay</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">learningRateScheduler</span> <span class="p">=</span> <span class="n">ExponentialDecayScheduler</span><span class="p">(</span>
            <span class="n">initialLR</span><span class="p">:</span> <span class="n">baseLearningRate</span><span class="p">,</span>
            <span class="n">decayRate</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span>
            <span class="n">decaySteps</span><span class="p">:</span> <span class="mi">1000</span>
        <span class="p">)</span>
    <span class="p">}</span>

    <span class="kd">func</span> <span class="nf">trainEpoch</span><span class="p">(</span><span class="kc">_</span> <span class="n">dataLoader</span><span class="p">:</span> <span class="n">SpeechDataLoader</span><span class="p">,</span> <span class="n">epoch</span><span class="p">:</span> <span class="nb">Int</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="p">(</span><span class="nb">Float</span><span class="p">,</span> <span class="nb">Float</span><span class="p">)</span> <span class="p">{</span>
        <span class="kd">var</span> <span class="nv">totalLoss</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">=</span> <span class="mf">0.0</span>
        <span class="kd">var</span> <span class="nv">totalBatches</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">0</span>

        <span class="c1">// Training phase</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="k">in</span> <span class="n">dataLoader</span> <span class="p">{</span>
            <span class="kd">let</span> <span class="p">(</span><span class="n">spectrograms</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">inputLengths</span><span class="p">,</span> <span class="n">targetLengths</span><span class="p">)</span> <span class="p">=</span> <span class="n">batch</span>

            <span class="c1">// Forward pass</span>
            <span class="kd">let</span> <span class="nv">predictions</span> <span class="p">=</span> <span class="n">model</span><span class="p">(</span><span class="n">spectrograms</span><span class="p">)</span>

            <span class="c1">// Compute CTC loss</span>
            <span class="kd">let</span> <span class="nv">loss</span> <span class="p">=</span> <span class="n">computeCTCLoss</span><span class="p">(</span>
                <span class="n">predictions</span><span class="p">:</span> <span class="n">predictions</span><span class="p">,</span>
                <span class="n">targets</span><span class="p">:</span> <span class="n">targets</span><span class="p">,</span>
                <span class="n">inputLengths</span><span class="p">:</span> <span class="n">inputLengths</span><span class="p">,</span>
                <span class="n">targetLengths</span><span class="p">:</span> <span class="n">targetLengths</span>
            <span class="p">)</span>

            <span class="c1">// Backward pass and gradient computation</span>
            <span class="kd">let</span> <span class="nv">gradients</span> <span class="p">=</span> <span class="n">MLX</span><span class="p">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">())</span>

            <span class="c1">// Gradient clipping</span>
            <span class="kd">let</span> <span class="nv">clippedGradients</span> <span class="p">=</span> <span class="n">clipGradients</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">maxNorm</span><span class="p">:</span> <span class="n">gradientClipNorm</span><span class="p">)</span>

            <span class="c1">// Update model parameters</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="p">&amp;</span><span class="n">model</span><span class="p">,</span> <span class="n">gradients</span><span class="p">:</span> <span class="n">clippedGradients</span><span class="p">)</span>

            <span class="c1">// Update learning rate</span>
            <span class="n">learningRateScheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="n">learningRate</span> <span class="p">=</span> <span class="n">learningRateScheduler</span><span class="p">.</span><span class="n">currentLR</span>

            <span class="n">totalLoss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">totalBatches</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1">// Log progress every 100 batches</span>
            <span class="k">if</span> <span class="n">totalBatches</span> <span class="o">%</span> <span class="mi">100</span> <span class="p">==</span> <span class="mi">0</span> <span class="p">{</span>
                <span class="kd">let</span> <span class="nv">avgLoss</span> <span class="p">=</span> <span class="n">totalLoss</span> <span class="o">/</span> <span class="nb">Float</span><span class="p">(</span><span class="n">totalBatches</span><span class="p">)</span>
                <span class="bp">print</span><span class="p">(</span><span class="s">&quot;Epoch </span><span class="si">\(</span><span class="n">epoch</span><span class="si">)</span><span class="s">, Batch </span><span class="si">\(</span><span class="n">totalBatches</span><span class="si">)</span><span class="s">, Loss: </span><span class="si">\(</span><span class="n">avgLoss</span><span class="si">)</span><span class="s">, LR: </span><span class="si">\(</span><span class="n">optimizer</span><span class="p">.</span><span class="n">learningRate</span><span class="si">)</span><span class="s">&quot;</span><span class="p">)</span>
            <span class="p">}</span>
        <span class="p">}</span>

        <span class="kd">let</span> <span class="nv">avgLoss</span> <span class="p">=</span> <span class="n">totalLoss</span> <span class="o">/</span> <span class="nb">Float</span><span class="p">(</span><span class="n">totalBatches</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">avgLoss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">learningRate</span><span class="p">)</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="kd">class</span> <span class="nc">ExponentialDecayScheduler</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nv">initialLR</span><span class="p">:</span> <span class="nb">Float</span>
    <span class="kd">let</span> <span class="nv">decayRate</span><span class="p">:</span> <span class="nb">Float</span>
    <span class="kd">let</span> <span class="nv">decaySteps</span><span class="p">:</span> <span class="nb">Int</span>
    <span class="kd">private</span> <span class="kd">var</span> <span class="nv">step</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">0</span>

    <span class="kd">init</span><span class="p">(</span><span class="n">initialLR</span><span class="p">:</span> <span class="nb">Float</span><span class="p">,</span> <span class="n">decayRate</span><span class="p">:</span> <span class="nb">Float</span><span class="p">,</span> <span class="n">decaySteps</span><span class="p">:</span> <span class="nb">Int</span><span class="p">)</span> <span class="p">{</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">initialLR</span> <span class="p">=</span> <span class="n">initialLR</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">decayRate</span> <span class="p">=</span> <span class="n">decayRate</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">decaySteps</span> <span class="p">=</span> <span class="n">decaySteps</span>
    <span class="p">}</span>

    <span class="kd">var</span> <span class="nv">currentLR</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">{</span>
        <span class="kd">let</span> <span class="nv">decayFactor</span> <span class="p">=</span> <span class="n">pow</span><span class="p">(</span><span class="n">decayRate</span><span class="p">,</span> <span class="nb">Float</span><span class="p">(</span><span class="n">step</span> <span class="o">/</span> <span class="n">decaySteps</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">initialLR</span> <span class="o">*</span> <span class="n">decayFactor</span>
    <span class="p">}</span>

    <span class="kd">func</span> <span class="nf">step</span><span class="p">()</span> <span class="p">{</span>
        <span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="training-optimization-strategies">Training Optimization Strategies</h3>
<h4 id="curriculum-learning-and-data-ordering">Curriculum Learning and Data Ordering</h4>
<p>DeepSpeech employed sophisticated data ordering strategies:</p>
<div class="highlight"><pre><span></span><code><span class="kd">class</span> <span class="nc">CurriculumLearningScheduler</span> <span class="p">{</span>
    <span class="kd">private</span> <span class="kd">let</span> <span class="nv">maxUtteranceLength</span><span class="p">:</span> <span class="nb">Float</span>
    <span class="kd">private</span> <span class="kd">let</span> <span class="nv">epochsToFullLength</span><span class="p">:</span> <span class="nb">Int</span>

    <span class="kd">init</span><span class="p">(</span><span class="n">maxUtteranceLength</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">=</span> <span class="mf">16.0</span><span class="p">,</span> <span class="n">epochsToFullLength</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">5</span><span class="p">)</span> <span class="p">{</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">maxUtteranceLength</span> <span class="p">=</span> <span class="n">maxUtteranceLength</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">epochsToFullLength</span> <span class="p">=</span> <span class="n">epochsToFullLength</span>
    <span class="p">}</span>

    <span class="kd">func</span> <span class="nf">getMaxLengthForEpoch</span><span class="p">(</span><span class="kc">_</span> <span class="n">epoch</span><span class="p">:</span> <span class="nb">Int</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="nb">Float</span> <span class="p">{</span>
        <span class="c1">// Gradually increase maximum utterance length</span>
        <span class="kd">let</span> <span class="nv">progress</span> <span class="p">=</span> <span class="bp">min</span><span class="p">(</span><span class="nb">Float</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span> <span class="o">/</span> <span class="nb">Float</span><span class="p">(</span><span class="n">epochsToFullLength</span><span class="p">),</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">progress</span> <span class="o">*</span> <span class="n">maxUtteranceLength</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">progress</span><span class="p">)</span> <span class="o">*</span> <span class="mf">2.0</span>  <span class="c1">// Start with 2-second clips</span>
    <span class="p">}</span>

    <span class="kd">func</span> <span class="nf">shouldIncludeUtterance</span><span class="p">(</span><span class="n">length</span><span class="p">:</span> <span class="nb">Float</span><span class="p">,</span> <span class="n">epoch</span><span class="p">:</span> <span class="nb">Int</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="nb">Bool</span> <span class="p">{</span>
        <span class="k">return</span> <span class="n">length</span> <span class="o">&lt;=</span> <span class="n">getMaxLengthForEpoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="c1">// Sort training data by length for efficient batching</span>
<span class="kd">func</span> <span class="nf">sortedBatchSampling</span><span class="p">(</span><span class="kc">_</span> <span class="n">dataset</span><span class="p">:</span> <span class="n">SpeechDataset</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">:</span> <span class="nb">Int</span><span class="p">,</span> <span class="n">epoch</span><span class="p">:</span> <span class="nb">Int</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="p">[[</span><span class="nb">Int</span><span class="p">]]</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nv">curriculumScheduler</span> <span class="p">=</span> <span class="n">CurriculumLearningScheduler</span><span class="p">()</span>
    <span class="kd">let</span> <span class="nv">maxLength</span> <span class="p">=</span> <span class="n">curriculumScheduler</span><span class="p">.</span><span class="n">getMaxLengthForEpoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>

    <span class="c1">// Filter utterances by current maximum length</span>
    <span class="kd">var</span> <span class="nv">validIndices</span><span class="p">:</span> <span class="p">[(</span><span class="nb">Int</span><span class="p">,</span> <span class="nb">Float</span><span class="p">)]</span> <span class="p">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mf">0.</span><span class="p">.&lt;</span><span class="n">dataset</span><span class="p">.</span><span class="bp">count</span> <span class="p">{</span>
        <span class="kd">let</span> <span class="nv">utteranceLength</span> <span class="p">=</span> <span class="n">dataset</span><span class="p">.</span><span class="n">getUtteranceLength</span><span class="p">(</span><span class="n">at</span><span class="p">:</span> <span class="n">i</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">curriculumScheduler</span><span class="p">.</span><span class="n">shouldIncludeUtterance</span><span class="p">(</span><span class="n">length</span><span class="p">:</span> <span class="n">utteranceLength</span><span class="p">,</span> <span class="n">epoch</span><span class="p">:</span> <span class="n">epoch</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">validIndices</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">utteranceLength</span><span class="p">))</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="c1">// Sort by length for efficient padding</span>
    <span class="n">validIndices</span><span class="p">.</span><span class="bp">sort</span> <span class="p">{</span> <span class="nv">$0</span><span class="p">.</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="nv">$1</span><span class="p">.</span><span class="mi">1</span> <span class="p">}</span>

    <span class="c1">// Create batches with similar lengths</span>
    <span class="kd">var</span> <span class="nv">batches</span><span class="p">:</span> <span class="p">[[</span><span class="nb">Int</span><span class="p">]]</span> <span class="p">=</span> <span class="p">[]</span>
    <span class="kd">let</span> <span class="nv">indices</span> <span class="p">=</span> <span class="n">validIndices</span><span class="p">.</span><span class="bp">map</span> <span class="p">{</span> <span class="nv">$0</span><span class="p">.</span><span class="mi">0</span> <span class="p">}</span>

    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="bp">stride</span><span class="p">(</span><span class="n">from</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">to</span><span class="p">:</span> <span class="bp">indices</span><span class="p">.</span><span class="bp">count</span><span class="p">,</span> <span class="n">by</span><span class="p">:</span> <span class="n">batchSize</span><span class="p">)</span> <span class="p">{</span>
        <span class="kd">let</span> <span class="nv">endIdx</span> <span class="p">=</span> <span class="bp">min</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">batchSize</span><span class="p">,</span> <span class="bp">indices</span><span class="p">.</span><span class="bp">count</span><span class="p">)</span>
        <span class="n">batches</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nb">Array</span><span class="p">(</span><span class="bp">indices</span><span class="p">[</span><span class="n">i</span><span class="p">..&lt;</span><span class="n">endIdx</span><span class="p">]))</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">batches</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="advanced-training-techniques">Advanced Training Techniques</h3>
<h4 id="batch-normalization-and-regularization">Batch Normalization and Regularization</h4>
<div class="highlight"><pre><span></span><code><span class="kd">class</span> <span class="nc">RegularizedDeepSpeech</span><span class="p">:</span> <span class="n">Module</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nv">baseModel</span><span class="p">:</span> <span class="n">DeepSpeech</span>
    <span class="kd">let</span> <span class="nv">dropout</span><span class="p">:</span> <span class="n">Dropout</span>
    <span class="kd">let</span> <span class="nv">weightDecay</span><span class="p">:</span> <span class="nb">Float</span>

    <span class="kd">init</span><span class="p">(</span><span class="n">baseModel</span><span class="p">:</span> <span class="n">DeepSpeech</span><span class="p">,</span> <span class="n">dropoutRate</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">weightDecay</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">=</span> <span class="mf">1e-5</span><span class="p">)</span> <span class="p">{</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">baseModel</span> <span class="p">=</span> <span class="n">baseModel</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">dropout</span> <span class="p">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">:</span> <span class="n">dropoutRate</span><span class="p">)</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">weightDecay</span> <span class="p">=</span> <span class="n">weightDecay</span>
    <span class="p">}</span>

    <span class="kd">func</span> <span class="nf">callAsFunction</span><span class="p">(</span><span class="kc">_</span> <span class="n">x</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">,</span> <span class="n">training</span><span class="p">:</span> <span class="nb">Bool</span> <span class="p">=</span> <span class="kc">true</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">MLXArray</span> <span class="p">{</span>
        <span class="kd">var</span> <span class="nv">output</span> <span class="p">=</span> <span class="n">baseModel</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">training</span> <span class="p">{</span>
            <span class="c1">// Apply dropout during training</span>
            <span class="n">output</span> <span class="p">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="p">}</span>

        <span class="k">return</span> <span class="n">output</span>
    <span class="p">}</span>

    <span class="kd">func</span> <span class="nf">regularizationLoss</span><span class="p">()</span> <span class="p">-&gt;</span> <span class="n">MLXArray</span> <span class="p">{</span>
        <span class="c1">// L2 weight decay</span>
        <span class="kd">var</span> <span class="nv">totalWeightNorm</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">=</span> <span class="mf">0.0</span>

        <span class="k">for</span> <span class="n">parameter</span> <span class="k">in</span> <span class="n">baseModel</span><span class="p">.</span><span class="n">parameters</span><span class="p">().</span><span class="n">values</span> <span class="p">{</span>
            <span class="n">totalWeightNorm</span> <span class="o">+=</span> <span class="n">pow</span><span class="p">(</span><span class="n">parameter</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="n">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
        <span class="p">}</span>

        <span class="k">return</span> <span class="n">MLXArray</span><span class="p">(</span><span class="n">weightDecay</span> <span class="o">*</span> <span class="n">totalWeightNorm</span><span class="p">)</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h4 id="learning-rate-warm-up-and-annealing">Learning Rate Warm-up and Annealing</h4>
<div class="highlight"><pre><span></span><code><span class="kd">class</span> <span class="nc">WarmupLearningRateScheduler</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nv">warmupSteps</span><span class="p">:</span> <span class="nb">Int</span>
    <span class="kd">let</span> <span class="nv">maxLR</span><span class="p">:</span> <span class="nb">Float</span>
    <span class="kd">let</span> <span class="nv">totalSteps</span><span class="p">:</span> <span class="nb">Int</span>
    <span class="kd">private</span> <span class="kd">var</span> <span class="nv">currentStep</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">0</span>

    <span class="kd">init</span><span class="p">(</span><span class="n">warmupSteps</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">maxLR</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">=</span> <span class="mf">3e-4</span><span class="p">,</span> <span class="n">totalSteps</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">100000</span><span class="p">)</span> <span class="p">{</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">warmupSteps</span> <span class="p">=</span> <span class="n">warmupSteps</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">maxLR</span> <span class="p">=</span> <span class="n">maxLR</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">totalSteps</span> <span class="p">=</span> <span class="n">totalSteps</span>
    <span class="p">}</span>

    <span class="kd">var</span> <span class="nv">currentLR</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">{</span>
        <span class="k">if</span> <span class="n">currentStep</span> <span class="o">&lt;</span> <span class="n">warmupSteps</span> <span class="p">{</span>
            <span class="c1">// Linear warmup</span>
            <span class="k">return</span> <span class="n">maxLR</span> <span class="o">*</span> <span class="nb">Float</span><span class="p">(</span><span class="n">currentStep</span><span class="p">)</span> <span class="o">/</span> <span class="nb">Float</span><span class="p">(</span><span class="n">warmupSteps</span><span class="p">)</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="c1">// Cosine annealing</span>
            <span class="kd">let</span> <span class="nv">progress</span> <span class="p">=</span> <span class="nb">Float</span><span class="p">(</span><span class="n">currentStep</span> <span class="o">-</span> <span class="n">warmupSteps</span><span class="p">)</span> <span class="o">/</span> <span class="nb">Float</span><span class="p">(</span><span class="n">totalSteps</span> <span class="o">-</span> <span class="n">warmupSteps</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">maxLR</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">cos</span><span class="p">(</span><span class="nb">Float</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">progress</span><span class="p">))</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="kd">func</span> <span class="nf">step</span><span class="p">()</span> <span class="p">{</span>
        <span class="n">currentStep</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="model-validation-and-early-stopping">Model Validation and Early Stopping</h3>
<h4 id="validation-strategy">Validation Strategy</h4>
<div class="highlight"><pre><span></span><code><span class="kd">class</span> <span class="nc">ValidationMonitor</span> <span class="p">{</span>
    <span class="kd">private</span> <span class="kd">var</span> <span class="nv">bestValidationLoss</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">=</span> <span class="nb">Float</span><span class="p">.</span><span class="n">infinity</span>
    <span class="kd">private</span> <span class="kd">var</span> <span class="nv">patienceCounter</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">0</span>
    <span class="kd">private</span> <span class="kd">let</span> <span class="nv">patience</span><span class="p">:</span> <span class="nb">Int</span>
    <span class="kd">private</span> <span class="kd">let</span> <span class="nv">minImprovement</span><span class="p">:</span> <span class="nb">Float</span>

    <span class="kd">init</span><span class="p">(</span><span class="n">patience</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">minImprovement</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">=</span> <span class="mf">1e-4</span><span class="p">)</span> <span class="p">{</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">patience</span> <span class="p">=</span> <span class="n">patience</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">minImprovement</span> <span class="p">=</span> <span class="n">minImprovement</span>
    <span class="p">}</span>

    <span class="kd">func</span> <span class="nf">validateModel</span><span class="p">(</span><span class="kc">_</span> <span class="n">model</span><span class="p">:</span> <span class="n">DeepSpeech</span><span class="p">,</span> <span class="kc">_</span> <span class="n">validationLoader</span><span class="p">:</span> <span class="n">SpeechDataLoader</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="p">(</span><span class="nb">Float</span><span class="p">,</span> <span class="nb">Float</span><span class="p">,</span> <span class="nb">Bool</span><span class="p">)</span> <span class="p">{</span>
        <span class="kd">var</span> <span class="nv">totalLoss</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">=</span> <span class="mf">0.0</span>
        <span class="kd">var</span> <span class="nv">totalWER</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">=</span> <span class="mf">0.0</span>
        <span class="kd">var</span> <span class="nv">batchCount</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">0</span>

        <span class="n">MLX</span><span class="p">.</span><span class="n">eval</span> <span class="p">{</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="k">in</span> <span class="n">validationLoader</span> <span class="p">{</span>
                <span class="kd">let</span> <span class="p">(</span><span class="n">spectrograms</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">inputLengths</span><span class="p">,</span> <span class="n">targetLengths</span><span class="p">)</span> <span class="p">=</span> <span class="n">batch</span>

                <span class="c1">// Forward pass without gradient computation</span>
                <span class="kd">let</span> <span class="nv">predictions</span> <span class="p">=</span> <span class="n">model</span><span class="p">(</span><span class="n">spectrograms</span><span class="p">)</span>

                <span class="c1">// Compute validation loss</span>
                <span class="kd">let</span> <span class="nv">loss</span> <span class="p">=</span> <span class="n">computeCTCLoss</span><span class="p">(</span>
                    <span class="n">predictions</span><span class="p">:</span> <span class="n">predictions</span><span class="p">,</span>
                    <span class="n">targets</span><span class="p">:</span> <span class="n">targets</span><span class="p">,</span>
                    <span class="n">inputLengths</span><span class="p">:</span> <span class="n">inputLengths</span><span class="p">,</span>
                    <span class="n">targetLengths</span><span class="p">:</span> <span class="n">targetLengths</span>
                <span class="p">)</span>

                <span class="c1">// Compute Word Error Rate</span>
                <span class="kd">let</span> <span class="nv">wer</span> <span class="p">=</span> <span class="n">computeWordErrorRate</span><span class="p">(</span><span class="n">predictions</span><span class="p">:</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">:</span> <span class="n">targets</span><span class="p">)</span>

                <span class="n">totalLoss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">totalWER</span> <span class="o">+=</span> <span class="n">wer</span>
                <span class="n">batchCount</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="p">}</span>
        <span class="p">}</span>

        <span class="kd">let</span> <span class="nv">avgLoss</span> <span class="p">=</span> <span class="n">totalLoss</span> <span class="o">/</span> <span class="nb">Float</span><span class="p">(</span><span class="n">batchCount</span><span class="p">)</span>
        <span class="kd">let</span> <span class="nv">avgWER</span> <span class="p">=</span> <span class="n">totalWER</span> <span class="o">/</span> <span class="nb">Float</span><span class="p">(</span><span class="n">batchCount</span><span class="p">)</span>

        <span class="c1">// Check for early stopping</span>
        <span class="kd">let</span> <span class="nv">shouldStop</span> <span class="p">=</span> <span class="n">checkEarlyStopping</span><span class="p">(</span><span class="n">validationLoss</span><span class="p">:</span> <span class="n">avgLoss</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">avgLoss</span><span class="p">,</span> <span class="n">avgWER</span><span class="p">,</span> <span class="n">shouldStop</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="kd">private</span> <span class="kd">func</span> <span class="nf">checkEarlyStopping</span><span class="p">(</span><span class="n">validationLoss</span><span class="p">:</span> <span class="nb">Float</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="nb">Bool</span> <span class="p">{</span>
        <span class="k">if</span> <span class="n">validationLoss</span> <span class="o">&lt;</span> <span class="n">bestValidationLoss</span> <span class="o">-</span> <span class="n">minImprovement</span> <span class="p">{</span>
            <span class="n">bestValidationLoss</span> <span class="p">=</span> <span class="n">validationLoss</span>
            <span class="n">patienceCounter</span> <span class="p">=</span> <span class="mi">0</span>
            <span class="k">return</span> <span class="kc">false</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="n">patienceCounter</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">return</span> <span class="n">patienceCounter</span> <span class="o">&gt;=</span> <span class="n">patience</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="kd">func</span> <span class="nf">computeWordErrorRate</span><span class="p">(</span><span class="n">predictions</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">,</span> <span class="n">targets</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="nb">Float</span> <span class="p">{</span>
    <span class="c1">// Simplified WER computation</span>
    <span class="kd">let</span> <span class="nv">batchSize</span> <span class="p">=</span> <span class="n">predictions</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="kd">var</span> <span class="nv">totalWER</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">=</span> <span class="mf">0.0</span>

    <span class="k">for</span> <span class="n">b</span> <span class="k">in</span> <span class="mf">0.</span><span class="p">.&lt;</span><span class="n">batchSize</span> <span class="p">{</span>
        <span class="kd">let</span> <span class="nv">predictionSeq</span> <span class="p">=</span> <span class="n">ctcDecode</span><span class="p">(</span><span class="n">predictions</span><span class="p">:</span> <span class="n">predictions</span><span class="p">[</span><span class="n">b</span><span class="p">])</span>
        <span class="kd">let</span> <span class="nv">targetSeq</span> <span class="p">=</span> <span class="nb">Array</span><span class="p">(</span><span class="n">targets</span><span class="p">[</span><span class="n">b</span><span class="p">].</span><span class="n">asArray</span><span class="p">(</span><span class="nb">Int</span><span class="p">.</span><span class="kc">self</span><span class="p">))</span>

        <span class="kd">let</span> <span class="nv">wer</span> <span class="p">=</span> <span class="n">editDistance</span><span class="p">(</span><span class="n">predictionSeq</span><span class="p">,</span> <span class="n">targetSeq</span><span class="p">)</span> <span class="o">/</span> <span class="nb">Float</span><span class="p">(</span><span class="n">targetSeq</span><span class="p">.</span><span class="bp">count</span><span class="p">)</span>
        <span class="n">totalWER</span> <span class="o">+=</span> <span class="n">wer</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">totalWER</span> <span class="o">/</span> <span class="nb">Float</span><span class="p">(</span><span class="n">batchSize</span><span class="p">)</span>
<span class="p">}</span>

<span class="kd">func</span> <span class="nf">editDistance</span><span class="p">&lt;</span><span class="n">T</span><span class="p">:</span> <span class="nb">Equatable</span><span class="p">&gt;(</span><span class="kc">_</span> <span class="n">a</span><span class="p">:</span> <span class="p">[</span><span class="n">T</span><span class="p">],</span> <span class="kc">_</span> <span class="n">b</span><span class="p">:</span> <span class="p">[</span><span class="n">T</span><span class="p">])</span> <span class="p">-&gt;</span> <span class="nb">Float</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nv">m</span> <span class="p">=</span> <span class="n">a</span><span class="p">.</span><span class="bp">count</span>
    <span class="kd">let</span> <span class="nv">n</span> <span class="p">=</span> <span class="n">b</span><span class="p">.</span><span class="bp">count</span>

    <span class="kd">var</span> <span class="nv">dp</span> <span class="p">=</span> <span class="nb">Array</span><span class="p">(</span><span class="n">repeating</span><span class="p">:</span> <span class="nb">Array</span><span class="p">(</span><span class="n">repeating</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">count</span><span class="p">:</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="bp">count</span><span class="p">:</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1">// Initialize base cases</span>
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mf">0.</span><span class="p">..</span><span class="n">m</span> <span class="p">{</span> <span class="n">dp</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="p">=</span> <span class="n">i</span> <span class="p">}</span>
    <span class="k">for</span> <span class="n">j</span> <span class="k">in</span> <span class="mf">0.</span><span class="p">..</span><span class="n">n</span> <span class="p">{</span> <span class="n">dp</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="p">=</span> <span class="n">j</span> <span class="p">}</span>

    <span class="c1">// Fill the DP table</span>
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mf">1.</span><span class="p">..</span><span class="n">m</span> <span class="p">{</span>
        <span class="k">for</span> <span class="n">j</span> <span class="k">in</span> <span class="mf">1.</span><span class="p">..</span><span class="n">n</span> <span class="p">{</span>
            <span class="k">if</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="p">==</span> <span class="n">b</span><span class="p">[</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="p">{</span>
                <span class="n">dp</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="p">=</span> <span class="n">dp</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
                <span class="n">dp</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="p">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="bp">min</span><span class="p">(</span><span class="n">dp</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="p">],</span> <span class="n">dp</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dp</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="nb">Float</span><span class="p">(</span><span class="n">dp</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">n</span><span class="p">])</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="complete-training-loop-implementation">Complete Training Loop Implementation</h3>
<div class="highlight"><pre><span></span><code><span class="kd">class</span> <span class="nc">DeepSpeechTrainingPipeline</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nv">model</span><span class="p">:</span> <span class="n">RegularizedDeepSpeech</span>
    <span class="kd">let</span> <span class="nv">trainer</span><span class="p">:</span> <span class="n">DistributedDeepSpeechTrainer</span>
    <span class="kd">let</span> <span class="nv">validationMonitor</span><span class="p">:</span> <span class="n">ValidationMonitor</span>
    <span class="kd">let</span> <span class="nv">curriculumScheduler</span><span class="p">:</span> <span class="n">CurriculumLearningScheduler</span>

    <span class="kd">init</span><span class="p">()</span> <span class="p">{</span>
        <span class="kd">let</span> <span class="nv">baseModel</span> <span class="p">=</span> <span class="n">DeepSpeech</span><span class="p">(</span><span class="n">inputSize</span><span class="p">:</span> <span class="mi">161</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">:</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">numLayers</span><span class="p">:</span> <span class="mi">5</span><span class="p">)</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">model</span> <span class="p">=</span> <span class="n">RegularizedDeepSpeech</span><span class="p">(</span><span class="n">baseModel</span><span class="p">:</span> <span class="n">baseModel</span><span class="p">)</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">trainer</span> <span class="p">=</span> <span class="n">DistributedDeepSpeechTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">baseModel</span><span class="p">)</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">validationMonitor</span> <span class="p">=</span> <span class="n">ValidationMonitor</span><span class="p">()</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">curriculumScheduler</span> <span class="p">=</span> <span class="n">CurriculumLearningScheduler</span><span class="p">()</span>
    <span class="p">}</span>

    <span class="kd">func</span> <span class="nf">train</span><span class="p">(</span>
        <span class="n">trainDataset</span><span class="p">:</span> <span class="n">SpeechDataset</span><span class="p">,</span>
        <span class="n">validationDataset</span><span class="p">:</span> <span class="n">SpeechDataset</span><span class="p">,</span>
        <span class="n">maxEpochs</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">batchSize</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">256</span>
    <span class="p">)</span> <span class="p">{</span>
        <span class="bp">print</span><span class="p">(</span><span class="s">&quot;Starting DeepSpeech Training...&quot;</span><span class="p">)</span>
        <span class="bp">print</span><span class="p">(</span><span class="s">&quot;Training samples: </span><span class="si">\(</span><span class="n">trainDataset</span><span class="p">.</span><span class="bp">count</span><span class="si">)</span><span class="s">&quot;</span><span class="p">)</span>
        <span class="bp">print</span><span class="p">(</span><span class="s">&quot;Validation samples: </span><span class="si">\(</span><span class="n">validationDataset</span><span class="p">.</span><span class="bp">count</span><span class="si">)</span><span class="s">&quot;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">epoch</span> <span class="k">in</span> <span class="mf">0.</span><span class="p">.&lt;</span><span class="n">maxEpochs</span> <span class="p">{</span>
            <span class="bp">print</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">=== Epoch </span><span class="si">\(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">)</span><span class="s">/</span><span class="si">\(</span><span class="n">maxEpochs</span><span class="si">)</span><span class="s"> ===&quot;</span><span class="p">)</span>

            <span class="c1">// Create curriculum-based batches</span>
            <span class="kd">let</span> <span class="nv">batches</span> <span class="p">=</span> <span class="n">sortedBatchSampling</span><span class="p">(</span><span class="n">trainDataset</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">:</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">epoch</span><span class="p">:</span> <span class="n">epoch</span><span class="p">)</span>
            <span class="kd">let</span> <span class="nv">trainLoader</span> <span class="p">=</span> <span class="n">SpeechDataLoader</span><span class="p">(</span><span class="n">batches</span><span class="p">:</span> <span class="n">batches</span><span class="p">,</span> <span class="n">dataset</span><span class="p">:</span> <span class="n">trainDataset</span><span class="p">)</span>

            <span class="c1">// Training phase</span>
            <span class="kd">let</span> <span class="p">(</span><span class="n">trainLoss</span><span class="p">,</span> <span class="n">currentLR</span><span class="p">)</span> <span class="p">=</span> <span class="n">trainer</span><span class="p">.</span><span class="n">trainEpoch</span><span class="p">(</span><span class="n">trainLoader</span><span class="p">,</span> <span class="n">epoch</span><span class="p">:</span> <span class="n">epoch</span><span class="p">)</span>

            <span class="c1">// Validation phase</span>
            <span class="kd">let</span> <span class="nv">validationLoader</span> <span class="p">=</span> <span class="n">SpeechDataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">:</span> <span class="n">validationDataset</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">:</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">shuffle</span><span class="p">:</span> <span class="kc">false</span><span class="p">)</span>
            <span class="kd">let</span> <span class="p">(</span><span class="n">validLoss</span><span class="p">,</span> <span class="n">validWER</span><span class="p">,</span> <span class="n">shouldStop</span><span class="p">)</span> <span class="p">=</span> <span class="n">validationMonitor</span><span class="p">.</span><span class="n">validateModel</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">baseModel</span><span class="p">,</span> <span class="n">validationLoader</span><span class="p">)</span>

            <span class="c1">// Logging</span>
            <span class="bp">print</span><span class="p">(</span><span class="s">&quot;Training Loss: </span><span class="si">\(</span><span class="n">trainLoss</span><span class="si">)</span><span class="s">&quot;</span><span class="p">)</span>
            <span class="bp">print</span><span class="p">(</span><span class="s">&quot;Validation Loss: </span><span class="si">\(</span><span class="n">validLoss</span><span class="si">)</span><span class="s">&quot;</span><span class="p">)</span>
            <span class="bp">print</span><span class="p">(</span><span class="s">&quot;Validation WER: </span><span class="si">\(</span><span class="n">validWER</span> <span class="o">*</span> <span class="mi">100</span><span class="si">)</span><span class="s">%&quot;</span><span class="p">)</span>
            <span class="bp">print</span><span class="p">(</span><span class="s">&quot;Learning Rate: </span><span class="si">\(</span><span class="n">currentLR</span><span class="si">)</span><span class="s">&quot;</span><span class="p">)</span>
            <span class="bp">print</span><span class="p">(</span><span class="s">&quot;Max Utterance Length: </span><span class="si">\(</span><span class="n">curriculumScheduler</span><span class="p">.</span><span class="n">getMaxLengthForEpoch</span><span class="si">(</span><span class="n">epoch</span><span class="si">))</span><span class="s">s&quot;</span><span class="p">)</span>

            <span class="c1">// Save checkpoint</span>
            <span class="n">saveModelCheckpoint</span><span class="p">(</span><span class="n">epoch</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">model</span><span class="p">.</span><span class="n">baseModel</span><span class="p">,</span> <span class="n">validationLoss</span><span class="p">:</span> <span class="n">validLoss</span><span class="p">)</span>

            <span class="c1">// Early stopping</span>
            <span class="k">if</span> <span class="n">shouldStop</span> <span class="p">{</span>
                <span class="bp">print</span><span class="p">(</span><span class="s">&quot;Early stopping triggered at epoch </span><span class="si">\(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">)</span><span class="s">&quot;</span><span class="p">)</span>
                <span class="k">break</span>
            <span class="p">}</span>
        <span class="p">}</span>

        <span class="bp">print</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">Training completed!&quot;</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="kd">private</span> <span class="kd">func</span> <span class="nf">saveModelCheckpoint</span><span class="p">(</span><span class="n">epoch</span><span class="p">:</span> <span class="nb">Int</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">DeepSpeech</span><span class="p">,</span> <span class="n">validationLoss</span><span class="p">:</span> <span class="nb">Float</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// Save model parameters and training state</span>
        <span class="kd">let</span> <span class="nv">checkpointPath</span> <span class="p">=</span> <span class="s">&quot;checkpoints/deepspeech_epoch_</span><span class="si">\(</span><span class="n">epoch</span><span class="si">)</span><span class="s">.mlx&quot;</span>
        <span class="c1">// Implementation would save model state using MLX&#39;s serialization</span>
        <span class="bp">print</span><span class="p">(</span><span class="s">&quot;Saved checkpoint: </span><span class="si">\(</span><span class="n">checkpointPath</span><span class="si">)</span><span class="s">&quot;</span><span class="p">)</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h2 id="deepspeech-improvements-and-variants">DeepSpeech Improvements and Variants</h2>
<h3 id="deepspeech-2-enhancements">DeepSpeech 2 Enhancements</h3>
<ol>
<li><strong>Batch Normalization</strong>: Added between RNN layers for better training stability</li>
<li><strong>Convolutional Layers</strong>: Optional conv layers before RNN for local feature extraction</li>
<li><strong>Improved Regularization</strong>: Dropout and weight decay</li>
<li><strong>Better Optimization</strong>: Advanced learning rate scheduling</li>
</ol>
<h3 id="architectural-variants">Architectural Variants</h3>
<div class="highlight"><pre><span></span><code><span class="kd">import</span> <span class="nc">MLX</span>
<span class="kd">import</span> <span class="nc">MLXNN</span>

<span class="kd">class</span> <span class="nc">DeepSpeech2</span><span class="p">:</span> <span class="n">Module</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nv">convLayers</span><span class="p">:</span> <span class="n">Sequential</span>
    <span class="kd">let</span> <span class="nv">rnnLayers</span><span class="p">:</span> <span class="p">[</span><span class="n">LSTM</span><span class="p">]</span>
    <span class="kd">let</span> <span class="nv">batchNorms</span><span class="p">:</span> <span class="p">[</span><span class="n">BatchNorm</span><span class="p">]</span>
    <span class="kd">let</span> <span class="nv">outputLayer</span><span class="p">:</span> <span class="n">Linear</span>

    <span class="kd">init</span><span class="p">(</span><span class="n">inputSize</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">161</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">numLayers</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">7</span><span class="p">,</span> <span class="n">numClasses</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">29</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// Optional: Convolutional layers</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">convLayers</span> <span class="p">=</span> <span class="n">Sequential</span><span class="p">([</span>
            <span class="n">Conv2d</span><span class="p">(</span><span class="n">inputChannels</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">outputChannels</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernelSize</span><span class="p">:</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="bp">stride</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="p">:</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
            <span class="n">BatchNorm</span><span class="p">(</span><span class="n">featureCount</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="n">axes</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span>
            <span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">Conv2d</span><span class="p">(</span><span class="n">inputChannels</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="n">outputChannels</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernelSize</span><span class="p">:</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="bp">stride</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="p">:</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
            <span class="n">BatchNorm</span><span class="p">(</span><span class="n">featureCount</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="n">axes</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span>
            <span class="n">ReLU</span><span class="p">()</span>
        <span class="p">])</span>

        <span class="c1">// Calculate conv output size</span>
        <span class="kd">let</span> <span class="nv">convOutputSize</span> <span class="p">=</span> <span class="n">getConvOutputSize</span><span class="p">(</span><span class="n">inputSize</span><span class="p">:</span> <span class="n">inputSize</span><span class="p">)</span>

        <span class="c1">// RNN layers with batch normalization</span>
        <span class="kd">var</span> <span class="nv">rnnLayersList</span><span class="p">:</span> <span class="p">[</span><span class="n">LSTM</span><span class="p">]</span> <span class="p">=</span> <span class="p">[]</span>
        <span class="kd">var</span> <span class="nv">batchNormsList</span><span class="p">:</span> <span class="p">[</span><span class="n">BatchNorm</span><span class="p">]</span> <span class="p">=</span> <span class="p">[]</span>

        <span class="c1">// Build RNN stack</span>
        <span class="kd">var</span> <span class="nv">inputDim</span> <span class="p">=</span> <span class="n">convOutputSize</span>
        <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mf">0.</span><span class="p">.&lt;</span><span class="n">numLayers</span> <span class="p">{</span>
            <span class="kd">let</span> <span class="nv">bidirectional</span> <span class="p">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">numLayers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

            <span class="n">rnnLayersList</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">LSTM</span><span class="p">(</span><span class="n">inputSize</span><span class="p">:</span> <span class="n">inputDim</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">:</span> <span class="n">hiddenSize</span><span class="p">,</span> <span class="n">bidirectional</span><span class="p">:</span> <span class="n">bidirectional</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="kd">let</span> <span class="nv">outputDim</span> <span class="p">=</span> <span class="n">hiddenSize</span> <span class="o">*</span> <span class="p">(</span><span class="n">bidirectional</span> <span class="p">?</span> <span class="mi">2</span> <span class="p">:</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">batchNormsList</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">BatchNorm</span><span class="p">(</span><span class="n">featureCount</span><span class="p">:</span> <span class="n">outputDim</span><span class="p">))</span>
            <span class="n">inputDim</span> <span class="p">=</span> <span class="n">outputDim</span>
        <span class="p">}</span>

        <span class="kc">self</span><span class="p">.</span><span class="n">rnnLayers</span> <span class="p">=</span> <span class="n">rnnLayersList</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">batchNorms</span> <span class="p">=</span> <span class="n">batchNormsList</span>

        <span class="c1">// Output layer</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">outputLayer</span> <span class="p">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hiddenSize</span><span class="p">,</span> <span class="n">numClasses</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="kd">func</span> <span class="nf">callAsFunction</span><span class="p">(</span><span class="kc">_</span> <span class="n">x</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">MLXArray</span> <span class="p">{</span>
        <span class="c1">// Add channel dimension for conv layers: (batch, 1, time, freq)</span>
        <span class="kd">var</span> <span class="nv">output</span> <span class="p">=</span> <span class="n">x</span><span class="p">.</span><span class="n">expandedDimensions</span><span class="p">(</span><span class="n">axis</span><span class="p">:</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1">// Apply convolutional layers</span>
        <span class="n">output</span> <span class="p">=</span> <span class="n">convLayers</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="c1">// Reshape for RNN: (batch, time, features)</span>
        <span class="kd">let</span> <span class="nv">batchSize</span> <span class="p">=</span> <span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="kd">let</span> <span class="nv">timeSteps</span> <span class="p">=</span> <span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="kd">let</span> <span class="nv">features</span> <span class="p">=</span> <span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="n">output</span> <span class="p">=</span> <span class="n">output</span><span class="p">.</span><span class="n">reshaped</span><span class="p">([</span><span class="n">batchSize</span><span class="p">,</span> <span class="n">timeSteps</span><span class="p">,</span> <span class="n">features</span><span class="p">])</span>

        <span class="c1">// Apply RNN layers with batch normalization</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="n">bn</span><span class="p">))</span> <span class="k">in</span> <span class="n">zip</span><span class="p">(</span><span class="n">rnnLayers</span><span class="p">,</span> <span class="n">batchNorms</span><span class="p">).</span><span class="n">enumerated</span><span class="p">()</span> <span class="p">{</span>
            <span class="n">output</span> <span class="p">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

            <span class="c1">// Apply batch normalization</span>
            <span class="n">output</span> <span class="p">=</span> <span class="n">output</span><span class="p">.</span><span class="n">transposed</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">// (batch, features, time)</span>
            <span class="n">output</span> <span class="p">=</span> <span class="n">bn</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
            <span class="n">output</span> <span class="p">=</span> <span class="n">output</span><span class="p">.</span><span class="n">transposed</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">// (batch, time, features)</span>
        <span class="p">}</span>

        <span class="c1">// Output layer</span>
        <span class="n">output</span> <span class="p">=</span> <span class="n">outputLayer</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>
    <span class="p">}</span>

    <span class="kd">private</span> <span class="kd">func</span> <span class="nf">getConvOutputSize</span><span class="p">(</span><span class="n">inputSize</span><span class="p">:</span> <span class="nb">Int</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="nb">Int</span> <span class="p">{</span>
        <span class="c1">// Calculate the output size after conv layers</span>
        <span class="c1">// This is a simplified calculation</span>
        <span class="k">return</span> <span class="n">inputSize</span> <span class="o">/</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">32</span> <span class="c1">// Approximate</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h2 id="performance-and-evaluation">Performance and Evaluation</h2>
<h3 id="key-metrics">Key Metrics</h3>
<ol>
<li><strong>Word Error Rate (WER)</strong>: Primary metric for ASR evaluation</li>
</ol>
<p><code>text
   WER = (Substitutions + Deletions + Insertions) / Total_Words</code></p>
<ol>
<li><strong>Character Error Rate (CER)</strong>: Character-level accuracy</li>
</ol>
<p><code>text
   CER = (Character_Errors) / Total_Characters</code></p>
<h3 id="benchmark-results">Benchmark Results</h3>
<p>DeepSpeech achieved competitive results on standard datasets:</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Model</th>
<th>WER (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>LibriSpeech test-clean</td>
<td>DeepSpeech 2</td>
<td>5.33</td>
</tr>
<tr>
<td>LibriSpeech test-other</td>
<td>DeepSpeech 2</td>
<td>13.25</td>
</tr>
<tr>
<td>Wall Street Journal</td>
<td>DeepSpeech</td>
<td>8.2</td>
</tr>
</tbody>
</table>
<h2 id="advantages-and-limitations">Advantages and Limitations</h2>
<h3 id="advantages">Advantages</h3>
<ol>
<li><strong>End-to-End Learning</strong>: Single neural network replaces complex pipeline</li>
<li><strong>No Pronunciation Dictionary</strong>: Learns pronunciation patterns automatically</li>
<li><strong>Language Agnostic</strong>: Can be trained on any language with minimal changes</li>
<li><strong>Scalability</strong>: Performance improves with more data and compute</li>
<li><strong>Simplicity</strong>: Easier to implement and maintain than traditional systems</li>
</ol>
<h3 id="limitations">Limitations</h3>
<ol>
<li><strong>Data Requirements</strong>: Needs large amounts of labeled audio data</li>
<li><strong>Computational Intensity</strong>: Requires significant compute resources for training</li>
<li><strong>Real-time Constraints</strong>: Large models may be too slow for real-time applications</li>
<li><strong>Lack of Explicit Language Model</strong>: May make more grammatical errors</li>
<li><strong>Limited Interpretability</strong>: Difficult to debug or understand model decisions</li>
</ol>
<h2 id="practical-implementation-tips">Practical Implementation Tips</h2>
<h3 id="optimization-strategies">Optimization Strategies</h3>
<ol>
<li><strong>Gradient Clipping</strong>: Essential for stable RNN training</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="c1">// Gradient clipping implementation</span>
<span class="kd">func</span> <span class="nf">clipGradNorm</span><span class="p">(</span><span class="kc">_</span> <span class="n">gradients</span><span class="p">:</span> <span class="p">[</span><span class="nb">String</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">],</span> <span class="n">maxNorm</span><span class="p">:</span> <span class="nb">Float</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="p">[</span><span class="nb">String</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">]</span> <span class="p">{</span>
    <span class="kd">var</span> <span class="nv">totalNorm</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">=</span> <span class="mf">0.0</span>

    <span class="c1">// Compute total norm</span>
    <span class="k">for</span> <span class="p">(</span><span class="kc">_</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span> <span class="k">in</span> <span class="n">gradients</span> <span class="p">{</span>
        <span class="n">totalNorm</span> <span class="o">+=</span> <span class="n">pow</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="n">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
    <span class="p">}</span>
    <span class="n">totalNorm</span> <span class="p">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">totalNorm</span><span class="p">)</span>

    <span class="c1">// Clip gradients if necessary</span>
    <span class="kd">let</span> <span class="nv">clipCoeff</span> <span class="p">=</span> <span class="n">maxNorm</span> <span class="o">/</span> <span class="p">(</span><span class="n">totalNorm</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">clipCoeff</span> <span class="o">&lt;</span> <span class="mf">1.0</span> <span class="p">{</span>
        <span class="kd">var</span> <span class="nv">clippedGrads</span><span class="p">:</span> <span class="p">[</span><span class="nb">String</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">]</span> <span class="p">=</span> <span class="p">[:]</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span> <span class="k">in</span> <span class="n">gradients</span> <span class="p">{</span>
            <span class="n">clippedGrads</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="p">=</span> <span class="n">grad</span> <span class="o">*</span> <span class="n">clipCoeff</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">clippedGrads</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">gradients</span>
<span class="p">}</span>
</code></pre></div>

<ol>
<li><strong>Learning Rate Scheduling</strong>: Use learning rate decay</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="kd">import</span> <span class="nc">MLXOptimizers</span>

<span class="kd">class</span> <span class="nc">LearningRateScheduler</span> <span class="p">{</span>
    <span class="kd">private</span> <span class="kd">let</span> <span class="nv">optimizer</span><span class="p">:</span> <span class="kr">inout</span> <span class="n">Optimizer</span>
    <span class="kd">private</span> <span class="kd">let</span> <span class="nv">patience</span><span class="p">:</span> <span class="nb">Int</span>
    <span class="kd">private</span> <span class="kd">let</span> <span class="nv">factor</span><span class="p">:</span> <span class="nb">Float</span>
    <span class="kd">private</span> <span class="kd">var</span> <span class="nv">bestLoss</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">=</span> <span class="nb">Float</span><span class="p">.</span><span class="n">infinity</span>
    <span class="kd">private</span> <span class="kd">var</span> <span class="nv">waitCount</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">0</span>

    <span class="kd">init</span><span class="p">(</span><span class="n">optimizer</span><span class="p">:</span> <span class="kr">inout</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">patience</span><span class="p">:</span> <span class="nb">Int</span> <span class="p">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">factor</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">=</span> <span class="mf">0.5</span><span class="p">)</span> <span class="p">{</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">optimizer</span> <span class="p">=</span> <span class="p">&amp;</span><span class="n">optimizer</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">patience</span> <span class="p">=</span> <span class="n">patience</span>
        <span class="kc">self</span><span class="p">.</span><span class="n">factor</span> <span class="p">=</span> <span class="n">factor</span>
    <span class="p">}</span>

    <span class="kd">func</span> <span class="nf">step</span><span class="p">(</span><span class="n">validationLoss</span><span class="p">:</span> <span class="nb">Float</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span> <span class="n">validationLoss</span> <span class="o">&lt;</span> <span class="n">bestLoss</span> <span class="p">{</span>
            <span class="n">bestLoss</span> <span class="p">=</span> <span class="n">validationLoss</span>
            <span class="n">waitCount</span> <span class="p">=</span> <span class="mi">0</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="n">waitCount</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">waitCount</span> <span class="o">&gt;=</span> <span class="n">patience</span> <span class="p">{</span>
                <span class="c1">// Reduce learning rate</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="n">learningRate</span> <span class="o">*=</span> <span class="n">factor</span>
                <span class="n">waitCount</span> <span class="p">=</span> <span class="mi">0</span>
                <span class="bp">print</span><span class="p">(</span><span class="s">&quot;Reduced learning rate to </span><span class="si">\(</span><span class="n">optimizer</span><span class="p">.</span><span class="n">learningRate</span><span class="si">)</span><span class="s">&quot;</span><span class="p">)</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<ol>
<li><strong>Data Augmentation</strong>: Improve robustness</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="kd">import</span> <span class="nc">MLX</span>
<span class="kd">import</span> <span class="nc">MLXRandom</span>

<span class="kd">func</span> <span class="nf">augmentAudio</span><span class="p">(</span><span class="kc">_</span> <span class="n">audio</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">MLXArray</span> <span class="p">{</span>
    <span class="c1">// Add noise</span>
    <span class="kd">let</span> <span class="nv">noiseStd</span><span class="p">:</span> <span class="nb">Float</span> <span class="p">=</span> <span class="mf">0.005</span>
    <span class="kd">let</span> <span class="nv">noise</span> <span class="p">=</span> <span class="n">MLXRandom</span><span class="p">.</span><span class="n">normal</span><span class="p">([</span><span class="n">audio</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">mean</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="p">:</span> <span class="n">noiseStd</span><span class="p">)</span>
    <span class="kd">var</span> <span class="nv">augmented</span> <span class="p">=</span> <span class="n">audio</span> <span class="o">+</span> <span class="n">noise</span>

    <span class="c1">// Time stretching (simplified implementation)</span>
    <span class="kd">let</span> <span class="nv">stretchFactor</span> <span class="p">=</span> <span class="nb">Float</span><span class="p">.</span><span class="n">random</span><span class="p">(</span><span class="k">in</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">...</span><span class="mf">1.1</span><span class="p">)</span>

    <span class="c1">// Apply time stretching by resampling</span>
    <span class="kd">let</span> <span class="nv">originalLength</span> <span class="p">=</span> <span class="n">audio</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="kd">let</span> <span class="nv">newLength</span> <span class="p">=</span> <span class="nb">Int</span><span class="p">(</span><span class="nb">Float</span><span class="p">(</span><span class="n">originalLength</span><span class="p">)</span> <span class="o">/</span> <span class="n">stretchFactor</span><span class="p">)</span>

    <span class="c1">// Simple linear interpolation for time stretching</span>
    <span class="n">augmented</span> <span class="p">=</span> <span class="n">resampleAudio</span><span class="p">(</span><span class="n">augmented</span><span class="p">,</span> <span class="n">newLength</span><span class="p">:</span> <span class="n">newLength</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">augmented</span>
<span class="p">}</span>

<span class="kd">func</span> <span class="nf">resampleAudio</span><span class="p">(</span><span class="kc">_</span> <span class="n">audio</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">,</span> <span class="n">newLength</span><span class="p">:</span> <span class="nb">Int</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">MLXArray</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nv">originalLength</span> <span class="p">=</span> <span class="n">audio</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="kd">let</span> <span class="nv">ratio</span> <span class="p">=</span> <span class="nb">Float</span><span class="p">(</span><span class="n">originalLength</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="nb">Float</span><span class="p">(</span><span class="n">newLength</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="kd">var</span> <span class="nv">resampled</span> <span class="p">=</span> <span class="n">MLXArray</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">newLength</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mf">0.</span><span class="p">.&lt;</span><span class="n">newLength</span> <span class="p">{</span>
        <span class="kd">let</span> <span class="nv">originalIndex</span> <span class="p">=</span> <span class="nb">Float</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="n">ratio</span>
        <span class="kd">let</span> <span class="nv">lowerIndex</span> <span class="p">=</span> <span class="nb">Int</span><span class="p">(</span><span class="n">floor</span><span class="p">(</span><span class="n">originalIndex</span><span class="p">))</span>
        <span class="kd">let</span> <span class="nv">upperIndex</span> <span class="p">=</span> <span class="bp">min</span><span class="p">(</span><span class="n">lowerIndex</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">originalLength</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="kd">let</span> <span class="nv">fraction</span> <span class="p">=</span> <span class="n">originalIndex</span> <span class="o">-</span> <span class="nb">Float</span><span class="p">(</span><span class="n">lowerIndex</span><span class="p">)</span>

        <span class="kd">let</span> <span class="nv">lowerValue</span> <span class="p">=</span> <span class="n">audio</span><span class="p">[</span><span class="n">lowerIndex</span><span class="p">].</span><span class="n">item</span><span class="p">()</span>
        <span class="kd">let</span> <span class="nv">upperValue</span> <span class="p">=</span> <span class="n">audio</span><span class="p">[</span><span class="n">upperIndex</span><span class="p">].</span><span class="n">item</span><span class="p">()</span>
        <span class="kd">let</span> <span class="nv">interpolatedValue</span> <span class="p">=</span> <span class="n">lowerValue</span> <span class="o">+</span> <span class="n">fraction</span> <span class="o">*</span> <span class="p">(</span><span class="n">upperValue</span> <span class="o">-</span> <span class="n">lowerValue</span><span class="p">)</span>

        <span class="n">resampled</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="p">=</span> <span class="n">MLXArray</span><span class="p">(</span><span class="n">interpolatedValue</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">resampled</span>
<span class="p">}</span>
</code></pre></div>

<ol>
<li><strong>Batch Size Optimization</strong>: Balance memory usage and convergence</li>
<li><strong>Mixed Precision Training</strong>: Use FP16 for faster training</li>
</ol>
<h2 id="modern-context-and-legacy">Modern Context and Legacy</h2>
<h3 id="impact-on-speech-recognition">Impact on Speech Recognition</h3>
<p>DeepSpeech's end-to-end approach paved the way for:</p>
<ul>
<li><strong>Transformer-based Models</strong>: Attention mechanisms in speech recognition</li>
<li><strong>Wav2Vec</strong>: Self-supervised speech representations</li>
<li><strong>Whisper</strong>: Large-scale multilingual speech recognition</li>
</ul>
<h3 id="current-relevance">Current Relevance</h3>
<p>While newer models have surpassed DeepSpeech's performance, its principles remain fundamental:</p>
<ul>
<li>End-to-end learning approach</li>
<li>CTC for sequence alignment</li>
<li>Deep neural networks for acoustic modeling</li>
</ul>
<h2 id="fine-tuning-deepspeech-for-domain-specific-applications">Fine-Tuning DeepSpeech for Domain-Specific Applications</h2>
<h3 id="understanding-transfer-learning-in-speech-recognition">Understanding Transfer Learning in Speech Recognition</h3>
<p>Fine-tuning a pre-trained DeepSpeech model involves adapting a model that has already learned general speech recognition patterns to perform well on specific domains, accents, or use cases. This approach leverages the acoustic and linguistic knowledge encoded in the pre-trained model while allowing it to specialize for particular applications.</p>
<p>The fundamental principle behind fine-tuning DeepSpeech lies in the hierarchical nature of speech features learned by the neural network. Lower layers typically capture general acoustic patterns like phonemes and basic sound units, while higher layers learn more specific linguistic patterns and contextual relationships. By fine-tuning, we preserve the general acoustic knowledge while adapting the higher-level representations to domain-specific characteristics.</p>
<h3 id="preparation-for-fine-tuning">Preparation for Fine-Tuning</h3>
<h4 id="data-requirements-and-quality">Data Requirements and Quality</h4>
<p>Successful fine-tuning requires careful consideration of your target domain data. Unlike training from scratch, fine-tuning can be effective with significantly smaller datasets, typically ranging from 10 to 100 hours of transcribed audio depending on the domain complexity and how different it is from the original training data.</p>
<p>The quality of fine-tuning data is paramount. Audio should match the target deployment conditions as closely as possible, including recording environments, microphone types, background noise levels, and speaker demographics. For domain-specific applications like medical transcription or legal proceedings, the vocabulary, speaking style, and acoustic characteristics may differ substantially from general conversational speech.</p>
<p>Data diversity within your target domain is crucial. Include speakers with various accents, speaking rates, and vocal characteristics representative of your end users. For specialized domains, ensure coverage of domain-specific terminology, proper nouns, and speaking patterns unique to that field.</p>
<h4 id="domain-analysis-and-gap-assessment">Domain Analysis and Gap Assessment</h4>
<p>Before beginning fine-tuning, conduct a thorough analysis of how your target domain differs from the original training data. Key factors to evaluate include:</p>
<p><strong>Acoustic Differences</strong>: Recording conditions, background noise, channel effects, and audio quality variations between your domain and the pre-trained model's training data.</p>
<p><strong>Linguistic Differences</strong>: Vocabulary coverage, sentence structures, speaking styles, and the prevalence of domain-specific terms or phrases that may not have been well-represented in the original training corpus.</p>
<p><strong>Speaker Demographics</strong>: Age ranges, accent distributions, native vs. non-native speakers, and gender representation in your target application compared to the original training data.</p>
<h3 id="fine-tuning-strategies">Fine-Tuning Strategies</h3>
<h4 id="layer-specific-fine-tuning-approaches">Layer-Specific Fine-Tuning Approaches</h4>
<p>Different fine-tuning strategies can be employed depending on your domain requirements and available computational resources:</p>
<p><strong>Full Model Fine-Tuning</strong>: Updating all model parameters during fine-tuning provides maximum adaptation capability but requires more computational resources and larger domain-specific datasets to avoid overfitting.</p>
<p><strong>Frozen Feature Extractor</strong>: Keeping the lower RNN layers frozen while only fine-tuning the output layers and optionally the final RNN layer. This approach works well when the acoustic characteristics of your domain are similar to the original training data but the vocabulary or linguistic patterns differ.</p>
<p><strong>Progressive Unfreezing</strong>: Starting with frozen lower layers and gradually unfreezing them during fine-tuning. This technique allows the model to first adapt its higher-level representations before adjusting lower-level acoustic features.</p>
<p><strong>Layer-wise Learning Rates</strong>: Applying different learning rates to different parts of the model, typically using smaller learning rates for lower layers (to preserve general acoustic knowledge) and higher learning rates for upper layers (to encourage adaptation to domain-specific patterns).</p>
<h4 id="transfer-learning-considerations">Transfer Learning Considerations</h4>
<p>The effectiveness of fine-tuning depends heavily on the relationship between the source domain (original training data) and target domain (your specific application). When domains are closely related, fine-tuning typically requires fewer epochs and smaller learning rates. For highly specialized domains with unique acoustic or linguistic characteristics, more aggressive fine-tuning may be necessary.</p>
<p>Consider the vocabulary overlap between domains. If your target domain includes many out-of-vocabulary terms, you may need to expand the character set or modify the output layer to accommodate new symbols or characters. This is particularly relevant for applications involving technical terminology, foreign names, or specialized abbreviations.</p>
<h3 id="hyperparameter-optimization-for-fine-tuning">Hyperparameter Optimization for Fine-Tuning</h3>
<h4 id="learning-rate-selection">Learning Rate Selection</h4>
<p>Learning rate selection is critical for successful fine-tuning. Starting with learning rates that are typically 10-100 times smaller than those used for training from scratch helps preserve the pre-trained knowledge while allowing adaptation. A common approach is to begin with a learning rate around 1e-5 to 1e-4 and use learning rate scheduling to gradually decrease it during fine-tuning.</p>
<p>The optimal learning rate often depends on how different your target domain is from the original training data. More similar domains can use smaller learning rates to make subtle adjustments, while very different domains may benefit from slightly higher learning rates to overcome the domain gap.</p>
<h4 id="regularization-and-overfitting-prevention">Regularization and Overfitting Prevention</h4>
<p>Fine-tuning with limited domain-specific data poses significant overfitting risks. Several regularization techniques help mitigate this challenge:</p>
<p><strong>Early Stopping</strong>: Monitor validation performance closely and stop training when validation loss begins to increase, indicating overfitting to the fine-tuning data.</p>
<p><strong>Dropout Adjustment</strong>: Increasing dropout rates during fine-tuning can help prevent overfitting, particularly when working with small datasets.</p>
<p><strong>Data Augmentation</strong>: Applying audio augmentations like speed perturbation, noise addition, and time masking can effectively increase the apparent size of your fine-tuning dataset.</p>
<p><strong>Weight Decay</strong>: Applying L2 regularization helps prevent the model weights from deviating too far from their pre-trained values.</p>
<h3 id="evaluation-and-validation-strategies">Evaluation and Validation Strategies</h3>
<h4 id="metrics-and-benchmarking">Metrics and Benchmarking</h4>
<p>Evaluating fine-tuned models requires careful consideration of domain-specific metrics. While Word Error Rate (WER) remains the primary metric, consider computing WER separately for domain-specific vocabulary versus general vocabulary to understand where improvements are most needed.</p>
<p>For specialized domains, consider metrics beyond WER:</p>
<p><strong>Terminology Accuracy</strong>: Specifically measure recognition accuracy for domain-specific terms, proper nouns, and technical vocabulary.</p>
<p><strong>Semantic Accuracy</strong>: In some domains, semantic correctness may be more important than exact word-level accuracy.</p>
<p><strong>Confidence Calibration</strong>: Evaluate whether the model's confidence scores correlate well with actual accuracy, particularly important for applications where uncertain predictions should be flagged for human review.</p>
<h4 id="cross-validation-and-generalization">Cross-Validation and Generalization</h4>
<p>When working with limited fine-tuning data, implement robust cross-validation strategies to ensure your model generalizes well within the target domain. Consider speaker-independent splits to ensure the model doesn't overfit to particular speakers' characteristics.</p>
<p>Test generalization across different conditions within your domain, such as various recording setups, background noise levels, or speaker demographics. This helps identify potential weaknesses before deployment.</p>
<h3 id="deployment-considerations">Deployment Considerations</h3>
<h4 id="model-optimization-and-compression">Model Optimization and Compression</h4>
<p>Fine-tuned models often need optimization for deployment constraints. Consider techniques like:</p>
<p><strong>Quantization</strong>: Reducing model precision from 32-bit to 16-bit or 8-bit can significantly reduce model size and inference time with minimal accuracy loss.</p>
<p><strong>Pruning</strong>: Removing less important neural network connections can reduce model complexity while maintaining performance.</p>
<p><strong>Knowledge Distillation</strong>: Training a smaller student model to match the performance of your fine-tuned teacher model can create more deployment-friendly versions.</p>
<h4 id="continuous-learning-and-adaptation">Continuous Learning and Adaptation</h4>
<p>Plan for ongoing model improvement after deployment. Implement systems to collect user feedback, identify common error patterns, and periodically retrain or further fine-tune the model with new data from actual usage.</p>
<p>Consider implementing active learning strategies where the model identifies uncertain predictions for human annotation, creating a continuous feedback loop for model improvement.</p>
<h2 id="summary-and-future-directions">Summary and Future Directions</h2>
<p>DeepSpeech represents a significant milestone in making speech recognition more accessible and practical for real-world applications. Its end-to-end architecture, combined with CTC training and the ability to handle variable-length sequences, provides a solid foundation for building robust speech recognition systems.</p>
<p>While newer transformer-based approaches have achieved state-of-the-art results on benchmarks, DeepSpeech's computational efficiency and straightforward architecture continue to make it valuable for many practical applications, especially when computational resources are limited or when transparency and interpretability are important requirements.</p>
<p>The techniques demonstrated in this article, from basic model implementation to advanced training strategies and domain adaptation, provide a comprehensive foundation for working with DeepSpeech in modern machine learning frameworks like MLX. Whether you're building a voice assistant, transcription service, or specialized domain application, understanding these core concepts will help you create effective speech recognition solutions.</p>
        </div>

        <!-- Neighbors -->

        <!-- Google Adsense -->

    <!-- Releated posts -->

    <!-- Comments -->
                </div>
        </main>

    </div>

    <!-- Footer -->
    <footer class="flex-shrink-0 bg-dark text-light small py-1">
        <div class="container text-center">
            &copy;  <a href="https://blogs.entropypages.in">Entropy Pages</a> by <a href="https://blogs.entropypages.in/pages/about.html">Tejus Adiga M</a>. Powered by <a href="http://getpelican.com">Pelican</a>, <a href="http://python.org">Python</a>, <a href="https://getbootstrap.com">Bootstrap 4</a><br>
            <!-- Do not remove below license sentence -->
            License: <a href="https://spdx.org/licenses/CC-BY-4.0.html">CC-BY-4.0</a>, based on <a href="https://github.com/vuquangtrong/simplify-theme">Simplify Bootstrap Theme</a>
        </div>
    </footer>

    <!-- Scripts -->
    <!--
    <script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js"></script>
    -->
    <script type="text/javascript" src="https://blogs.entropypages.in/theme/jquery/jquery-3.4.1.min.js"></script>
    <!--
    <script src="https://ajax.aspnetcdn.com/ajax/bootstrap/4.3.1/bootstrap.min.js"></script>
    -->
    <script type="text/javascript" src="https://blogs.entropypages.in/theme/bootstrap/bootstrap.min.js"></script>
    <!--
    <script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>
    -->
    <script type="text/javascript" src="https://blogs.entropypages.in/theme/style.js"></script>

    <!-- Sharing -->

    <!-- JSON LD -->
<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "name": "DeepSpeech: End-to-End Deep Learning for Speech Recognition",
    "headline": "DeepSpeech: End-to-End Deep Learning for Speech Recognition",
    "datePublished": "2025-07-15 00:00:00+05:30",
    "dateModified": "",
    "author": {
        "@type": "Person",
        "name": "Tejus Adiga M",
        "url": "https://blogs.entropypages.in/author/tejus-adiga-m.html"
    },
    "image": "https://blogs.entropypages.in/images/SiteImage.png",
    "url": "https://blogs.entropypages.in/deepspeech-architecture-speech-processing.html",
    "description": "Explore the DeepSpeech model architecture and discover how it revolutionized automatic speech recognition through end-to-end deep learning, from raw audio input to text transcription."
}
</script>
    <!-- Disqus count -->
</body>

</html>