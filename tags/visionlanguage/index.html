<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"/><meta property="og:site_name" content="Entropy Labs"/><link rel="canonical" href="https://tejusadiga2004.github.io/octave.github.io/tags/visionlanguage"/><meta name="twitter:url" content="https://tejusadiga2004.github.io/octave.github.io/tags/visionlanguage"/><meta property="og:url" content="https://tejusadiga2004.github.io/octave.github.io/tags/visionlanguage"/><title>Entropy Labs</title><meta name="twitter:title" content="Entropy Labs"/><meta property="og:title" content="Entropy Labs"/><meta name="description" content="Pages of Tejus Adiga"/><meta name="twitter:description" content="Pages of Tejus Adiga"/><meta property="og:description" content="Pages of Tejus Adiga"/><meta name="twitter:card" content="summary"/><link rel="stylesheet" href="/octave.github.io/styles.css" type="text/css"/><link rel="stylesheet" href="/octave.github.io/code-styles.css" type="text/css"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><link rel="shortcut icon" href="/images/favicon.png" type="image/png"/><link rel="alternate" href="/feed.rss" type="application/rss+xml" title="Subscribe to Entropy Labs"/></head><body><header><div class="wrapper"><a href="https://tejusadiga2004.github.io/octave.github.io" class="site-name">Entropy Labs</a><nav><ul><li><a href="/octave.github.io/aboutMe">Aboutme</a></li><li><a href="/octave.github.io/posts">My posts</a></li></ul></nav></div></header><div class="wrapper"><h1>Tagged with <span class="tag">vision-language</span></h1><a href="/octave.github.io/tags" class="browse-all">Browse all tags</a><ul class="item-list"><li><article><h1><a href="/octave.github.io/posts/SegCLIP">Improving CLIP with SegCLIP Image segmentation</a></h1><ul class="tags"><li><div class="variant-a"><a href="/octave.github.io/tags/segclip">segclip</a></div></li><li><div class="variant-c"><a href="/octave.github.io/tags/clip">clip</a></div></li><li><div class="variant-e"><a href="/octave.github.io/tags/machinelearning">machine-learning</a></div></li><li><div class="variant-a"><a href="/octave.github.io/tags/visionlanguage">vision-language</a></div></li><li><div class="variant-c"><a href="/octave.github.io/tags/segmentation">segmentation</a></div></li><li><div class="variant-e"><a href="/octave.github.io/tags/classification">classification</a></div></li></ul><p>A comparative analysis of SegCLIP and CLIP models, exploring how image segmentation enhances classification accuracy on the ImageNet dataset.</p></article></li><li><article><h1><a href="/octave.github.io/posts/ClipModelMarked">Understanding and Fine-tuning CLIP: A Revolutionary Vision-Language Model</a></h1><ul class="tags"><li><div class="variant-b"><a href="/octave.github.io/tags/clip">clip</a></div></li><li><div class="variant-b"><a href="/octave.github.io/tags/machinelearning">machine-learning</a></div></li><li><div class="variant-d"><a href="/octave.github.io/tags/visionlanguage">vision-language</a></div></li><li><div class="variant-e"><a href="/octave.github.io/tags/zeroshot">zero-shot</a></div></li><li><div class="variant-d"><a href="/octave.github.io/tags/mlx">mlx</a></div></li><li><div class="variant-a"><a href="/octave.github.io/tags/swift">swift</a></div></li></ul><p>An in-depth exploration of OpenAI's CLIP model, its architecture, training process, zero-shot classification capabilities, and implementation of fine-tuning using Apple's MLX framework.</p></article></li></ul></div><footer><p>Copyright Â© 2025 Tejus Adiga M.Published with Publish Swift package</p><p><ul class="social-links"><li><a href="https://github.com/tejusadiga2004">Github</a></li><li><a href="https://www.linkedin.com/in/tejusadigam">Linked In</a></li></ul></p></footer></body></html>