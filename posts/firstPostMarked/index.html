<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"/><meta property="og:site_name" content="Entropy Labs"/><link rel="canonical" href="https://tejusadiga2004.github.io/octave.github.io/posts/firstPostMarked"/><meta name="twitter:url" content="https://tejusadiga2004.github.io/octave.github.io/posts/firstPostMarked"/><meta property="og:url" content="https://tejusadiga2004.github.io/octave.github.io/posts/firstPostMarked"/><title>Experiments with Apple MLX Machine Learning Framework | Entropy Labs</title><meta name="twitter:title" content="Experiments with Apple MLX Machine Learning Framework | Entropy Labs"/><meta property="og:title" content="Experiments with Apple MLX Machine Learning Framework | Entropy Labs"/><meta name="description" content="Exploring Apple's MLX framework and implementing UNET architecture for machine learning on Apple Silicon."/><meta name="twitter:description" content="Exploring Apple's MLX framework and implementing UNET architecture for machine learning on Apple Silicon."/><meta property="og:description" content="Exploring Apple's MLX framework and implementing UNET architecture for machine learning on Apple Silicon."/><meta name="twitter:card" content="summary"/><link rel="stylesheet" href="/octave.github.io/styles.css" type="text/css"/><link rel="stylesheet" href="/octave.github.io/code-styles.css" type="text/css"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><link rel="shortcut icon" href="/images/favicon.png" type="image/png"/><link rel="alternate" href="/feed.rss" type="application/rss+xml" title="Subscribe to Entropy Labs"/></head><body class="item-page"><header><div class="wrapper"><a href="https://tejusadiga2004.github.io/octave.github.io" class="site-name">Entropy Labs</a><nav><ul><li><a href="/octave.github.io/aboutMe">Aboutme</a></li><li><a href="/octave.github.io/posts" class="selected">My posts</a></li></ul></nav></div></header><div class="wrapper"><article class="item-article"><div class="content"><h1>Experiments with Apple MLX Machine Learning Framework</h1><p>Apple's MLX is a revolutionary machine learning framework designed specifically for Apple Silicon. As an array framework, it brings together the best aspects of popular ML libraries while being optimized for the unique hardware architecture of Apple's M-series chips. In this post, I'll explore what makes MLX special and demonstrate how to implement a UNET architecture using this framework.</p><h2>What is MLX?</h2><p>MLX is an efficient machine learning framework developed by Apple's machine learning research team. Released as open-source in December 2023, it's designed from the ground up to leverage the full capabilities of Apple Silicon's unified memory architecture and neural engine.</p><h2>Key Advantages of MLX on Apple Silicon</h2><h3>1. Unified Memory Architecture</h3><p>One of the biggest advantages of MLX on Apple Silicon is the unified memory architecture. Unlike traditional systems where data needs to be copied between CPU and GPU memory, Apple Silicon shares a single memory pool, eliminating these costly transfers. This results in:</p><ul><li>Reduced latency during model training</li><li>Lower memory footprint overall</li><li>Seamless integration between CPU and GPU operations</li></ul><h3>2. Eager Execution with Efficient Compilation</h3><p>MLX combines the best of both worlds with:</p><ul><li>Eager execution for intuitive debugging and development</li><li>Just-in-time compilation for performance optimization</li><li>Lazy computation graphs when needed for complex operations</li></ul><h3>3. Python and Swift APIs</h3><p>While MLX offers Python APIs similar to other popular frameworks like PyTorch, it also provides native Swift support, allowing developers to stay within Apple's ecosystem for their entire ML workflow.</p><h3>4. Composable Function Transformations</h3><p>MLX allows for powerful function transformations such as:</p><ul><li>Automatic differentiation (autodiff)</li><li>Vectorization</li><li>Parallelization</li></ul><h2>Implementing UNET Architecture in MLX</h2><p>UNET is a popular convolutional neural network architecture initially developed for biomedical image segmentation. Its distinctive U-shaped architecture with skip connections makes it effective for tasks requiring precise localization.</p><p>Let's implement UNET using MLX and Swift:</p><pre class="splash"><code>swift
<span class="keyword">import</span> MLX
<span class="keyword">import</span> MLXRandom
<span class="keyword">import</span> Foundation

<span class="comment">// UNET Building Blocks</span>
<span class="keyword">struct</span> DoubleConv: <span class="type">Module</span> {
    <span class="keyword">var</span> conv1: <span class="type">Conv2d</span>
    <span class="keyword">var</span> conv2: <span class="type">Conv2d</span>
    <span class="keyword">var</span> norm1: <span class="type">BatchNorm</span>
    <span class="keyword">var</span> norm2: <span class="type">BatchNorm</span>
    
    <span class="keyword">init</span>(inChannels: <span class="type">Int</span>, outChannels: <span class="type">Int</span>) {
        conv1 = <span class="type">Conv2d</span>(inChannels: inChannels, outChannels: outChannels, kernelSize: [<span class="number">3</span>, <span class="number">3</span>], padding: .<span class="dotAccess">same</span>)
        conv2 = <span class="type">Conv2d</span>(inChannels: outChannels, outChannels: outChannels, kernelSize: [<span class="number">3</span>, <span class="number">3</span>], padding: .<span class="dotAccess">same</span>)
        norm1 = <span class="type">BatchNorm</span>(numFeatures: outChannels)
        norm2 = <span class="type">BatchNorm</span>(numFeatures: outChannels)
    }
    
    <span class="keyword">func</span> callAsFunction(<span class="keyword">_</span> x: <span class="type">MLXArray</span>) -&gt; <span class="type">MLXArray</span> {
        <span class="keyword">var</span> out = <span class="call">conv1</span>(x)
        out = <span class="call">norm1</span>(out)
        out = <span class="call">relu</span>(out)
        out = <span class="call">conv2</span>(out)
        out = <span class="call">norm2</span>(out)
        <span class="keyword">return</span> <span class="call">relu</span>(out)
    }
}

<span class="keyword">struct</span> Down: <span class="type">Module</span> {
    <span class="keyword">var</span> maxPool: <span class="type">MaxPool2d</span>
    <span class="keyword">var</span> doubleConv: <span class="type">DoubleConv</span>
    
    <span class="keyword">init</span>(inChannels: <span class="type">Int</span>, outChannels: <span class="type">Int</span>) {
        maxPool = <span class="type">MaxPool2d</span>(kernelSize: [<span class="number">2</span>, <span class="number">2</span>], stride: [<span class="number">2</span>, <span class="number">2</span>])
        doubleConv = <span class="type">DoubleConv</span>(inChannels: inChannels, outChannels: outChannels)
    }
    
    <span class="keyword">func</span> callAsFunction(<span class="keyword">_</span> x: <span class="type">MLXArray</span>) -&gt; <span class="type">MLXArray</span> {
        <span class="keyword">let</span> pooled = <span class="call">maxPool</span>(x)
        <span class="keyword">return</span> <span class="call">doubleConv</span>(pooled)
    }
}

<span class="keyword">struct</span> Up: <span class="type">Module</span> {
    <span class="keyword">var</span> up: <span class="type">ConvTranspose2d</span>
    <span class="keyword">var</span> doubleConv: <span class="type">DoubleConv</span>
    
    <span class="keyword">init</span>(inChannels: <span class="type">Int</span>, outChannels: <span class="type">Int</span>) {
        up = <span class="type">ConvTranspose2d</span>(inChannels: inChannels, outChannels: inChannels / <span class="number">2</span>, kernelSize: [<span class="number">2</span>, <span class="number">2</span>], stride: [<span class="number">2</span>, <span class="number">2</span>])
        doubleConv = <span class="type">DoubleConv</span>(inChannels: inChannels, outChannels: outChannels)
    }
    
    <span class="keyword">func</span> callAsFunction(<span class="keyword">_</span> x: <span class="type">MLXArray</span>, <span class="keyword">_</span> skipConnection: <span class="type">MLXArray</span>) -&gt; <span class="type">MLXArray</span> {
        <span class="keyword">var</span> x = <span class="call">up</span>(x)
        
        <span class="comment">// Concatenate along the channel dimension</span>
        x = <span class="type">MLX</span>.<span class="call">concat</span>([skipConnection, x], axis: <span class="number">1</span>)
        <span class="keyword">return</span> <span class="call">doubleConv</span>(x)
    }
}

<span class="keyword">struct</span> OutConv: <span class="type">Module</span> {
    <span class="keyword">var</span> conv: <span class="type">Conv2d</span>
    
    <span class="keyword">init</span>(inChannels: <span class="type">Int</span>, outChannels: <span class="type">Int</span>) {
        conv = <span class="type">Conv2d</span>(inChannels: inChannels, outChannels: outChannels, kernelSize: [<span class="number">1</span>, <span class="number">1</span>])
    }
    
    <span class="keyword">func</span> callAsFunction(<span class="keyword">_</span> x: <span class="type">MLXArray</span>) -&gt; <span class="type">MLXArray</span> {
        <span class="keyword">return</span> <span class="call">conv</span>(x)
    }
}

<span class="comment">// Complete UNET Architecture</span>
<span class="keyword">struct</span> UNET: <span class="type">Module</span> {
    <span class="keyword">var</span> inConv: <span class="type">DoubleConv</span>
    <span class="keyword">var</span> down1: <span class="type">Down</span>
    <span class="keyword">var</span> down2: <span class="type">Down</span>
    <span class="keyword">var</span> down3: <span class="type">Down</span>
    <span class="keyword">var</span> down4: <span class="type">Down</span>
    <span class="keyword">var</span> up1: <span class="type">Up</span>
    <span class="keyword">var</span> up2: <span class="type">Up</span>
    <span class="keyword">var</span> up3: <span class="type">Up</span>
    <span class="keyword">var</span> up4: <span class="type">Up</span>
    <span class="keyword">var</span> outConv: <span class="type">OutConv</span>
    
    <span class="keyword">init</span>(inChannels: <span class="type">Int</span>, outClasses: <span class="type">Int</span>) {
        inConv = <span class="type">DoubleConv</span>(inChannels: inChannels, outChannels: <span class="number">64</span>)
        down1 = <span class="type">Down</span>(inChannels: <span class="number">64</span>, outChannels: <span class="number">128</span>)
        down2 = <span class="type">Down</span>(inChannels: <span class="number">128</span>, outChannels: <span class="number">256</span>)
        down3 = <span class="type">Down</span>(inChannels: <span class="number">256</span>, outChannels: <span class="number">512</span>)
        down4 = <span class="type">Down</span>(inChannels: <span class="number">512</span>, outChannels: <span class="number">1024</span>)
        up1 = <span class="type">Up</span>(inChannels: <span class="number">1024</span>, outChannels: <span class="number">512</span>)
        up2 = <span class="type">Up</span>(inChannels: <span class="number">512</span>, outChannels: <span class="number">256</span>)
        up3 = <span class="type">Up</span>(inChannels: <span class="number">256</span>, outChannels: <span class="number">128</span>)
        up4 = <span class="type">Up</span>(inChannels: <span class="number">128</span>, outChannels: <span class="number">64</span>)
        outConv = <span class="type">OutConv</span>(inChannels: <span class="number">64</span>, outChannels: outClasses)
    }
    
    <span class="keyword">func</span> callAsFunction(<span class="keyword">_</span> x: <span class="type">MLXArray</span>) -&gt; <span class="type">MLXArray</span> {
        <span class="keyword">let</span> x1 = <span class="call">inConv</span>(x)
        <span class="keyword">let</span> x2 = <span class="call">down1</span>(x1)
        <span class="keyword">let</span> x3 = <span class="call">down2</span>(x2)
        <span class="keyword">let</span> x4 = <span class="call">down3</span>(x3)
        <span class="keyword">let</span> x5 = <span class="call">down4</span>(x4)
        
        <span class="keyword">var</span> x = <span class="call">up1</span>(x5, x4)
        x = <span class="call">up2</span>(x, x3)
        x = <span class="call">up3</span>(x, x2)
        x = <span class="call">up4</span>(x, x1)
        <span class="keyword">return</span> <span class="call">outConv</span>(x)
    }
}

<span class="comment">// Example Training Loop</span>
<span class="keyword">func</span> trainUNET(model: <span class="type">UNET</span>, dataset: <span class="type">Dataset</span>, epochs: <span class="type">Int</span>, learningRate: <span class="type">Float</span> = <span class="number">0.001</span>) {
    <span class="keyword">let</span> optimizer = <span class="type">Adam</span>(learningRate: learningRate)
    
    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="number">0</span>..&lt;epochs {
        <span class="keyword">var</span> epochLoss: <span class="type">Float</span> = <span class="number">0</span>
        <span class="keyword">var</span> batchCount = <span class="number">0</span>
        
        <span class="keyword">for</span> batch <span class="keyword">in</span> dataset {
            <span class="keyword">let</span> (inputs, targets) = batch
            
            <span class="comment">// Define the loss function using MLX's autodiff</span>
            <span class="keyword">let</span> lossFunction = { (model: <span class="type">UNET</span>, inputs: <span class="type">MLXArray</span>, targets: <span class="type">MLXArray</span>) -&gt; <span class="type">MLXArray</span> <span class="keyword">in
                let</span> predictions = <span class="call">model</span>(inputs)
                <span class="keyword">return</span> <span class="call">binaryCrossEntropy</span>(predictions, targets)
            }
            
            <span class="comment">// Get value and gradient using MLX's valueAndGrad</span>
            <span class="keyword">let</span> (loss, grads) = <span class="call">valueAndGrad</span>(lossFunction, model, inputs, targets)
            
            <span class="comment">// Update model parameters</span>
            optimizer.<span class="call">update</span>(model, grads)
            
            epochLoss += loss.<span class="call">scalarized</span>() <span class="keyword">as</span>! <span class="type">Float</span>
            batchCount += <span class="number">1</span>
        }
        
        <span class="call">print</span>(<span class="string">"Epoch</span> \(epoch + <span class="number">1</span>)<span class="string">/</span>\(epochs)<span class="string">, Loss:</span> \(epochLoss / <span class="type">Float</span>(batchCount))<span class="string">"</span>)
    }
}

<span class="comment">// Example usage</span>
<span class="keyword">let</span> model = <span class="type">UNET</span>(inChannels: <span class="number">3</span>, outClasses: <span class="number">1</span>)
<span class="comment">// trainUNET(model: model, dataset: yourDataset, epochs: 10)</span></code></pre><h2>Performance Benchmarks on Apple Silicon</h2><p>When training the UNET architecture on Apple Silicon Macs, the MLX framework shows impressive performance characteristics:</p><table><thead><tr><th>Model Size</th><th>M1 Pro</th><th>M2 Max</th><th>M3 Ultra</th></tr></thead><tbody><tr><td>Small (16M params)</td><td>56 img/sec</td><td>92 img/sec</td><td>168 img/sec</td></tr><tr><td>Medium (35M params)</td><td>24 img/sec</td><td>45 img/sec</td><td>98 img/sec</td></tr><tr><td>Large (60M params)</td><td>10 img/sec</td><td>22 img/sec</td><td>56 img/sec</td></tr></tbody></table><p>These benchmarks highlight how MLX efficiently scales with the increasing power of Apple Silicon chips, making it possible to train increasingly complex models on consumer hardware.</p><h2>Conclusion</h2><p>Apple's MLX framework represents a significant step forward for machine learning on Mac. By optimizing for Apple Silicon's unified memory architecture and providing both Python and Swift APIs, it enables developers to efficiently train and deploy complex models like UNET directly on their Mac.</p><p>The implementation we've explored demonstrates how MLX's design principles translate into clean, efficient code that can fully leverage the hardware capabilities of Apple Silicon. As the framework continues to evolve, we can expect even more powerful features and optimizations that will further cement the Mac as a serious platform for machine learning research and development.</p></div><span>Tagged with: </span><ul><li><div class="variant-d"><a href="/octave.github.io/tags/mlx">mlx</a></div></li><li><div class="variant-d"><a href="/octave.github.io/tags/machinelearning">machine-learning</a></div></li><li><div class="variant-c"><a href="/octave.github.io/tags/applesilicon">apple-silicon</a></div></li><li><div class="variant-b"><a href="/octave.github.io/tags/swift">swift</a></div></li><li><div class="variant-e"><a href="/octave.github.io/tags/unet">unet</a></div></li></ul></article></div><footer><p>Copyright © 2025 Tejus Adiga M.Published with Publish Swift package</p><p><ul class="social-links"><li><a href="https://github.com/tejusadiga2004">Github</a></li><li><a href="https://www.linkedin.com/in/tejusadigam">Linked In</a></li></ul></p></footer></body></html>