
<!DOCTYPE html>
<html lang="en">

<!-- Head -->
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-26R9CS17CT"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-26R9CS17CT');
    </script>


        <!-- Required metadata tags -->
        <meta charset="utf-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />

        <!-- Default metadata -->
    <meta name="author" content="Tejus Adiga M" />
    <meta name="description" content="A deep dive into why the XOR operation cannot be solved by a single-layer perceptron and how hidden layers enable neural networks to solve non-linearly separable problems." />
    <meta name="keywords" content="neural networks, perceptron, linear separability, deep learning">
<meta property="og:site_name" content="Entropy Pages" />
<meta property="og:title" content="Why Neural Networks Need Hidden Layers: The XOR Problem Explained" />
<meta property="og:description" content="A deep dive into why the XOR operation cannot be solved by a single-layer perceptron and how hidden layers enable neural networks to solve non-linearly separable problems." />
<meta property="og:locale" content="en_US" />
<meta property="og:url" content="https://blogs.entropypages.in/xor-neural-network-hidden-layer.html" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-06-30 00:00:00+05:30" />
<meta property="article:modified_time" content="" />
<meta property="article:author" content="https://blogs.entropypages.in/author/tejus-adiga-m.html">
<meta property="article:section" content="Machine Learning" />
	<meta property="article:tag" content="neural networks" />
	<meta property="article:tag" content="perceptron" />
	<meta property="article:tag" content="linear separability" />
	<meta property="article:tag" content="deep learning" />
	<meta property="og:image" content="https://blogs.entropypages.in/images/SiteImage.png">

        <!-- Site Claim -->


        <!-- Title -->
        <title>
    Why Neural Networks Need Hidden Layers: The XOR Problem Explained &ndash; Entropy Pages
        </title>
        
        <!-- Icon -->
        <link rel="shortcut icon" href="https://blogs.entropypages.in/favicon.ico" type="image/x-icon">
        <link rel="icon" href="https://blogs.entropypages.in/favicon.ico" type="image/x-icon">

        <!-- Search engine -->
            <meta name="robots" content="" />

        <!-- Feeds -->
            <link href="https://blogs.entropypages.in/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Entropy Pages Full Atom Feed" />




            <link href="https://blogs.entropypages.in/feeds/machine-learning.atom.xml" type="application/atom+xml" rel="alternate" title="Entropy Pages Categories Atom Feed" />




        <!-- Styles -->
        <!--
        <link rel="stylesheet" href="https://ajax.aspnetcdn.com/ajax/bootstrap/4.3.1/css/bootstrap.min.css">
        -->
        <link rel="stylesheet" href="https://blogs.entropypages.in/theme/bootstrap/bootstrap.min.css">
        <!--
        <link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css">
        -->
        <link rel="stylesheet" href="https://blogs.entropypages.in/theme/pygment/friendly.css">
        <!--
        <link rel="stylesheet" href="https://blogs.entropypages.in/theme/extra/admonition.min.css">
        -->
        <link rel="stylesheet" href="https://blogs.entropypages.in/theme/style.css">
        
        <!-- Google Fonts -->
        <link href="https://fonts.googleapis.com/css2?family=Sankofa+Display:wght@400&display=swap" rel="stylesheet">

        <!-- Google Analytics -->

        <!-- Google Global Site Tag -->

        <!-- Google Tag Manager -->

        <!-- Google Adsense -->

        <!-- Heap Analytic -->

        <!-- Piwik Tracking -->

        <!-- Matomo Tracking -->

        <!-- MathJax Support -->
        <script type="text/javascript">
            window.MathJax = {
                tex: {
                    inlineMath: [['$', '$'], ['\\(', '\\)']],
                    displayMath: [['$$', '$$'], ['\\[', '\\]']],
                    processEscapes: true,
                    processEnvironments: true,
                    packages: {'[+]': ['ams', 'newcommand', 'configmacros']},
                    macros: {
                        land: "\\wedge",
                        lor: "\\vee", 
                        lnot: "\\neg"
                    }
                },
                options: {
                    ignoreHtmlClass: 'tex2jax_ignore',
                    processHtmlClass: 'tex2jax_process'
                }
            };
        </script>
        <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js">
        </script>

</head>

<!-- Body -->
<body class="d-flex flex-column" data-spy="scroll" data-target="#toc" data-offset="0" style="position: relative;">
    <!-- Top anchor -->
    <a href="#" id="backToTop" style="display: none; z-index: 1;" title="Back to top"><span></span></a>

    <!-- Google tag manager -->

    <!-- Navigation -->
    <nav class="flex-shrink-0 navbar navbar-expand-md navbar-expand-lg navbar-dark bg-dark text-light shadow-sm">
        <!-- Logo -->
        <a class="navbar-brand site-name" href="https://blogs.entropypages.in/">Entropy Pages</a>

        <!-- Desktop divider -->
        <div class="navbar-divider d-none d-md-block"></div>

        <!-- Collapse button -->
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMenu" aria-controls="navbarMenu" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon small"></span>
        </button>

        <!-- Collapsible content -->
        <div class="collapse navbar-collapse" id="navbarMenu">

            <!-- i18n subsites -->

            <!-- Page links -->
            <ul class="navbar-nav mr-auto text-center">
                <li class="nav-item ">                           
                    <a class="nav-link" href="https://blogs.entropypages.in">
                        <svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24">
                            <path d="M21 13v10h-6v-6h-6v6h-6v-10h-3l12-12 12 12h-3zm-1-5.907v-5.093h-3v2.093l3 3z" fill="currentColor"></path>
                        </svg>
                        Home <span class="sr-only">(current)</span>
                    </a>
                </li>
                <li class="nav-item ">
                    <a class="nav-link" href="https://blogs.entropypages.in/categories.html">
                        <svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24">
                            <path d="M16 6h-8v-6h8v6zm-8 12h-8v6h8v-6zm16 0h-8v6h8v-6zm-11-7v-3h-2v3h-8v5h2v-3h14v3h2v-5h-8z" fill="currentColor"></path>
                        </svg>
                        Categories
                    </a>
                </li>
                <li class="nav-item ">
                    <a class="nav-link" href="https://blogs.entropypages.in/archives.html">
                        <svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24">
                            <path d="M1.8 9l-.8-4h22l-.8 4h-2.029l.39-2h-17.122l.414 2h-2.053zm18.575-6l.604-2h-17.979l.688 2h16.687zm3.625 8l-2 13h-20l-2-13h24zm-8 4c0-.552-.447-1-1-1h-6c-.553 0-1 .448-1 1s.447 1 1 1h6c.553 0 1-.448 1-1z" fill="currentColor"></path>
                        </svg>
                        Archives
                    </a>
                </li>
                <li class="nav-item ">
                    <a class="nav-link" href="https://blogs.entropypages.in/pages/about.html">
                        <svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24">
                            <path d="M20.822 18.096c-3.439-.794-6.64-1.49-5.09-4.418 4.72-8.912 1.251-13.678-3.732-13.678-5.082 0-8.464 4.949-3.732 13.678 1.597 2.945-1.725 3.641-5.09 4.418-3.073.71-3.188 2.236-3.178 4.904l.004 1h23.99l.004-.969c.012-2.688-.092-4.222-3.176-4.935z" fill="currentColor"></path>
                        </svg>
                        About
                    </a>
                </li>
            </ul>

            <!-- Search form -->
            <form class="form-inline text-center" action="https://blogs.entropypages.in/pages/search.html">
                <input class="form-control w-100 bg-dark text-light text-center border-0 p-2" type="text" name="q" pattern=".{3,}" title="At least 3 characters" required="" placeholder="Type here to search" aria-label="Search">
            </form>

            <!-- Social links -->
            <ul class="navbar-nav text-center">
                <li class="nav-item">
                    <a class="nav-link" href="#">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                            <title>Facebook</title>
                            <path d="M12 0c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm3 8h-1.35c-.538 0-.65.221-.65.778v1.222h2l-.209 2h-1.791v7h-3v-7h-2v-2h2v-2.308c0-1.769.931-2.692 3.029-2.692h1.971v3z" fill="currentColor"></path>
                        </svg>
                    </a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="https://github.com/tejusadiga2004">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                            <title>Github</title>
                            <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z" fill="currentColor"></path>
                        </svg>
                    </a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="https://www.linkedin.com/in/tejusadigam/">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                            <title>Linkedin</title>
                            <path d="M12 0c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm-2 16h-2v-6h2v6zm-1-6.891c-.607 0-1.1-.496-1.1-1.109 0-.612.492-1.109 1.1-1.109s1.1.497 1.1 1.109c0 .613-.493 1.109-1.1 1.109zm8 6.891h-1.998v-2.861c0-1.881-2.002-1.722-2.002 0v2.861h-2v-6h2v1.093c.872-1.616 4-1.736 4 1.548v3.359z" fill="currentColor"></path>
                        </svg>
                    </a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="https://x.com/tejusadiga2004">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                            <title>Twitter</title>
                            <path d="M12 0c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm6.066 9.645c.183 4.04-2.83 8.544-8.164 8.544-1.622 0-3.131-.476-4.402-1.291 1.524.18 3.045-.244 4.252-1.189-1.256-.023-2.317-.854-2.684-1.995.451.086.895.061 1.298-.049-1.381-.278-2.335-1.522-2.304-2.853.388.215.83.344 1.301.359-1.279-.855-1.641-2.544-.889-3.835 1.416 1.738 3.533 2.881 5.92 3.001-.419-1.796.944-3.527 2.799-3.527.825 0 1.572.349 2.096.907.654-.128 1.27-.368 1.824-.697-.215.671-.67 1.233-1.263 1.589.581-.07 1.135-.224 1.649-.453-.384.578-.87 1.084-1.433 1.489z" fill="currentColor"></path>
                        </svg>
                    </a>
                </li>
            </ul>
        </div>
    </nav>

    <!-- Full page -->
    <div class="flex-shrink-0 flex-grow-1">

        <!-- Header -->
        <header class="bg-dark text-light shadow-sm pt-3 pb-2">
	<div class="container">
		<h3 id="xor-neural-network-hidden-layer">Why Neural Networks Need Hidden Layers: The XOR Problem Explained</h3>
		<p style="font-size:larger;"><p>A deep dive into why the XOR operation cannot be solved by a single-layer perceptron and how hidden layers enable neural networks to solve non-linearly separable problems.</p></p>
        <div class="row mx-auto mt-3">
            <div class="col-xs-12 col-sm-12 col-md-6 text-left" style="padding: 0">
                <a href="https://blogs.entropypages.in/author/tejus-adiga-m.html" class="card-link">Tejus Adiga M</a>
                <span class="card-link text-success">
                    <span class="post-date" title="Post date">Mon 30 June 2025</span>
                </span>
            </div>
            <div class="col-xs-12 col-sm-12 col-md-6 text-right" style="padding: 0">
                <a class="badge badge-success" href="https://blogs.entropypages.in/category/machine-learning.html">machine learning</a>
                    <a class="badge badge-info" href="https://blogs.entropypages.in/tag/neural-networks.html">neural networks</a>
                    <a class="badge badge-info" href="https://blogs.entropypages.in/tag/perceptron.html">perceptron</a>
                    <a class="badge badge-info" href="https://blogs.entropypages.in/tag/linear-separability.html">linear separability</a>
                    <a class="badge badge-info" href="https://blogs.entropypages.in/tag/deep-learning.html">deep learning</a>
            </div>
        </div>
	</div>
        </header>

        <!-- Main -->
        <main class="py-3">
                <div class="container">
                    <!-- Sharing -->

                    <!-- Content -->
    <!-- 2 columns layout -->
    <!-- single column layout -->
        <!-- Sharing -->

        <!-- Share post -->

        <!-- Article -->
        <div>
            <p>The XOR (exclusive OR) problem is a classic example in neural network theory that beautifully illustrates why multi-layer networks are necessary for solving certain types of problems. In this post, we'll explore why a single-layer perceptron fails at XOR and how adding a hidden layer solves this fundamental limitation.</p>
<h2 id="understanding-the-xor-problem">Understanding the XOR Problem</h2>
<p>The XOR operation returns true (1) only when its inputs differ. Here's the truth table:</p>
<table style="font-size: 0.8em; margin: 0 auto; border-collapse: collapse;">
<thead>
<tr>
<th style="border: 1px solid #ccc; padding: 8px;">Input A</th>
<th style="border: 1px solid #ccc; padding: 8px;">Input B</th>
<th style="border: 1px solid #ccc; padding: 8px;">XOR Output</th>
</tr>
</thead>
<tbody>
<tr>
<td style="border: 1px solid #ccc; padding: 8px; text-align: center;">0</td>
<td style="border: 1px solid #ccc; padding: 8px; text-align: center;">0</td>
<td style="border: 1px solid #ccc; padding: 8px; text-align: center;">0</td>
</tr>
<tr>
<td style="border: 1px solid #ccc; padding: 8px; text-align: center;">0</td>
<td style="border: 1px solid #ccc; padding: 8px; text-align: center;">1</td>
<td style="border: 1px solid #ccc; padding: 8px; text-align: center;">1</td>
</tr>
<tr>
<td style="border: 1px solid #ccc; padding: 8px; text-align: center;">1</td>
<td style="border: 1px solid #ccc; padding: 8px; text-align: center;">0</td>
<td style="border: 1px solid #ccc; padding: 8px; text-align: center;">1</td>
</tr>
<tr>
<td style="border: 1px solid #ccc; padding: 8px; text-align: center;">1</td>
<td style="border: 1px solid #ccc; padding: 8px; text-align: center;">1</td>
<td style="border: 1px solid #ccc; padding: 8px; text-align: center;">0</td>
</tr>
</tbody>
</table>

<p>At first glance, this seems like a simple pattern that any learning algorithm should be able to capture. However, this problem exposed a fundamental limitation of early neural networks.</p>
<h2 id="the-single-layer-perceptron-limitation">The Single-Layer Perceptron Limitation</h2>
<p>A single-layer perceptron (also called a linear classifier) can only learn linearly separable functions. This means it can only draw a straight line (or hyperplane in higher dimensions) to separate different classes.</p>
<h3 id="mathematical-representation">Mathematical Representation</h3>
<p>A single-layer perceptron computes:</p>
<div class="arithmatex">\[\text{output} = \text{activation}(w_1 \cdot x_1 + w_2 \cdot x_2 + b)\]</div>
<p>Where:</p>
<ul>
<li><span class="arithmatex">\(w_1, w_2\)</span> are weights</li>
<li><span class="arithmatex">\(b\)</span> is the bias term</li>
<li><span class="arithmatex">\(\text{activation}\)</span> is typically a step function or sigmoid</li>
</ul>
<p>The decision boundary is a straight line defined by:</p>
<div class="arithmatex">\[w_1 \cdot x_1 + w_2 \cdot x_2 + b = 0\]</div>
<p><img src="https://blogs.entropypages.in/images/perceptron_diagram.png" alt="Single-Layer Perceptron Architecture" style="width: 50%; display: block; margin: 0 auto;"></p>
<h3 id="visualizing-the-problem">Visualizing the Problem</h3>
<p>Let's plot the XOR data points on a 2D plane. The visualization clearly shows the XOR data points:</p>
<p><img src="https://blogs.entropypages.in/images/xor_visualization.png" alt="XOR Problem Visualization" style="width: 50%; display: block; margin: 0 auto;"></p>
<p>Where:</p>
<ul>
<li>● (blue squares) represents points that should output 1</li>
<li>○ (red circles) represents points that should output 0</li>
</ul>
<p><strong>The Problem</strong>: There's no single straight line that can separate the ● points from the ○ points!</p>
<p>Try drawing any straight line on this plot - you'll find that it's impossible to put all the ● points on one side and all the ○ points on the other side.</p>
<h2 id="why-linear-separability-matters">Why Linear Separability Matters</h2>
<h3 id="examples-of-linearly-separable-problems">Examples of Linearly Separable Problems</h3>
<p>Problems that CAN be solved by a single-layer perceptron:</p>
<p><img src="https://blogs.entropypages.in/images/logic_operations_comparison.png" alt="Logic Operations Comparison" style="width: 50%; display: block; margin: 0 auto;"></p>
<p><strong>AND Operation:</strong></p>
<div class="highlight"><pre><span></span><code>(0,0) → 0    (0,1) → 0
(1,0) → 0    (1,1) → 1
</code></pre></div>

<p>A line like <span class="arithmatex">\(x + y = 1.5\)</span> separates these perfectly.</p>
<p><strong>OR Operation:</strong></p>
<div class="highlight"><pre><span></span><code>(0,0) → 0    (0,1) → 1
(1,0) → 1    (1,1) → 1
</code></pre></div>

<p>A line like <span class="arithmatex">\(x + y = 0.5\)</span> works here.</p>
<h3 id="the-xor-difference">The XOR Difference</h3>
<p>XOR requires a <strong>non-linear</strong> decision boundary. The data points form a pattern that can only be separated by multiple lines or a curved boundary, which a single linear classifier cannot provide.</p>
<h2 id="what-happens-when-training-fails">What Happens When Training Fails?</h2>
<p>When we attempt to train a single-layer perceptron on XOR data, the training process fails to converge. Here's why:</p>
<h3 id="the-training-process">The Training Process</h3>
<p>During training, gradient descent tries to find weights that minimize the error:</p>
<ol>
<li><strong>Forward Pass</strong>: Compute <span class="arithmatex">\(y = \sigma(w_1x_1 + w_2x_2 + b)\)</span></li>
<li><strong>Loss Calculation</strong>: Measure error between predicted and actual output</li>
<li><strong>Backward Pass</strong>: Calculate gradients <span class="arithmatex">\(\frac{\partial L}{\partial w_1}\)</span>, <span class="arithmatex">\(\frac{\partial L}{\partial w_2}\)</span>, <span class="arithmatex">\(\frac{\partial L}{\partial b}\)</span></li>
<li><strong>Weight Update</strong>: <span class="arithmatex">\(w_i \leftarrow w_i - \eta \frac{\partial L}{\partial w_i}\)</span></li>
</ol>
<h3 id="why-weights-dont-converge">Why Weights Don't Converge</h3>
<p>The fundamental issue is that <strong>no linear combination of inputs can satisfy all XOR constraints simultaneously</strong>:</p>
<ul>
<li>To separate (0,0)→0 and (0,1)→1: Need <span class="arithmatex">\(w_2 &gt; 0\)</span></li>
<li>To separate (1,0)→1 and (1,1)→0: Need <span class="arithmatex">\(w_2 &lt; 0\)</span></li>
<li>To separate (0,0)→0 and (1,0)→1: Need <span class="arithmatex">\(w_1 &gt; 0\)</span>  </li>
<li>To separate (0,1)→1 and (1,1)→0: Need <span class="arithmatex">\(w_1 &lt; 0\)</span></li>
</ul>
<p>These constraints are <strong>mathematically contradictory</strong>! The algorithm oscillates between different weight configurations, never finding a solution. This oscillating weights indicate that the network is incapable of fitting the given curve hence deeper network is required.</p>
<h3 id="training-behavior">Training Behavior</h3>
<div class="highlight"><pre><span></span><code>Epoch 1-100:   Weights try to satisfy (0,0)→0 and (0,1)→1
Epoch 101-200: Weights adjust for (1,0)→1, breaking previous constraints
Epoch 201-300: Weights try different combination, still failing
...            (continues indefinitely without convergence)
</code></pre></div>

<h3 id="mathematical-proof-of-impossibility">Mathematical Proof of Impossibility</h3>
<p>For a linear classifier to work, we need:</p>
<div class="arithmatex">\[w_1 \cdot 0 + w_2 \cdot 0 + b &lt; 0 \text{ (for (0,0)→0)}$$
$$w_1 \cdot 0 + w_2 \cdot 1 + b &gt; 0 \text{ (for (0,1)→1)}$$  
$$w_1 \cdot 1 + w_2 \cdot 0 + b &gt; 0 \text{ (for (1,0)→1)}$$
$$w_1 \cdot 1 + w_2 \cdot 1 + b &lt; 0 \text{ (for (1,1)→0)}\]</div>
<p>Simplifying:</p>
<ul>
<li>From constraints 1&amp;2: <span class="arithmatex">\(b &lt; 0\)</span> and <span class="arithmatex">\(w_2 + b &gt; 0\)</span> → <span class="arithmatex">\(w_2 &gt; -b &gt; 0\)</span></li>
<li>From constraints 1&amp;3: <span class="arithmatex">\(b &lt; 0\)</span> and <span class="arithmatex">\(w_1 + b &gt; 0\)</span> → <span class="arithmatex">\(w_1 &gt; -b &gt; 0\)</span>  </li>
<li>From constraint 4: <span class="arithmatex">\(w_1 + w_2 + b &lt; 0\)</span> → <span class="arithmatex">\(w_1 + w_2 &lt; -b\)</span></li>
</ul>
<p>But if <span class="arithmatex">\(w_1 &gt; -b &gt; 0\)</span> and <span class="arithmatex">\(w_2 &gt; -b &gt; 0\)</span>, then <span class="arithmatex">\(w_1 + w_2 &gt; -2b &gt; -b\)</span>, which contradicts <span class="arithmatex">\(w_1 + w_2 &lt; -b\)</span>.</p>
<p><strong>This mathematical contradiction proves that no linear solution exists.</strong></p>
<h2 id="the-solution-hidden-layers">The Solution: Hidden Layers</h2>
<p>Adding a hidden layer transforms the problem by creating a new representation space where XOR becomes linearly separable.</p>
<h3 id="how-hidden-layers-work">How Hidden Layers Work</h3>
<p>A neural network with one hidden layer performs these transformations:</p>
<ol>
<li><strong>First Layer (Input → Hidden)</strong>: Maps the original 2D input space to a higher-dimensional hidden space</li>
<li><strong>Second Layer (Hidden → Output)</strong>: Performs linear classification in this new space</li>
</ol>
<p><img src="https://blogs.entropypages.in/images/xor_transformation.png" alt="XOR Space Transformation" style="width: 50%; display: block; margin: 0 auto;"></p>
<p>The visualization above shows how the hidden layer transforms the XOR problem from a non-linearly separable 2D space into a linearly separable 3D space where a simple plane can separate the classes.</p>
<h3 id="mathematical-insight">Mathematical Insight</h3>
<p>The hidden layer essentially learns to transform the input coordinates. For XOR, a common solution involves:</p>
<ol>
<li><strong>Hidden neuron 1</strong>: Learns to detect <span class="arithmatex">\((x_1 \wedge \neg x_2)\)</span></li>
<li><strong>Hidden neuron 2</strong>: Learns to detect <span class="arithmatex">\((\neg x_1 \wedge x_2)\)</span></li>
<li><strong>Output neuron</strong>: Learns <span class="arithmatex">\((h_1 \vee h_2)\)</span></li>
</ol>
<p>This decomposition shows that <span class="arithmatex">\(\text{XOR} = (A \wedge \neg B) \vee (\neg A \wedge B)\)</span></p>
<h3 id="network-architecture">Network Architecture</h3>
<p><img src="https://blogs.entropypages.in/images/multilayer_xor_network.png" alt="Multi-Layer Neural Network for XOR" style="width: 50%; display: block; margin: 0 auto;"></p>
<p>The diagram above shows the complete multi-layer network architecture for solving XOR, with specific weight values that mathematically solve the problem. The hidden layer creates new features that make the problem linearly separable in the transformed space.</p>
<h2 id="implementation-example">Implementation Example</h2>
<p>The sigmoid activation function is defined as:</p>
<div class="arithmatex">\[\sigma(x) = \frac{1}{1 + e^{-x}}\]</div>
<p>Here's a Swift example using the MLX framework showing how a multi-layer network can solve XOR:
As the logic of XOR perfectly fits the given curve, there logic learnt by the network would be deterministic in nature. We could generate the training data on the fly.</p>
<div class="highlight"><pre><span></span><code><span class="kd">import</span> <span class="nc">MLX</span>

<span class="c1">// XOR dataset</span>
<span class="kd">let</span> <span class="nv">X</span> <span class="p">=</span> <span class="n">MLXArray</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> 
    <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="p">])</span>
<span class="kd">let</span> <span class="nv">y</span> <span class="p">=</span> <span class="n">MLXArray</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]])</span>

<span class="c1">// Sigmoid activation function: σ(x) = 1 / (1 + e^(-x))</span>
<span class="kd">func</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="kc">_</span> <span class="n">x</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">MLXArray</span> <span class="p">{</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
<span class="p">}</span>

<span class="c1">// Example weights that solve XOR (after training)</span>
<span class="kd">let</span> <span class="nv">W1</span> <span class="p">=</span> <span class="n">MLXArray</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">20.0</span><span class="p">,</span> <span class="mf">20.0</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">20.0</span><span class="p">,</span> <span class="mf">20.0</span><span class="p">]</span>
<span class="p">])</span>  <span class="c1">// Input to hidden weights</span>

<span class="kd">let</span> <span class="nv">b1</span> <span class="p">=</span> <span class="n">MLXArray</span><span class="p">([[</span><span class="o">-</span><span class="mf">30.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">10.0</span><span class="p">]])</span>  <span class="c1">// Hidden bias</span>

<span class="kd">let</span> <span class="nv">W2</span> <span class="p">=</span> <span class="n">MLXArray</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">20.0</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">20.0</span><span class="p">]</span>
<span class="p">])</span>  <span class="c1">// Hidden to output weights</span>

<span class="kd">let</span> <span class="nv">b2</span> <span class="p">=</span> <span class="n">MLXArray</span><span class="p">([[</span><span class="o">-</span><span class="mf">10.0</span><span class="p">]])</span>  <span class="c1">// Output bias</span>

<span class="c1">// Forward pass</span>
<span class="kd">let</span> <span class="nv">hidden</span> <span class="p">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
<span class="kd">let</span> <span class="nv">output</span> <span class="p">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">matmul</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span>

<span class="bp">print</span><span class="p">(</span><span class="s">&quot;XOR Results:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mf">0.</span><span class="p">.&lt;</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nv">input</span> <span class="p">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="kd">let</span> <span class="nv">result</span> <span class="p">=</span> <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="bp">print</span><span class="p">(</span><span class="s">&quot;</span><span class="si">\(</span><span class="n">input</span><span class="p">.</span><span class="n">asArray</span><span class="si">(</span><span class="nb">Float</span><span class="p">.</span><span class="kc">self</span><span class="si">))</span><span class="s"> → </span><span class="si">\(</span><span class="nb">String</span><span class="si">(</span><span class="n">format</span><span class="p">:</span> <span class="s">&quot;%.3f&quot;</span><span class="p">,</span> <span class="n">result</span><span class="p">.</span><span class="n">item</span><span class="si">(</span><span class="nb">Float</span><span class="p">.</span><span class="kc">self</span><span class="si">)))</span><span class="s">&quot;</span><span class="p">)</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="training-the-network-with-mlx">Training the Network with MLX</h3>
<p>Here's how you could train this network from scratch using simple gradient descent:</p>
<div class="highlight"><pre><span></span><code><span class="kd">import</span> <span class="nc">MLX</span>

<span class="kd">struct</span> <span class="nc">XORNetwork</span> 
<span class="p">{</span>
    <span class="kd">var</span> <span class="nv">W1</span><span class="p">:</span> <span class="n">MLXArray</span>
    <span class="kd">var</span> <span class="nv">b1</span><span class="p">:</span> <span class="n">MLXArray</span>
    <span class="kd">var</span> <span class="nv">W2</span><span class="p">:</span> <span class="n">MLXArray</span>
    <span class="kd">var</span> <span class="nv">b2</span><span class="p">:</span> <span class="n">MLXArray</span>

    <span class="kd">init</span><span class="p">(</span><span class="n">inputSize</span><span class="p">:</span> <span class="nb">Int</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">:</span> <span class="nb">Int</span><span class="p">,</span> <span class="n">outputSize</span><span class="p">:</span> <span class="nb">Int</span><span class="p">)</span> 
    <span class="p">{</span>
        <span class="c1">// Initialize weights with small random values</span>
        <span class="n">W1</span> <span class="p">=</span> <span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">([</span><span class="n">inputSize</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">])</span> <span class="o">*</span> <span class="mf">0.5</span>
        <span class="n">b1</span> <span class="p">=</span> <span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">])</span>
        <span class="n">W2</span> <span class="p">=</span> <span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">([</span><span class="n">hiddenSize</span><span class="p">,</span> <span class="n">outputSize</span><span class="p">])</span> <span class="o">*</span> <span class="mf">0.5</span>
        <span class="n">b2</span> <span class="p">=</span> <span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputSize</span><span class="p">])</span>
    <span class="p">}</span>

    <span class="kd">func</span> <span class="nf">forward</span><span class="p">(</span><span class="kc">_</span> <span class="n">x</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">MLXArray</span> 
    <span class="p">{</span>
        <span class="kd">let</span> <span class="nv">hidden</span> <span class="p">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
        <span class="kd">let</span> <span class="nv">output</span> <span class="p">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">matmul</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
    <span class="p">}</span>

    <span class="kd">func</span> <span class="nf">loss</span><span class="p">(</span><span class="kc">_</span> <span class="n">x</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">,</span> <span class="kc">_</span> <span class="n">y</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="n">MLXArray</span> 
    <span class="p">{</span>
        <span class="kd">let</span> <span class="nv">predictions</span> <span class="p">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1">// Mean squared error loss</span>
        <span class="k">return</span> <span class="n">mean</span><span class="p">(</span><span class="n">square</span><span class="p">(</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="c1">// Training function</span>
<span class="kd">func</span> <span class="nf">train</span><span class="p">(</span><span class="n">network</span><span class="p">:</span> <span class="kr">inout</span> <span class="n">XORNetwork</span><span class="p">,</span> 
           <span class="n">X</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">,</span> 
           <span class="n">y</span><span class="p">:</span> <span class="n">MLXArray</span><span class="p">,</span> 
           <span class="n">epochs</span><span class="p">:</span> <span class="nb">Int</span><span class="p">,</span> 
           <span class="n">learningRate</span><span class="p">:</span> <span class="nb">Float</span><span class="p">)</span> 
<span class="p">{</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="k">in</span> <span class="mf">0.</span><span class="p">.&lt;</span><span class="n">epochs</span> <span class="p">{</span>
        <span class="c1">// Compute gradients using automatic differentiation</span>
        <span class="kd">let</span> <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span> <span class="p">=</span> <span class="n">valueAndGrad</span><span class="p">(</span><span class="n">network</span><span class="p">.</span><span class="n">loss</span><span class="p">)(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="c1">// Update weights using gradient descent</span>
        <span class="n">network</span><span class="p">.</span><span class="n">W1</span> <span class="p">=</span> <span class="n">network</span><span class="p">.</span><span class="n">W1</span> <span class="o">-</span> <span class="n">learningRate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">.</span><span class="n">W1</span>
        <span class="n">network</span><span class="p">.</span><span class="n">b1</span> <span class="p">=</span> <span class="n">network</span><span class="p">.</span><span class="n">b1</span> <span class="o">-</span> <span class="n">learningRate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">.</span><span class="n">b1</span>
        <span class="n">network</span><span class="p">.</span><span class="n">W2</span> <span class="p">=</span> <span class="n">network</span><span class="p">.</span><span class="n">W2</span> <span class="o">-</span> <span class="n">learningRate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">.</span><span class="n">b2</span>
        <span class="n">network</span><span class="p">.</span><span class="n">b2</span> <span class="p">=</span> <span class="n">network</span><span class="p">.</span><span class="n">b2</span> <span class="o">-</span> <span class="n">learningRate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">.</span><span class="n">b2</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="p">==</span> <span class="mi">0</span> <span class="p">{</span>
            <span class="bp">print</span><span class="p">(</span><span class="s">&quot;Epoch </span><span class="si">\(</span><span class="n">epoch</span><span class="si">)</span><span class="s">, Loss: </span><span class="si">\(</span><span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="si">(</span><span class="nb">Float</span><span class="p">.</span><span class="kc">self</span><span class="si">))</span><span class="s">&quot;</span><span class="p">)</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="c1">// Create and train the network</span>
<span class="kd">var</span> <span class="nv">network</span> <span class="p">=</span> <span class="n">XORNetwork</span><span class="p">(</span><span class="n">inputSize</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">outputSize</span><span class="p">:</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">train</span><span class="p">(</span><span class="n">network</span><span class="p">:</span> <span class="p">&amp;</span><span class="n">network</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">)</span>

<span class="c1">// Test the trained network</span>
<span class="kd">let</span> <span class="nv">predictions</span> <span class="p">=</span> <span class="n">network</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="bp">print</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">Trained XOR Results:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mf">0.</span><span class="p">.&lt;</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nv">input</span> <span class="p">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="kd">let</span> <span class="nv">prediction</span> <span class="p">=</span> <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="kd">let</span> <span class="nv">actual</span> <span class="p">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="bp">print</span><span class="p">(</span><span class="s">&quot;</span><span class="si">\(</span><span class="n">input</span><span class="p">.</span><span class="n">asArray</span><span class="si">(</span><span class="nb">Float</span><span class="p">.</span><span class="kc">self</span><span class="si">))</span><span class="s"> → </span><span class="si">\(</span><span class="nb">String</span><span class="si">(</span><span class="n">format</span><span class="p">:</span> <span class="s">&quot;%.3f&quot;</span><span class="p">,</span> <span class="n">prediction</span><span class="p">.</span><span class="n">item</span><span class="si">(</span><span class="nb">Float</span><span class="p">.</span><span class="kc">self</span><span class="si">)))</span><span class="s"> (expected: </span><span class="si">\(</span><span class="n">actual</span><span class="p">.</span><span class="n">item</span><span class="si">(</span><span class="nb">Float</span><span class="p">.</span><span class="kc">self</span><span class="si">))</span><span class="s">)&quot;</span><span class="p">)</span>
<span class="p">}</span>
</code></pre></div>

<h2 id="historical-significance">Historical Significance</h2>
<p>The XOR problem was historically significant because:</p>
<ol>
<li>
<p><strong>Minsky and Papert (1969)</strong>: Their book "Perceptrons" proved that single-layer networks couldn't solve XOR, leading to the first "AI Winter"</p>
</li>
<li>
<p><strong>Revival in the 1980s</strong>: The development of backpropagation and multi-layer networks showed that hidden layers could solve these limitations</p>
</li>
<li>
<p><strong>Foundation for Deep Learning</strong>: This insight laid the groundwork for understanding why depth matters in neural networks</p>
</li>
</ol>
<h2 id="key-takeaways">Key Takeaways</h2>
<ol>
<li><strong>Linear vs. Non-linear</strong>: Single-layer perceptrons can only solve linearly separable problems</li>
<li><strong>Hidden layers enable non-linearity</strong>: They transform the input space to make non-linearly separable problems solvable</li>
<li><strong>Universal approximation</strong>: With enough hidden units, neural networks can approximate any continuous function</li>
<li><strong>Representation learning</strong>: Hidden layers learn useful intermediate representations of the data</li>
</ol>
<h2 id="conclusion">Conclusion</h2>
<p>The XOR problem elegantly demonstrates why neural networks need hidden layers. While it might seem like a simple logical operation, XOR requires the network to learn a non-linear decision boundary that single-layer perceptrons simply cannot represent.</p>
<p>This limitation isn't just theoretical - it applies to many real-world problems that involve complex, non-linear relationships in data. Understanding the XOR problem helps us appreciate why deep learning architectures with multiple hidden layers are so powerful for solving complex tasks in computer vision, natural language processing, and other domains.</p>
<p>The journey from recognizing this limitation to developing solutions paved the way for the deep learning revolution we see today. Sometimes, the simplest problems teach us the most profound lessons about the nature of learning and computation.</p>
        </div>

        <!-- Neighbors -->

        <!-- Google Adsense -->

    <!-- Releated posts -->

    <!-- Comments -->
                </div>
        </main>

    </div>

    <!-- Footer -->
    <footer class="flex-shrink-0 bg-dark text-light small py-1">
        <div class="container text-center">
            &copy;  <a href="https://blogs.entropypages.in">Entropy Pages</a> by <a href="https://blogs.entropypages.in/pages/about.html">Tejus Adiga M</a>. Powered by <a href="http://getpelican.com">Pelican</a>, <a href="http://python.org">Python</a>, <a href="https://getbootstrap.com">Bootstrap 4</a><br>
            <!-- Do not remove below license sentence -->
            License: <a href="https://spdx.org/licenses/CC-BY-4.0.html">CC-BY-4.0</a>, based on <a href="https://github.com/vuquangtrong/simplify-theme">Simplify Bootstrap Theme</a>
        </div>
    </footer>

    <!-- Scripts -->
    <!--
    <script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js"></script>
    -->
    <script type="text/javascript" src="https://blogs.entropypages.in/theme/jquery/jquery-3.4.1.min.js"></script>
    <!--
    <script src="https://ajax.aspnetcdn.com/ajax/bootstrap/4.3.1/bootstrap.min.js"></script>
    -->
    <script type="text/javascript" src="https://blogs.entropypages.in/theme/bootstrap/bootstrap.min.js"></script>
    <!--
    <script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>
    -->
    <script type="text/javascript" src="https://blogs.entropypages.in/theme/style.js"></script>

    <!-- Sharing -->

    <!-- JSON LD -->
<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "name": "Why Neural Networks Need Hidden Layers: The XOR Problem Explained",
    "headline": "Why Neural Networks Need Hidden Layers: The XOR Problem Explained",
    "datePublished": "2025-06-30 00:00:00+05:30",
    "dateModified": "",
    "author": {
        "@type": "Person",
        "name": "Tejus Adiga M",
        "url": "https://blogs.entropypages.in/author/tejus-adiga-m.html"
    },
    "image": "https://blogs.entropypages.in/images/SiteImage.png",
    "url": "https://blogs.entropypages.in/xor-neural-network-hidden-layer.html",
    "description": "A deep dive into why the XOR operation cannot be solved by a single-layer perceptron and how hidden layers enable neural networks to solve non-linearly separable problems."
}
</script>
    <!-- Disqus count -->
</body>

</html>